<?xml version="1.0" encoding="UTF-8"?>
<book xmlns="http://docbook.org/ns/docbook"
    xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xml:lang="de"
    version="5.0-extension Avve-1.0">
    <info>
        <title xml:lang="de">Automatische Klassifikation von E-Books durch Verfahren des maschinellen Lernens</title>
        <subtitle xml:lang="en">Classification of e-books by means of machine learning approaches</subtitle>
        <author>
            <personname>
                <firstname>Kai</firstname>
                <surname>Weber</surname>
            </personname>
            <email>sermo_de_arboribus@seznam.cz</email>
        </author>
        <releaseinfo>Master-Abschlussarbeit</releaseinfo>
        <othercredit role="Betreuer"><personname><honorific>Betreuer: Prof. Dr</honorific><firstname>Christoph</firstname><surname>Schmitz</surname></personname></othercredit>
        <date>Stuttgart, 5.4.2018</date>
        <abstract xml:lang="de"> <!-- Abstract deutsch -->
            <title>Kurzfassung</title>
            <para>In der vorliegenden Arbeit werden verschiedene Maschinenlernverfahren für den Einsatz in der automatischen Klassifikation von E-Books 
                untersucht. Als Klassifikationsschemata werden die in der Buchbranche verbreiteten Taxonomien <emphasis>VLB-Warengruppe</emphasis> und 
                <emphasis>Thema</emphasis> verwendet. Die Transformation der E-Books in Lerndatensätze wird erläutert und evaluiert. Nachdem sich in einer
                Vorauswahlphase Naïve Bayes und (lineare) Supportvektormaschine (SVM) als die beiden Verfahren mit der höchsten Erfolgsquote erwiesen haben, werden spezifische
                Maßnahmen zur Optimierung dieser Verfahren experimentell überprüft. Die Optimierungsversuche erbrachten nur moderate Verbesserungen, am
                hilfreichsten erwiesen sich bei Naïve Bayes der Einsatz von AdaBoost und bei SVM die Zugabe von semantischen Oberbegriffen (Hyperonymen).
                Die erreichte Klassifikationsqualität ist bei VLB-Warengruppen geeignet für ein Vorschlagssystem, welches menschliche Domänenspezialisten bei
                der Klassifikation unterstützen kann. Für das schwierigere Multilabel-Klassifikationsproblem, welches das <emphasis>Thema</emphasis>-Schema aufwirft,
                konnte vorerst keine zufriedenstellende Erfolgsquote erreicht werden. Die gemachten Erfahrungen münden am Ende der Arbeit in Ausblicke und
                Vorschläge für weitere Forschungen.</para>
        </abstract>
        <abstract xml:lang="en"> <!-- Abstract englisch -->
            <title>Abstract</title>
            <para>The present work examines several means of machine learning for usage in the domain of electronic books classification. The classification 
                schemas used are <emphasis>VLB-Warengruppe</emphasis>, common in the German book market, and the international book business standard
                <emphasis>Thema</emphasis>. The data preprocessing transformations of the e-books are elaborated and evaluated. After 
                Naïve Bayes and (linear) support vector machines (SVM) have given the best accuracy in the first selection phase, the study concentrates on 
                optimizing these two algorithms experimentally. The attempts at optimizing only brought about moderate improvements; the most useful of these
                were AdaBoost for Naïve Bayes and hyperonym additions for SVM. The accuracy gained from the trained models was sufficient for building a suggestion
                system for human domain expert indexers in the case of <emphasis>VLB Warengruppe</emphasis>. The more difficult multi-label classification problem 
                of <emphasis>Thema</emphasis> could not reach acceptable accuracy values. The experiences gained so far are summed up and used for suggested further
                research areas at the end of this study.</para>
        </abstract>
    </info>
    <chapter xml:id="ch_Intro"><!-- Einleitung -->
        <title>Einleitung und Problemstellung</title>
        <sect1 xml:id="ch_Intro_Intro"> <!-- Einleitung -->
            <title>Einleitung</title>
            <sect2><!-- Problemstellung -->
                <title>Problemstellung</title>
                <para>Bis in die 1980er Jahre erfolgte die automatische, computergestützte Kategorisierung von Texten in ein Raster von vorgegebenen Textklassen
                    vorwiegend nach dem <indexterm><primary>Knowledge Engineering</primary></indexterm>Knowledge-Engineering-Ansatz, bei dem Expertenwissen codiert wurde.
                    Dieser Ansatz verlor in den 1990er Jahren an Zuspruch und wurde vor allem in der Forschungsgemeinde durch Maschinenlernverfahren ersetzt.
                    Die Erfolgsquote bei der <indexterm><primary>Textklassifikation</primary></indexterm>Textklassifikation durch Maschinenlernen reicht inzwischen an
                    diejenige des Knowledge Engineerings heran und sie ist günstiger
                    und einfacher umzusetzen, weil keine Expertinnen und Domänenspezialisten mehr benötigt werden (<biblioref linkend="bib_Seb02"/>, S. 2).</para>
                <para>In der vorliegenden Arbeit wird versucht, eine <indexterm><primary>Schlagwort</primary></indexterm>Schlagwort-Klassifikation von
                    <glossterm linkend="gt_ebook">E-Books</glossterm> durch Verfahren des Maschinenlernens automatisch durchzuführen<footnote>
                        <para>Eine Anmerkung zur Terminologie: In dieser Arbeit ist mit dem Wort „Klassifikation“ 
                        das gemeint, was im deutschsprachigen Buchwesen gewöhnlich als „Verschlagwortung“ bezeichnet wird, also die Zuordnung 
                        von Büchern bzw. Texten zu thematischen Schlagwörtern oder anderen inhaltlichen, teilweise auch formalen Kriterien. Im 
                        Bezug auf Bücher oder Texte werden „Klassifikation“ und „Verschlagwortung“ hier somit synonym verwendet. Nichtsdestotrotz 
                        unterscheiden sich die beiden Begriffe in Nuancen, indem sie unterschiedliche Betrachtungsweisen evozieren: 
                        „Klassifikation“ ist zugleich ein Verfahren des Maschinenlernens (und lässt sich somit auch auf andere Inhalte als 
                        Buchtexte anwenden), „Verschlagwortung“ steht in einer bibliothekarischen bzw. bibliografischen Tradition. Zur 
                        Unterscheidung von „Schlagwort“ und „Stichwort“ im Buchwesen vgl. darüber hinaus die entsprechenden Einträge in 
                        <biblioref linkend="bib_ABC14"/>.</para></footnote>. Informell bedeutet Maschinenlernen, dass aus einer begrenzten Zahl von bereits
                    klassifizierten Eingabedaten unter Verwendung von Algorithmen bzw. statistischen Berechnungen ein Modell erstellt (d. h. gelernt) wird,
                    welches es ermöglicht, in Zukunft unklassifizierte Daten desselben Anwendungsbereichs automatisch zu klassifizieren. Damit dies überhaupt möglich ist,
                    muss davon ausgegangen werden, dass die Lerndaten und die zukünftigen Daten derselben Wahrscheinlichkeitsverteilung unterliegen, welche
                    jedoch unbekannt ist (vgl. <biblioref linkend="bib_SC08"/>, S. 2-3). Man spricht bei diesem Vorgehen auch von <indexterm><primary>Lernen</primary>
                        <secondary>beobachtetes</secondary></indexterm>beobachtetem Lernen (engl. <emphasis>supervised learning</emphasis>), bei dem eine gewisse Menge
                    von bereits vorklassifizierten Trainingsdaten verfügbar ist. Etwas formaler ausgedrückt besteht das hier untersuchte Problem darin, 
                    aus Tupeln <inlineequation><mml:math><mml:mo>(</mml:mo><mml:mover><mml:mi>d</mml:mi><mml:mo stretchy="true">⃗</mml:mo></mml:mover><mml:mtext>,</mml:mtext>
                        <mml:mi>c</mml:mi><mml:mo>)</mml:mo><mml:mtext>&#160;mit&#160;</mml:mtext><mml:mover><mml:mi>d</mml:mi><mml:mo stretchy="true">⃗</mml:mo></mml:mover>
                        <mml:mtext>&#160;</mml:mtext><mml:mo>∈</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>D</mml:mi></mml:math></inlineequation> (Menge aller E-Book-Dokumente)
                    und <inlineequation><mml:math><mml:mi>c</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>∈</mml:mo><mml:mtext>&#160;</mml:mtext>
                                <mml:mi>C</mml:mi></mml:math></inlineequation> (Menge aller möglichen Klassen) eine Funktion <inlineequation><mml:math><mml:mi>f</mml:mi>
                                    <mml:mo>(</mml:mo><mml:mover><mml:mi>d</mml:mi><mml:mo stretchy="true">⃗</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:math></inlineequation> zu 
                        lernen, welche zukünftige, zum Trainingszeitpunkt noch unbekannte, E-Books möglichst gut klassifizieren kann: <inlineequation><mml:math>
                            <mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mo>↦</mml:mo><mml:mo>C</mml:mo></mml:math></inlineequation>.<footnote>
                    <para>Eine vollständige formale Formulierung des Lernproblems im Maschinenlernens ist sehr viel umfangreicher, trifft notwendige Vorannahmen und 
                        zeigt dann, wie empirisch ermittelte Klassifikationsfunktionen über die Berechnung von Maxima bzw. Minima bei Verlust- und Risikofunktionen an
                        eine unbekannte, nicht direkt beobachtbare inhärente Klassifikationsfunktion angenähert werden können. Für eine rigide Formulierung sei verwiesen auf
                        das Einführungskapitel in <biblioref linkend="bib_SC08"/>. In der hier vorliegenden Arbeit werden wir „moderate“ mathematische Formulierungen angeben,
                        wo sie zum Verständnis notwendig oder hilfreich sind, streben aber weder Vollständigkeit noch formale Strenge an.</para></footnote></para>
                <para>Die konkrete Aufgabe besteht also im hier untersuchten Fall darin, ein Maschinenlernmodell zunächst mit bereits klassifizierten E-Book-Texten zu
                    trainieren und das Modell anschließend qualitativ zu beurteilen. Als Grundlage für die Klassifikation dienen dabei einzig die vorliegenden 
                    E-Book-Dateien und deren Eigenschaften, insbesondere der enthaltene Text. Es handelt sich allgemein gesprochen um ein Problem
                    der Klassifikation von Texten in natürlicher Sprache. Einige zusätzliche Aspekte struktureller Art, wie z.&#160;B. Dateigröße und Anzahl im E-Book enthaltener
                    Abbildungen werden zusätzlich berücksichtigt werden, lassen sich aber, wie zu zeigen sein wird, problemlos neben den Textinformationen in die 
                    gewählten Maschinenlernverfahren integrieren.</para>
                <para><xref linkend="abb9"/> zeigt den gewöhnlichen Ablauf eines Wissensgewinnungsprozesses<indexterm><primary>Wissensgewinnung</primary></indexterm> mit
                    Hilfe des Maschinenlernens. Der Aufbau der vorliegenden Arbeit ist lose an diesem Prozessschaubild orientiert: Für ein Verständnis der Daten 
                    und des Anwendungsfeldes sorgen zunächst die weiteren Abschnitte dieser Einleitung, in denen die Warengruppensystematik des Deutschen Buchhandels
                    und die Thema-Klassifikation von EDItEUR vorgestellt werden. Dem Schritt <emphasis>Data Preprocessing</emphasis> ist dann <xref linkend="ch_TextPropIntro"/>
                    gewidmet. Hier werden sowohl unser Korpus als auch die daraus ermittelten Lernattribute vorgestellt und die technische Verarbeitung der E-Books bis 
                    hin zur Phase <emphasis>Model Building</emphasis> kurz geschildert. <xref linkend="ch_Evaluation"/> erläutert dann Kriterien der Evaluation in allgemeiner
                    Weise und diskutiert Referenzwerte für die Bewertung unserer eigenen E-Book-Klassifikatoren.</para>
                <figure xml:id="abb9" pgwide="1"> <!-- Abbildung Knowledge Discovery im Maschinenlernen -->
                    <title>Knowledge Discovery im Maschinenlernen</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="../img/Abb9_KDD_Process.png" width="100%"/>
                        </imageobject>
                    </mediaobject>
                    <caption><para>(Aus <biblioref linkend="bib_BGSV09"/>, S. 62)</para></caption>
                </figure>
                <para>Die folgenden drei Kapitel fokussieren auf die Phase <emphasis>Interpretation &amp; Evaluation</emphasis>, indem sie drei Maschinenlernverfahren 
                    vorstellen und die mit diesen Verfahren erzielten Ergebnisse bezüglich der angestrebten E-Book-Klassifikation berichten. Gegenstand dieser Kapitel sind
                    auch Untersuchungen hinsichtlich der Optimierung der Lernattribute, welche aufgrund ihrer iterativen Natur teilweise auf die früheren Phasen 
                    rekurrieren. In der oben angeführten Grafik wird dies durch die gestrichelten Pfeile angedeutet.</para>
            </sect2>
            <sect2><!-- Abgrenzung von ähnlichen Problemen -->
                <title>Abgrenzung von ähnlichen Problemen</title>
                <para>Die automatische Klassifikation von Texten<indexterm><primary>Textklassifikation</primary></indexterm> mit Verfahren des Maschinenlernens hat
                    bereits eine lange Tradition 
                    in der Informatik und der Computerlinguistik. Ein großer Teil der Untersuchungen zu diesem Thema bezieht sich 
                    jedoch auf kürzere Texte, z.&#160;B. Webseiten, E-Mails (<biblioref linkend="bib_KLS03"/>), wissenschaftliche Artikel und 
                    Abstracts (<biblioref linkend="bib_GS04"/>), Nachrichtenmeldungen (z.&#160;B. Reuters-Korpus, <biblioref linkend="bib_DPHS98"/>).
                    Aufgrund der deutlich höheren durchschnittlichen Textlänge von E-Books 
                    können sich sowohl Chancen als auch Probleme ergeben. Der Vergleich der Klassifikation kurzer und langer Texte ist 
                    nicht Untersuchungsziel dieser Arbeit. Wir gehen davon aus, dass die Basistechnologien, die beim Lernen von Kategorien für kurze Texte
                    erarbeitet worden sind, auch für längere Texte anwendbar sind, wobei die Anforderungen an Berechnungsdauer und benötigten Speicherplatz 
                    natürlich bei größerer Textmenge sehr viel höher sein können und besonders aufwändige Lernverfahren aus diesem Grund nicht für
                    unser E-Book-Korpus verwendet werden können. Im Übrigen ist zu bedenken, dass die Varianz des Textumfangs bei E-Books sehr hoch sein 
                    kann: Ein Bilderbuch für Kinder enthält für gewöhnlich sehr wenig Text, ein vielhundertseitiges Nachschlagewerk zu 
                    einem Fachthema hingegen sehr viel.</para>
                <para>Ein weiteres Maschinenlernverfahren, welches oft auf Texte angewendet wird, ist das <indexterm><primary>Clustern</primary></indexterm>Clustern.
                    Hierbei handelt es sich um 
                    unbeobachtetes Lernen, da hier keinerlei Trainingsdaten mit vorgegebenen Klassen verwendet werden. Beim Clustern werden automatisch Gruppen
                    von Eingabe-Datensätzen gebildet, welche Ähnlichkeiten miteinander aufweisen. Die so entstehenden Cluster haben keine Namen, man kann sie also 
                    nicht ohne weiteres einer Kategorie oder Klasse zuordnen. Fachliteratur zum Clustern von Texten ist auch für beobachtetes Erlernen von Textklassifikatoren 
                    interessant, da in der Vorverarbeitung und dem Aufbereiten von Texten in beiden Verfahren oftmals ähnlich oder gar identisch vorgegangen wird.</para>
            </sect2>
            <sect2><!-- Mögliche Anwendungsszenarien -->
                <title>Mögliche Anwendungsszenarien</title>
                <para><indexterm><primary>Schlagwort</primary></indexterm>Verschlagwortung war in der Buchbranche schon in Zeiten gedruckter bibliografischer
                    Kataloge ein wichtiges Thema, um Kundinnen und 
                    Kunden mit bestimmten Informationsbedürfnissen bedienen zu können. Durch die Digitalisierung der Branche und 
                    insbesondere durch die steigende Bedeutung von Internetsuchmaschinen bei der Informations- und Produktsuche wurde in 
                    der Branche eine Diskussion um die Qualität von Metadaten angeregt. Treibende Kraft ist hierbei eine „Taskforce 
                    Metadatenbank“ unter dem Dach des <indexterm><primary>Börsenverein des Deutschen Buchhandels</primary></indexterm>Börsenvereins des
                    Deutschen Buchhandels (<biblioref linkend="bib_VLBplus17"/>). In der 
                    Folge dieser Diskussionen wurde beschlossen, schlecht gepflegte oder nicht mehr aktuelle Datensätze der Verlage im 
                    Verzeichnis Lieferbarer Bücher<indexterm><primary>Verzeichnis Lieferbarer Bücher</primary></indexterm>, einer zentralen Datenbank im deutschen 
                    Buchwesen, zu sanktionieren. Parallel dazu traten einige Firmen auf den Markt, die Verlagen <indexterm><primary>Metadatenmanagement</primary>
                    </indexterm>Metadatenmanagement und -veredelung anboten (z.&#160;B. itex, medialike, newbooks solutions).</para>
                <para>Ein trainiertes Modell, welches eine automatische Klassifikation von E-Books vornehmen kann, könnte in der Praxis
                    als ein Qualitätskontrollinstrument eingesetzt werden, indem falsche oder fragwürdige Klassifizierungen erkannt würden
                    und ein Korrekturvorschlag unterbreitet werden kann. Auch könnte ein solches <indexterm><primary>Vorschlagssystem</primary>
                    </indexterm>Vorschlagssystem die Personen, welche mit der Vergabe von Warengruppen- oder Thema-Codes betraut sind, unterstützen.</para>
                <para>Neben dieser primären und direkten Anwendungsmöglichkeit wären auch Szenarien denkbar, in denen die hier entwickelten
                    Erkenntnisse nicht zum Trainieren von Schlagwort-Modellen eingesetzt würden, sondern mit einer noch stärkeren Fokussierung 
                    auf Ziel- oder Interessentengruppen ein Teil eines <glossterm linkend="gt_empfehlungsdienst">Empfehlungsdienstes</glossterm> 
                    werden könnten. Hierzu wären aber weitere Untersuchungen und ein größeres Maß an Transferleistung vonnöten.</para>
            </sect2>
            <sect2><!-- Darstellungskonventionen -->
                <title>Darstellungskonventionen</title>
                <para>Diese Arbeit wurde in DocBook 5.0, einem OASIS-Standard<footnote><para><link xlink:href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=docbook"/>
                    (Zugriff am 24.2.2018).</para></footnote>, ausgezeichnet und mit einer modifizierten XSL-FO-Transformation aus dem DocBook-XSL-Werkzeugkasten
                    nach PDF gerendert. Glossarbegriffe sind mindestens bei ihrer ersten Erwähnung kursiv dargestellt. Glossarbegriffe, bibliografische Hinweise, 
                    Verweise auf Tabellen, Abbildungen und Kapitel oder Abschnitte sind in der PDF-Version verlinkt. Es empfiehlt sich beim Lesen der PDF-Datei auf einem 
                    Computer einen PDF-Reader mit „Zurück“-Button zu verwenden, um leicht zwischen verlinkten Elementen und dem Ursprung der Verlinkung hin- und 
                    herspringen zu können. In Verweisen auf Kapitel, Abschnitte, Abbildungen und Tabellen wird die Seitenzahl in eckigen Klammern hinter den Verweis 
                    gesetzt, um auch in der gedruckten Version solchen Verweisen leicht folgen zu können. Programmcode sowie Methoden- und Klassennamen aus 
                    Programmen sind in <code>nichtproportionaler Schrift</code> ausgezeichnet.</para>
            </sect2>
        </sect1>
        <sect1 xml:id="ch_WGSdDB"><!-- Die Warengruppensystematik des Deutschen Buchhandels -->
            <title>Die Warengruppensystematik <indexterm class="startofrange" xml:id="idt_002"><primary>Warengruppensystematik</primary></indexterm> des Deutschen Buchhandels</title>
            <para>Im deutschen Sprachraum existieren verschiedene Klassifikationsschemata für Bücher. So verwendete die Deutsche Nationalbibliothek<indexterm>
                <primary>Deutsche Nationalbibliothek</primary></indexterm> (DNB) bei der bibliografischen Erschließung neuer Titel bis 2004 
                eine eigene Sachgruppensystematik (<biblioref linkend="bib_DNB03"/>). Im Buchhandel hingegen stand diese rein inhaltliche 
                Klassifikation in der Kritik, sich für die dort nötigen betriebswirtschaftlichen Auswertungen nicht zu eignen. Deshalb wurde 
                in zwei Schritten eine eigene Warengruppensystematik für den Buchhandel entwickelt. Die endgültige Fassung (Version 2.0) 
                wurde zum 1. Januar 2007 zum Branchenstandard erhoben und seither von allen Branchenteilnehmern (Verlage, Zwischenbuchhandel, 
                Sortimentsbuchhandel) verwendet  (<biblioref linkend="bib_ABC14"/>, S. 48/49). Das Verzeichnis Lieferbarer Bücher<indexterm>
                    <primary>Verzeichnis Lieferbarer Bücher</primary></indexterm> (VLB) machte die Angabe von genau einer Warengruppe pro 
                Produkt ab 1. Januar 2008 zur Pflichtangabe (<biblioref linkend="bib_BBL07"/>).</para>
            <para>Die Warengruppen<indexterm><primary>Warengruppe</primary></indexterm>, die die Warengruppensystematik des Deutschen 
                Buchhandels Version 2.0 definiert, werden durch eine vierstellige Zahl codiert. Die erste Stelle steht für die Editionsform
                (z.&#160;B. Hardcover, Taschenbuch, CD-ROM, Kalender), die zweite Stelle codiert die Haupt-Warengruppe (z.&#160;B. Belletristik, Ratgeber,
                Naturwissenschaften), die dritte und vierte Stelle schließlich dienen der Feindifferenzierung (<biblioref linkend="bib_WGS06"/>).</para>
            <para>Da für unsere Zwecke eine primär inhaltliche Klassifikation angestrebt wird, ignorieren wir im Folgenden die 
                erste Stelle zur Editionsform und betrachten als Warengruppe i.e.S. die Ziffern zwei bis vier.</para>
            <?dbfo-need height="8cm"?>
            <sidebar> <!-- Hauptwarengruppen -->
                <?dbfo sidebar-width="10cm"?>
                <?dbfo float-type="left"?>
                <table frame="all" orient="port" pgwide="0" tocentry="1" >
                    <info>
                        <title>Hauptwarengruppen</title>
                    </info>
                    <tgroup cols="2" align="left" colsep="1" rowsep="1">
                        <colspec colname="c1" colwidth="3cm"/>
                        <colspec colname="c2" colwidth="6cm"/>
                        <thead><row><entry>Zweite Ziffer</entry><entry>Hauptwarengruppe</entry></row></thead>
                        <tbody>
                            <row><entry>1</entry><entry>Belletristik</entry></row>
                            <row><entry>2</entry><entry>Kinder- und Jugendbücher</entry></row>
                            <row><entry>3</entry><entry>Reise</entry></row>
                            <row><entry>4</entry><entry>Ratgeber</entry></row>
                            <row><entry>5</entry><entry>Geisteswissenschaften, Kunst</entry></row>
                            <row><entry>6</entry><entry>MINT</entry></row>
                            <row><entry>7</entry><entry>Sozialwissenschaften</entry></row>
                            <row><entry>8</entry><entry>Schule und Lernen</entry></row>
                            <row><entry>9</entry><entry>Sachbuch</entry></row>
                        </tbody>
                    </tgroup>
                </table>
            </sidebar>
            <para>Die Warengruppen sind insofern binnenstrukturiert, als dass unterhalb der jeweiligen Haupt-Warengruppe eine 
                blockweise Gruppierung anhand der Zehnerstelle erfolgt (siehe <xref linkend="abb1"/>). So finden sich in der Haupt-Warengruppe
                1 (Belletristik) 
                unterhalb der Nummer 120 spannende Literatur, die weiter differenziert wird in 121 (Krimis, Thriller, Spionage), 
                122 (historische Kriminalromane) und 123 (Horror). Hieraus ergibt sich die Problematik, dass ein Verlag, der einen 
                Horrorroman zu klassifizieren hat, sich entweder für die Warengruppe 123 oder 120 entscheiden kann. (Wie oben erwähnt, 
                sieht das System vor, dass jedem Buch genau eine Warengruppe zugeordnet wird.) In der Praxis entscheiden sich manche 
                Verlage für die allgemeinere, andere für die speziellere Warengruppe, was für das Trainieren einer automatischen Klassifikation 
                problematisch sein kann. Innerhab eines Verlages ist die Vergabe von Warengruppen im Großen und Ganzen konsistent, 
                verlagsübergreifend ist aber keine Einheitlichkeit zu erwarten<footnote><para>Diese und die weiteren Informationen dieses
                Absatzes verdanke ich der persönlichen Auskunft von Angelika Rausch, Bereichsleiterin der Katalogabteilung des 
                <glossterm linkend="gt_barsortiment">Barsortiments</glossterm> Koch, Neff &amp; Volckmar in Stuttgart.</para></footnote>.
                Ein Antrieb für Verlage, eine möglichst kleine Teilmenge von Warengruppen zu verwenden und somit Bücher in eine 
                allgemeinere Warengruppe einzuordnen oder einzelne Titel mit einer verwandten, aber nicht ganz passenden Warengruppe 
                zu versehen, ist es auch, die Kosten für die Auswertung von Vertriebszahlen gering zu halten. Bei Marktforschungs-Dienstleistern
                wie mediacontrol<footnote xml:id="fn_rausch1"><para><link xlink:href="http://www.media-control.de/"/> (Zugriff am 14.02.2018).</para></footnote>
                oder der Gesellschaft für Konsumforschung<footnote><para><link xlink:href="http://www.gfk.com/de/branchen/medien-und-entertainment/"/>
                (Zugriff am 14.02.2018).</para></footnote> zahlen Verlage für Auswertungen pro Warengruppe.</para>
            <para>Neben einer inkonsistenten Vergabe von Warengruppen zwischen unterschiedlichen Verlagen ist bei semantisch orientierten 
                Klassifikationsaufgaben grundsätzlich auch ein Subjektivitätsfaktor einzurechnen, der sich z.B. in der <indexterm><primary>
                    Indexierungsinkonsistenz</primary></indexterm>Indexierungsinkonsistenz (vgl. 
                <biblioref linkend="bib_Seb02"/>, S. 3 und den Wikipedia-Artikel zum umgekehrten Phänomen, der Indexierungskonsistenz<footnote>
                    <para><link xlink:href="https://de.wikipedia.org/wiki/Indexierungskonsistenz"/> (Zugriff am 27.02.2018).</para>
                </footnote>) äußert.</para>
            <para>Mit Blick auf eine automatische Klassifikation von Texten tut sich noch ein zweites Problem auf: Einige Themen werden auf 
                mehrere Warengruppencodes abgebildet, indem zur Klassifikation nicht nur entweder die Textsorte (z.&#160;B. Fachbuch, Ratgeber, Roman) 
                oder das inhaltliche Thema (z.&#160;B. Psychologie<indexterm><primary>Psychologie</primary><secondary>Bücher über</secondary></indexterm>),
                sondern beide gemeinsam herangezogen werden: So gehören etwa professionelle oder 
                akademische Fachbücher zur Psychologie in die Warengruppen 530 bis 539, psycholgische Sachbücher für ein allgemein interessiertes 
                Publikum in die Warengruppen 930 bis 933 und psychologische Ratgeber zur Hilfe in lebenspraktischen Fragen in die Warengruppe 481. 
                Bei dieser Ausgangslage und eingedenk der Tatsache, dass jedes Buch genau einer Warengruppe zugeordnet wird, sind Abgrenzungsschwierigkeiten 
                zu erwarten, die natürlich nicht nur beim Maschinenlernen auftreten, sondern auch bei der Warengruppenerschließung durch Menschen. Die 
                besondere Schwierigkeit für das Maschinenlernen besteht darin, dass eine rein thematisch-inhaltliche Einordnung der Texte nicht ausreichen
                wird, sondern versucht werden muss, die Zielgruppe der Texte über stilistische oder weitere Merkmale zu erschließen.</para>
            <figure xml:id="abb1"> <!-- Ausschnitt aus der Warengruppensystematik in Baumstruktur -->
                <info>
                    <title>Ausschnitt aus der Warengruppensystematik in Baumstruktur</title>
                </info>
                <mediaobject>
                    <imageobject>
                        <imagedata format="SVG" fileref="../img/Abb1_Warengruppe7.svg" width="100%"/>
                    </imageobject>
                </mediaobject>
            </figure>
            <para>Ein ähnliches grundsätzliches Problem lässt sich bei der Abgrenzung zwischen <indexterm><primary>Jugendbuch</primary>
            </indexterm>Jugendbüchern (Warengruppe 260) und 
                anderen belletristischen Genres (Warengruppen 1xx) erwarten, denn ob ein Thriller, ein Liebes- oder Fantasyroman sich 
                an ein jugendliches oder ein erwachsenes Publikum richtet, ist vielleicht nicht immer rein am Textinhalt festzumachen. 
                In Extremfällen kommt es sogar vor, dass derselbe Text in unterschiedlichen Buchausgaben sowohl an Kinder als auch an 
                Erwachsene verkauft wird<footnote><para>So gibt es zum Beispiel den ersten Band der Harry-Potter-Reihe mit der ISBN 3-551-55167-7 
                    in einer Ausgabe für Kinder und mit ISBN 3-551-55200-2 für Erwachsene. Die Bücher unterscheiden sich ausschließlich durch die 
                    Covergestaltung.</para></footnote>.</para>
            <?dbfo-need height="10cm"?>
            <sidebar> <!-- Thema-Hauptgruppen -->
                <?dbfo float-type="left"?>
                <?dbfo sidebar-width="10cm"?>
                <table>
                    <info>
                        <title>Thema-Hauptgruppen</title>
                    </info>
                    <tgroup cols="2" align="left" colsep="1" rowsep="1">
                        <colspec colname="c1" colwidth="1*"/>
                        <colspec colname="c2" colwidth="6*"/>
                        <thead>
                            <row><entry>Code</entry><entry>Überschrift</entry></row>
                        </thead>
                        <tbody>
                            <row><entry>A</entry><entry>Kunst</entry></row>
                            <row><entry>C</entry><entry>Sprache und Sprachwissenschaft</entry></row>
                            <row><entry>D</entry><entry>Biografien, Literatur, Literaturwissenschaft</entry></row>
                            <row><entry>F</entry><entry>Belletristik und verwandte Gebiete</entry></row>
                            <row><entry>G</entry><entry>Nachschlagewerke, Informationswissenschaften und Interdisziplinäre Themen</entry></row>
                            <row><entry>J</entry><entry>Gesellschaft und Sozialwissenschaften</entry></row>
                            <row><entry>K</entry><entry>Volkswirtschaft, Finanzen, Betriebswirtschaft und Management</entry></row>
                            <row><entry>L</entry><entry>Recht</entry></row>
                            <row><entry>M</entry><entry>Medizin</entry></row>
                            <row><entry>N</entry><entry>Geschichte und Archäologie</entry></row>
                            <row><entry>P</entry><entry>Mathematik und Naturwissenschaften</entry></row>
                            <row><entry>Q</entry><entry>Philosophie und Religion</entry></row>
                            <row><entry>R</entry><entry>Geowissenschaften, Geographie, Umwelt, Planung</entry></row>
                            <row><entry>S</entry><entry>Sport und Freizeit</entry></row>
                            <row><entry>T</entry><entry>Technologie, Ingenieurswissenschaft, Landwirtschaft</entry></row>
                            <row><entry>U</entry><entry>EDV und Informationstechnologie</entry></row>
                            <row><entry>V</entry><entry>Gesundheit, Beziehungen und Persönlichkeitsentwicklung</entry></row>
                            <row><entry>W</entry><entry>Lifestyle, Hobbys und Freizeit</entry></row>
                            <row><entry>X</entry><entry>Graphic Novels, Comics, Cartoons</entry></row>
                            <row><entry>Y</entry><entry>Kinder, Jugendliche und Bildung</entry></row>
                            <row><entry>1</entry><entry>Geographische Qualifier</entry></row>
                            <row><entry>2</entry><entry>Sprach-Qualifier</entry></row>
                            <row><entry>3</entry><entry>Zeitliche Qualifier</entry></row>
                            <row><entry>4</entry><entry>Qualifier für pädagogische Zwecke</entry></row>
                            <row><entry>5</entry><entry>Qualifier für Altersstufen und Besondere Interessensgruppen</entry></row>
                            <row><entry>6</entry><entry>Stil-Qualifier</entry></row>
                        </tbody>
                    </tgroup>
                </table>
            </sidebar>
            <para>Mit uneindeutigen Grenzfällen muss somit sowohl bei der automatischen Klassifikation als auch bei der manuellen Eingruppierung 
                stets gerechnet werden.<indexterm class="endofrange" startref="idt_002"/></para>
        </sect1>
        <sect1 xml:id="ch_Intro_Thema"><!-- Die Thema-Klassifikation -->   
            <title>Die Thema-Klassifikation</title>
            <para><indexterm class="startofrange" xml:id="idt_003"><primary>Thema-Klassifikation</primary></indexterm>Thema ist ein internationales
                Klassifikationsschema, welches von <indexterm><primary>EDItEUR</primary></indexterm>EDItEUR, 
                einer Standardisierungsorganisation für die Buchbranche, seit 2011 entwickelt wird (<biblioref linkend="bib_VLB17"/>). 
                Der Standard liegt seit 2016 in der Version 1.2 vor (<biblioref linkend="bib_EDI17"/>). Im Gegensatz zur Warengruppensystematik 
                (siehe <xref linkend="ch_WGSdDB"/>) sieht das Thema-System vor, dass jedem Buch zugleich mehrere <indexterm><primary>Schlagwort</primary>
                </indexterm>Schlagworte (oder auch Suchstichworte und andere beschreibende oder qualifizierende Begriffe) zugeordnet werden können. Thema
                definiert alfanumerische Codes und unterscheidet zwischen 
                Schlagworten („Headings“) und zusätzlichen Bestimmungscodes („Qualifiers“).</para>
            <para>Die Headings erschließen den eigentlichen Inhalt eines Buches, die Qualifier situieren das Buch mit Blick auf Orte (geografisch), Sprache,
                historische Periode, Erziehungsziele (im 
                Falle von Schul- und Lehrbüchern), Interessenten (intendierte Lesergruppe, z.B. Alter bei Kinder- und Jugendbüchern) und Kunstepoche 
                (z.B. Barock). Formal unterscheiden sich diese beiden Kategorien von Thema-Codes dadurch, dass Headings mit einem Buchstaben beginnen, 
                Qualifier hingegen mit einer Zahl. Dabei gilt für Thema ähnlich wie für die Warengruppensystematik eine baumartige Strukturierung 
                mit Hauptkategorien, die durch Hinzufügung von weiteren Buchstaben oder Ziffern weiter ausdifferenziert werden (siehe <xref linkend="abb2"/>).
                Im Gegensatz zur 
                Warengruppensystematik variiert bei Thema jedoch die Länge der Codes: Einige Themenbereiche sind tiefer strukturiert (längere Pfade) 
                als andere.</para>
            <para>Mit Blick auf ein angestrebtes Maschinenlernen der Thema-Klassifikation stellt das Problem somit ein „multi-label and multi-class 
                classification problem“ (<biblioref linkend="bib_LH03"/>: 141) dar: Bei der <indexterm><primary>Multiklassen-Klassifikation</primary></indexterm>
                <glossterm linkend="gt_multiklassen_klassifikation">Multiklassen-Klassifikation</glossterm> gibt es, im Gegensatz zur Klassifikation
                <indexterm><primary>binäre Klassifikation</primary></indexterm>binärer Klassen, mehr als zwei Klassen, auf welche die Daten abgebildet werden
                müssen, bei der <indexterm><primary>Multilabel-Klassifikation</primary></indexterm><glossterm
                    linkend="gt_multilabel_klassifikation">Multilabel-Klassifikation</glossterm> können pro Datensatz 0 bis n Klassen vergeben werden, 
                mit n = |C| (Anzahl der Klassen).</para>
            <para>Die Interessengemeinschaft Produktmetadaten im <indexterm><primary>Börsenverein des Deutschen Buchhandels</primary>
            </indexterm>Börsenverein des Deutschen Buchhandels<footnote><para>
                <link xlink:href="https://www.boersenverein.de/de/portal/IG_Produktmetadaten/1283024"/> (Zugriff 14.02.2018).</para>
            </footnote> sieht vor, dass Verlage zukünftig ein „Haupt-Thema“ für jedes Buch vergeben sowie beliebig viele passende
                weitere Thema-Codes. Aus dem „Haupt-Thema“ ließe sich dann auch über Ersetzungslisten eine VLB-Warengruppe ableiten<footnote>
                    <para> Die Informationen dieses Absatzes basieren ebenfalls auf persönlicher Auskunft von Angelika Rausch, 
                        vgl. Fußnote <footnoteref linkend="fn_rausch1"/>.</para></footnote>. Die Pflege und Bereitstellung von Thema-Schlagwörtern 
                hat sich im deutschen Buchhandel noch nicht in derselben Breite durchgesetzt wie die (quasi verpflichtende) Angabe 
                einer VLB-Warengruppe. Eine Anleitung zum Tagging von Büchern mit Thema-Codes gibt <biblioref linkend="bib_Thema13"/>.
                <indexterm class="endofrange" startref="idt_003"/></para>
            <figure xml:id="abb2"> <!-- Beispiel für die hierarchische Struktur von Thema -->
                <info>
                    <title>Beispiel für die hierarchische Struktur von Thema</title>
                </info>
                <mediaobject>
                    <imageobject>
                        <imagedata format="PNG" fileref="../img/Abb2_Thema_Hierarchie.png"/>
                    </imageobject>
                </mediaobject>
            </figure>
        </sect1>
        <?dbfo-need height="6cm" ?>
        <sect1><!-- Vorauswahl der zu verwendenden Verfahren mit Hilfe von Weka -->
            <info>
                <title>Vorauswahl der zu verwendenden Verfahren mit Hilfe von Weka</title>
            </info>
            <para>In der Geschichte der automatischen Textklassifikation wurde eine Vielzahl von Verfahren angewendet und erforscht. Sauban und
                Pfahringer zählten in einem Aufsatz von 2003 (<biblioref linkend="bib_SP03"/>, S. 411-412) beispielhaft folgende Ansätze auf:</para>
            <?dbfo-need height="4cm" ?>
            <itemizedlist spacing="compact">
                <listitem>
                    <para>Entscheidungsbäume (decision trees) mit menschenlesbaren Modellen als Ergebnis</para>
                </listitem>
                <listitem>
                    <para>Probabilistische Verfahren (Naïve Bayes und Varianten)</para>
                </listitem>
                <listitem>
                    <para>Supportvektormaschinen</para>
                </listitem>
                <listitem>
                    <para>Rocchio-Methode, bei der zu jeder Dokumentklasse ein Prototyp errechnet wird und in der die Klassifikation eines
                        Dokuments dann durch Ermittlung des zu dem Dokument am nächsten gelegenen Prototypen erfolgt</para>
                </listitem>
                <listitem>
                    <para>Mapping durch multivariate Regression</para>
                </listitem>
                <listitem>
                    <para>Textklassifikation durch Kompressionsmodelle</para>
                </listitem>
                <listitem>
                    <para>Textklassifikation mit Hilfe neuronaler Netze</para>
                </listitem>
                <listitem>
                    <para>Erstellen von „dichten“ Dokumentprofilen vor der Anwendung traditioneller Lernverfahren</para>
                </listitem>
            </itemizedlist>
            <para>Für Anwender und Forscherinnen im Feld des Maschinenlernens bieten sich heute viele Möglichkeiten, mit unterschiedlichen
                Verfahren zu experimentieren und zu arbeiten. Es gibt <indexterm><primary>Benutzeroberfläche, grafische</primary></indexterm>
                grafische Benutzeroberflächen zum Visualisieren von Daten und Lernalgorithmen
                einerseits, umfangreiche Programmbibliotheken zur Integration von Maschinenlernverfahren in eigene Programme andererseits. Eine in 
                besonderer Weise integrierte Maschinenlernwelt hat sich um <glossterm linkend="gt_weka">Weka</glossterm> herausgebildet: Weka<indexterm>
                    <primary>Weka</primary></indexterm> ist ein Framework aus Programmbibliothek und
                <glossterm linkend="gt_gui">GUI</glossterm> und stellt ein Pluginkonzept und -repositorium bereit. Darüber hinaus haben einige der an Weka beteiligten Forscher und 
                Entwickler die umfangreiche Weka-Dokumentation inzwischen so modularisiert, dass daraus auch ein allgemeines Lehrbuch entstanden 
                ist, das nur noch durch lose Verweise mit dem Framework verbunden ist (<biblioref linkend="bib_WFHP17"/>). Aus allen diesen Gründen hat sich
                auch für das oben skizzierte Klassifikationsvorhaben die Verwendung von Werkzeugen aus dem Weka-Framework angeboten.</para>
            <para>Die leichte Verfügbarkeit von Maschinenlernwerkzeugen verführt dazu, eine große Zahl von Algorithmen durchzuprobieren, ohne sich
                intensiv damit zu beschäftigen und somit auch ohne sie zu verstehen. Dies wird auch in der Maschinenlern-Community durchaus beklagt. 
                So warnen etwa Witten et al. in <biblioref linkend="bib_WFHP17"/> (S. 286) einerseits davor, ein Maschinenlernverfahren zu 
                verwenden, ohne seine Parameter zu verstehen und andererseits davor, sich allein auf die Leistung eines Verfahrens in Bezug auf
                die Trainingsdaten zu verlassen, da ein solch einseitiges Vorgehen die Gefahr der <indexterm><primary>Überanpassung</primary>
                </indexterm><glossterm linkend="gt_überanpassung">Überanpassung</glossterm> (engl. overfitting) missachtet<footnote><para>Das Phänomen,
                    wissenschaftliche und technische Errungenschaften zu verwenden, ohne sich für deren Funktionsweise zu interessieren, kritisierte
                    Arthur Koestler schon in den 1950er Jahren auf eindrucksvolle
                Weise: „Modern man lives isolated in his artificial environment, not because the artificial is evil as such, but because of 
                his lack of comprehension of the forces which make it work &#8211; of the principles which relate his gadgets to the forces of nature, 
                to the universal order. It is not central heating which makes his existence 'unnatural', but his refusal to take an interest in the 
                principles behind it. By being entirely dependent on science, yet closing his mind to it, he leads the life of an urban barbarian.“
                (Arthur Koestler: „The Act of Creation. The Danube Edition“. London: Pan Books 1970, S. 247).</para></footnote>. Andererseits ist es
                meines Erachtens sowohl legitim als auch interessant, eine größere Zahl von Maschinenlernverfahren mit einem bestimmten Interesse
                <indexterm><primary>Empirie</primary></indexterm>durchzuprobieren um so ein grobes Gefühl dafür zu bekommen, welche Verfahren sich 
                für die eigenen Ziele eher eignen und welche weniger, um dann in einem Folgeschritt mit den vielversprechendsten weiter zu arbeiten.
                Die oben angesprochene Gefahr der Überanpassung ist dabei natürlich im Blick zu behalten.</para>
            <para>So wurde auch in der vorliegenden Arbeit vorgegangen: In Weka wurde zunächst ermittelt, welche Klassifizierer sich prinzipiell 
                für die vorliegenden Daten eignen (benötigt wurde die Fähigkeit des Umgangs mit numerischen Attributen und <glossterm
                    linkend="gt_nominelle_klasse">nominellen Klassen</glossterm>).
                Anschließend wurde in wiederholten Durchläufen mit steigender Datenmenge untersucht, welcher Algorithmus welche Ergebnisse in 
                welcher Zeit, ggf. auch unter welchen Speicherplatzanforderungen, erbringt. So wurde die Vorauswahl der im Hauptteil dieser Arbeit
                besprochenen Verfahren getroffen. Eine zusammenfassende Übersicht des Vorauswahlsprozesses gebe ich in 
                <xref linkend="app_Verfahrensauswahl"/>.</para>
            <para>Bezogen auf das einfache Kriterium der Prozentzahl korrekt klassifizierter Testdaten (Erfolgsquote, vgl. <xref linkend="ch_Evaluation"/>)
                ergaben sich als am besten arbeitende
                Verfahren für unsere E-Book-Daten bei der Warengruppenklassifikation das Lernen mit Naïve Bayes und mit einer Supportvektormaschine
                (in der Weka-Variante SMO). Diesen beiden Verfahren wurde daher größere Aufmerksamkeit geschenkt. Bei der Klassifikation von Thema-Codes
                wurden als Basislerner dieselben Verfahren eingesetzt, die sich bei den VLB-Warengruppen bewährt haben, allerdings mit entsprechenden
                Modifikation zur Anpassung an die Tatsache, dass pro Buch mehrere Codes vergeben werden können (<indexterm><primary>Multilabel-Klassifikation</primary>
                </indexterm><glossterm linkend="gt_multilabel_klassifikation">Multilabel-Klassifikation</glossterm>).</para>
        </sect1>
    </chapter>
    <chapter xml:id="ch_TextPropIntro"> <!-- Auswahl relevanter Texteigenschaften als Lernattribute -->
        <info>
            <title>Auswahl relevanter Texteigenschaften als Lernattribute</title>
        </info>
        <para><indexterm><primary>Texteigenschaft</primary><secondary>Auswahl von</secondary></indexterm>Um das Ziel dieser Arbeit zu erreichen und E-Books automatisch zu verschlagworten bzw. zu klassifizieren, müssen zunächst relevante 
            Eigenschaften aus den elektronischen Büchern extrahiert werden. Was in diesem Zusammenhang „relevant“ sein kann, musste zunächst erst 
            herausgefunden werden. Hierzu wurden in einem spiralförmig-inkrementellen Prozess Texteigenschaften (oder auch allgemeiner: 
            Dateieigenschaften) betrachtet und mit Blick auf ihre Performanz evaluiert. Eine initiale Auswahl von Eigenschaften speiste sich aus 
            drei Quellen: </para>
        <itemizedlist spacing="compact">
            <listitem>
                <para>Verwendung von Texteigenschaften, die sich laut Fachliteratur für ähnliche Aufgabestellungen bewährt haben;</para>
            </listitem>
            <listitem>
                <para>Verwendung von Texteigenschaften, die sich intuitiv sowie auf Basis von linguistischen Überlegungen anbieten;</para>
            </listitem>
            <listitem>
                <para>Verwendung von Texteigenschaften, die sich bei Verwendung bestimmter Programmier&#xAD;bibliotheken leicht herausarbeiten lassen.</para>
            </listitem>
        </itemizedlist>
        <para>Mit Blick auf die später zu erwartende Performance der Lern- und Klassifizierungsalgorithmen ist in dieser Phase aber ebenfalls zu 
            berücksichtigen, dass die Menge an Texteigenschaften, mit denen gearbeitet wird, möglichst klein gehalten werden sollte. Bei der 
            Textmenge, die unserer Untersuchung zugrunde liegt, führt ein ungefilterter Bag-of-Words-Ansatz (siehe <xref linkend="ch_LexSemProp"/>) leicht zu einer 
            Menge von über 2.000.000 Attributen. Beim Lernen mit einer Supportvektormaschine (siehe <xref linkend="ch_SVM" /> würden dann Hyperebenen 
            gesucht, welche die zu lernenden Klassen im über 2.000.000-dimensionalen Raum trennen. Dieses Beispiel zeigt, dass die Anzahl der 
            Textattribute einen starken Einfluss auf die Komplexität der mathematischen Berechnungen beim Trainieren eines Textklassifikators haben kann.
        </para>
        <para>Eine ausführliche Diskussion der Ermittlung von Textattributen erfolgt in <xref linkend="ch_TextProp"/></para>
        <sect1 xml:id="ch_Korpus"> <!-- Das Korpus -->
            <info>
                <title><indexterm class="startofrange" xml:id="idt_004"><primary>Korpus</primary></indexterm>Das Korpus</title>
            </info>
            <para>Als Datenbasis für die folgenden Untersuchungen dient eine Sammlung von <numberOfInstances/> E-Books im <glossterm linkend="gt_epub">EPUB-Format</glossterm> aus 
                ca. 70 verschiedenen Verlagen und <glossterm linkend="gt_imprint">Imprints</glossterm>. Das Programm dieser Verlage umfasst nicht die ganze Bandbreite
                des deutschen Buchmarkts – und somit auch nicht alle definierten <indexterm><primary>Warengruppe</primary></indexterm>Warengruppen –, 
                sondern konzentriert sich auf das, was in der Buchbranche <indexterm><primary>Allgemeines Sortiment</primary></indexterm>
                „Allgemeines Sortiment“ genannt wird, also Titel, die in „gewöhnlichen“ Buchhandlungen, die sich nicht auf Spezialmärkte 
                ausgerichtet haben, üblicherweise angeboten werden: Belletristik, Kinder- und Jugendbuch, politische und 
                historische Sachbücher sowie Ratgeberliteratur (Gesundheit, Kochen, Reisen, Berufsalltag, Erziehung, Garten). An speziellerer 
                Fachliteratur finden sich im Korpus vor allem medizinische und psychologische Bücher, in geringerem Umfang auch geistes- und 
                sozialwissenschaftliche Inhalte.</para>
            <para>Für alle E-Books des Korpus liegen neben den E-Book-Dateien auch Metadaten vor, welche insbesondere die von den Verlagen 
                vergebene Warengruppe sowie in vielen Fällen Thema-Klassifikationen beinhalten. Um <indexterm><primary>Rauschen</primary>
                </indexterm>Rauschen in der <indexterm><primary>Trainingsphase</primary></indexterm>Trainingsphase möglichst zu reduzieren, 
                wurden alle Warengruppen-Klassifikationen manuell überprüft und gegebenenfalls korrigiert. Dabei wurde in Zweifelsfällen die 
                Klassifikation des jeweiligen Verlags beibehalten, in eindeutigen Fällen hingegen die Warengruppe des Verlages überschrieben.
                Beispiele für eindeutige Fehlklassifikationen des Ausgangsmaterials sind zum Beispiel der Roman eines 1953 geborenen Autors, dem
                die Warengruppe 111 („Hauptwerk vor 1945“) zugeordnet wurde. Um einen Zweifelsfall handelt es sich hingegen, wenn ein Buch, welches
                überwiegend gesunde Kochrezepte enthält, als Warengruppe 461 (Ernährungsratgeber) statt als Warengruppe 456 (gesunde und schlanke
                Küche, vorwiegend Rezepte) identifiziert wurde. Es ist üblich, dass ein Buch mit Kochrezepten eine allgemeine Einführung über 
                Zutaten oder Nährwerte enthält, ebenso wie es üblich ist, dass ein Ernährungsratgeber auch um Rezepte ergänzt wurde. Hier fällt 
                eine genaue Abgrenzung und somit auch eine Korrektur im Einzelfall sehr schwer. Darüber hinaus wurden allgemeinere Warengruppen der 
                zweiten Hierarchiebene (z.B. 110, 120, 130, etc., vgl. <xref linkend="ch_WGSdDB"/>) wenn möglich spezifiziert (also z.B. 
                statt 110 eine der Warengruppen 111, 112, …, 118) gesetzt.</para>
            <para>Die Modifikation der Warengruppen auf diese beiden Arten betraf ca. <modifiedInstancesRatio/> der Titel im Korpus.</para>
            <para>Dieses Korpus stellt eine Teilmenge der E-Books dar, welche in einer großen deutschen <glossterm linkend="gt_verlagsauslieferung">Verlagsauslieferung</glossterm>
                vorliegen. Die <numberOfInstances/> Titel wurden zufällig, aber statistisch repräsentativ ausgewählt und im Verhältnis 2:1 in eine Trainings- und eine 
                Testmenge aufgeteilt, wobei darauf geachtet wurde, dass beide Mengen die gleiche prozentuale Verteilung der Warengruppen (Klassen) aufweisen.
                Bei der Auswahl wurde versucht, <indexterm><primary>Duplikate</primary></indexterm>Duplikate weitestgehend zu eliminieren, da einige Maschinenlernverfahren
                mit mehrfach vorkommenden identischen Datensätzen nicht gut umgehen können. Unter einem Duplikat verstehen wir hier Dateien, deren Haupttext 
                (Gesamttext abzüglich Impressumsangaben und evtl. in das Buch integrierte Werbetexte) identisch ist.</para>
            <para>Sammelbände, die z.&#160;B. drei Romane in einer Datei bündeln, konnten hingegen durchaus parallel zu den entsprechenden 
                Einzelausgaben in die Trainings- oder Testmenge aufgenommen werden. Die Duplikatskontrolle erfolgte manuell, Kandidaten wurden 
                über die Metadaten (Buchtitel oder ISBN) ermittelt, so dass in Einzelfällen durchaus gleiche oder sehr ähnliche Datensätze in 
                das Korpus gelangt sein können<footnote><para>E-Books werden häufig von Printbüchern „abgeleitet“, so dass bei Erscheinen einer 
                    neuen Printausgabe mit neuer ISBN, neuem Cover und unter Umständen aktualisierten Werbeanzeigen am Buchende auch eine neue 
                    E-Book-Ausgabe erscheint. Der Haupttext bleibt dann im Falle von Belletristik in der Regel gleich, geändert werden das 
                    Impressum und die erwähnten Anzeigen. Im Fach- und Sachbuchbereich wird bei einer neuen Auflage häufiger auch der Haupttext 
                    aktualisiert, es gibt hier sogar Fälle, in denen der Buchtitel angepasst wird. Ein einfacher Abgleich von ISBN und 
                    Haupttitel ist deshalb kein verlässliches Mittel, um Duplikate mit Sicherheit auszuschließen.</para></footnote>.</para>
            <para>Eine Übersicht über die statistische Verteilung der Warengruppen im Korpus findet sich im Anhang (siehe <xref linkend="app_WG_dist"/>). Verglichen
                mit der Warengruppenverteilung von E-Books im deutschen Markt ist unser Korpus nicht repräsentativ. Nach Vergleichszahlen, welche mir die
                Katalogabteilung des <glossterm linkend="gt_barsortiment">Barsortiments</glossterm> Koch, Neff &amp; Volckmar (KNV) zur Verfügung stellte, sind Romane und 
                Erzählungen (Warengruppen 110 bis 112) sowie Krimis (Warengruppe 121) in unserem Korpus um jeweils ca. 4% bis 7% überrepräsentiert. Hingegen findet sich 
                im Korpus kein einziges Buch aus Warengruppe 739 (Politikwissenschaft: Sonstiges), welche wiederum im KNV-Katalog die zweithäufigste Warengruppe darstellt.
                Weitere Verteilungsunterschiede sind in der erwähnten Tabelle im Anhang aufgeführt.</para>            
            <para>Die Beschränkung auf <indexterm><primary>E-Book</primary></indexterm>E-Book-Dateien im <indexterm><primary>EPUB</primary></indexterm>
                <glossterm linkend="gt_ebook">EPUB-Format</glossterm> ist nicht zwingend und rein pragmatisch begründet. Neben EPUB spielen auch <indexterm>
                    <primary>PDF</primary></indexterm><glossterm linkend="gt_pdf">PDF</glossterm> und <indexterm><primary>Mobipocket</primary></indexterm>Mobipocket eine Rolle im E-Book-Markt. 
                Bei Mobipocket handelt es sich um ein proprietäres Format, so dass der Volltext der Bücher nicht zuverlässig und leicht zu extrahieren 
                ist<footnote><para>Die Python-Skripte „KindleUnpack“ bzw. „MobiUnpack“, die man in diversen Internetforen finden kann, sowie die E-Book-Lesesoftware
                    „Calibre“ (<uri>https://calibre-ebook.com/</uri>) stellen programmierbaren Zugriff auf den Volltext von Mobipocket-E-Books bereit, 
                    basieren aber auf Reverse Engineering, nicht auf einem veröffentlichten Standard.</para></footnote>. PDF ist hingegen ein offener Standard<footnote><para>ISO 32000-1:2008, 
                        siehe <uri>https://www.iso.org/standard/51502.html</uri>.</para></footnote>, welcher jedoch ein sehr komplexes Binärformat definiert, 
                das ohne Nutzung vorgefertigter Programmbibliotheken in eigenen Programmen nur sehr schwer gelesen werden kann. EPUBs<footnote>
                    <para>Der EPUB-Standard wurde vom International Digital Publishing Forum entwickelt. Die Pflege und Weiterentwicklung des Standards ist 
                        im Februar 2017 an das World Wide Web Consortium (W3C) übergegangen: <uri>https://www.w3.org/2017/01/pressrelease-idpf-w3c-combination.html.en</uri>.
                    </para></footnote> bestehen aus einem Zip-Container, in dem vor allem Text- und Grafikdateien liegen. Alle Textdateien
                im EPUB-Container sind XML-Dateien, so dass ein Zugriff sowohl aus Programmen als auch mit einfachen Texteditoren leicht möglich ist.</para> 
                <para>Ein weiterer Vorteil des EPUB-Formats ist, dass immer zwei Inhaltsverzeichnisse eingebettet sind: Einerseits ein Inhaltsverzeichnis,
                    welches der Leserin oder dem Nutzer des E-Books angezeigt werden und zur Navigation im Buch dienen und andererseits eine rein 
                    technische Kurzbeschreibung der im EPUB-Zip-Container enthaltenen Dateien („Manifest“ inklusive <glossterm linkend="gt_mimetyp">
                        MIME-Typ</glossterm>). Diese Daten werden wir nutzen, um einige Daten über die Textstruktur, z.&#160;B. die Schachtelungstiefe der
                    Kapitel, zu gewinnen.</para>
            <para>Diese einfachen Zugriffs- und Verarbeitungsmöglichkeiten gaben den Ausschlag, uns für die hier vorliegende Untersuchung auf EPUB zu beschränken.
                Der Verzicht auf die Einbeziehung von PDF- und Mobipocket-Dateien fiel umso leichter, als Verlage viele E-Books mit identischem
                Textinhalt in mehreren der genannten Formate parallel anbieten, so dass der thematisch-inhaltliche Verlust durch die Formatbeschränkung äußerst
                gering ist. 
                Allenfalls im Bereich wissenschaftlicher Fachliteratur mit einem hohen Anteil an Tabellen und Grafiken sowie der Notwendigkeit, dass
                ein Text zitierfähig ist und somit über eine feste Paginierung verfügt, besteht noch eine Präferenz für das PDF-Format. Im 
                Korpus, der für die vorliegende Studie zur Verfügung stand, betrifft dies lediglich einige medizinische Fachbücher sowie Lehr-
                und Unterrichtsmaterial.<indexterm class="endofrange" startref="idt_004"/></para>
        </sect1>
        <sect1 xml:id="ch_TextProp"><!-- Verwendete Texteigenschaften -->
            <title>Verwendete Texteigenschaften</title>
            <para><indexterm><primary>Texteigenschaft</primary></indexterm>Texteigenschaften im Sinne dieser Arbeit sind quantifizierbare
                Größen, welche sich aus den zu untersuchenden Texten deterministisch ermitteln lassen. Diese Eigenschaften werden dann als <indexterm>
                    <primary>Attribut</primary></indexterm>Attribute beim Trainieren und bei der Evaluation der Maschinenlernalgorithmen 
                verwendet. Die Ausdrücke „Texteigenschaft“ und „Attribut“ bezeichnen somit denselben Sachverhalt aus unterschiedlichen 
                Blickwinkeln betrachtet: Eine Texteigenschaft wird durch ihre Verwendung im Maschinenlernen zum Attribut.</para>
            <para>Texteigenschaften wurden mit Blick darauf ausgewählt, ob sie in erwartbarer Weise bei der Textklassifikation helfen.
                Zunächst stützte sich ihre Auswahl, wie bereits in der Einleitung zu <xref linkend="ch_TextPropIntro"/> beschrieben, auf intuitive
                Erwartungen, welche sich sowohl aus der Fachliteratur als auch aus meiner subjektiven Erfahrung mit Texten speisten.
                Die folgenden Unterkapitel beschreiben die auf diese Art ermittelten Eigenschaften. Im abschließenden Kapitel werden
                die Attribute schließlich evaluiert.</para>
            <para>Es sei hier noch darauf hingewiesen, dass unser Begriff der Texteigenschaften über semantische
                Eigenschaften im eigentlichen Sinne hinausgeht. Automatische Textklassifikation solle sich nach Sebastiani (<biblioref linkend="bib_Seb02"/>,
                S. 3) nicht auf das Vorhandensein von außerhalb des Textes anzusiedelnden Metadaten stützen müssen. Es sei jedoch durchaus legitim, 
                derartige Meta-Information zu nutzen, wo sie vorhanden ist.</para>
            <para>Eine allgemeine Vorbemerkung ist an dieser Stelle noch nötig. Für einige Attribute werden im Folgenden minimale und maximale
                Werte angegeben. Die angegebenen Werte können mitunter unplausibel niedrig oder hoch erscheinen. Dies liegt daran, dass einige 
                der E-Books im Korpus keinen oder fast keinen auslesbaren Text enthalten, sondern ausschließlich Grafikdateien. Das betrifft 
                vor allem Comics und Bilderbücher. Wenn z.&#160;B. der einzige auslesbare Text aus einem <indexterm><primary>Comic</primary><secondary>Besonderheiten</secondary>
                </indexterm>Manga-Comic die ISBN ist und diese als
                Zahl gedeutet wird, so ergibt sich rechnerisch, dass der Text dieses E-Books zu hundert Prozent aus Kardinalzahlen besteht.
                Ein Auslesen des in die Grafikdateien eingebetteten Comic-Textes mit Hilfe von <glossterm linkend="gt_ocr">OCR</glossterm>-Software wäre
                zu aufwändig gewesen und konnte aus zeitlichen Gründen nicht geleistet werden.</para>
            <sect2 xml:id="ch_FileProp"><!-- Dateieigenschaften und Textstruktur -->
                <title>Dateieigenschaften und Textstruktur</title>
                <para>Werfen wir zunächst einen makroskopischen Blick auf den Text und seine Auslieferungsform, so ergibt sich als unmittelbar
                    ablesbare Größe die Anzahl der Bytes, welche die E-Book-Datei auf einem Speichermedium einnimmt. Diese Zahl ist nicht 
                    zu verwechseln mit der Textlänge, denn eine E-Book-Datei enthält neben dem Text und den Metadaten auch Abbildungen, unter
                    Umständen auch weitere multimediale Inhalte wie Audio- oder Videodateien. Da es sich bei den untersuchten 
                    <glossterm linkend="gt_epub">EPUBs</glossterm><indexterm class="startofrange" xml:id="idt_011">
                        <primary>EPUB</primary></indexterm> um zip-komprimierte Dateien handelt und sich Textinhalte stärker komprimieren
                    lassen<footnote><para>Zu verschiedenen Verfahren der Textkompression vgl. <biblioref linkend="bib_BR99"/>, S. 173-188.</para>
                    </footnote> als Grafiken oder Audiomaterial, weist eine große EPUB-Datei auf das Vorhandensein solcher <indexterm><primary>Multimedia</primary>
                    </indexterm>Multimediainhalte hin. Die Dateigröße ist 
                    somit eigentlich keine Texteigenschaft, sondern eine E-Book-Eigenschaft, die über den Buchtext hinausweist. Sie kann 
                    bei der Klassifikation aber gerade deshalb von Interesse sein, da für bestimmte Buchklassen ein höheres Maß an 
                    Multimedialität erwartet werden kann als für andere: Kinderbücher können eine Vorlesefunktion mit Hilfe eingebetteter
                    Audiodateien enthalten, Kochbücher und Reiseführer enthalten häufig hochauflösendes Fotomaterial, Romane hingegen nicht.</para>
                <para>Zur Überprüfung dieser Intuition gibt <xref linkend="table_filesize" /> einige statistische Werte zur Dateigröße für ausgewählte
                    Warengruppen aus unserem Korpus.</para>
                <para>Neben der Dateigröße als einem eher unspezifisch-diffusen Maß für multimediale Inhalte haben wir aus den Metadaten<footnote>
                    <para>Gemäß der EPUB-Spezifikation enthält das sogenannte <indexterm><primary>EPUB</primary><secondary>Manifest</secondary>
                    </indexterm>Manifest eine Auflistung aller im EPUB-Zip-Container enthaltenen 
                        Dateien inklusive einer Angabe des MIME-Typs für jede Datei. Ein Auszählen der Grafikdateien lässt sich somit über ein 
                        Zählen der Manifest-Einträge mit entsprechendem <glossterm linkend="gt_mimetyp">MIME-Typ</glossterm> bewerkstelligen.</para>
                </footnote> der EPUB-Datei auch die Anzahl der enthaltenen Grafikdateien ausgezählt und der Attributmenge hinzugefügt.</para>
                <!--<?dbfo-need height="6cm"?>-->
                <table frame="all" orient="port" pgwide="0" tocentry="1" xml:id="table_filesize"> <!-- Dateigrößenverteilung für ausgewählte Klassen -->
                    <?dbfo keep-together="always" ?>
                    <info>
                        <title>Dateigrößenverteilung für ausgewählte Klassen</title>
                    </info>
                    <tgroup cols="7" align="left" colsep="1" rowsep="1">
                        <colspec colname="warengruppe" colnum="1" colwidth="1*" align="left"/>
                        <colspec colname="instanzen" colnum="2" colwidth="1*" align="left"/>
                        <colspec colname="minimum" colnum="3" colwidth="1.5*" align="right"/>
                        <colspec colname="maximum" colnum="4" colwidth="1.5*" align="right"/>
                        <colspec colname="mean" colnum="5" colwidth="1.5*" align="right"/>
                        <colspec colname="stddev" colnum="6" colwidth="1.5*" align="right"/>
                        <colspec colname="bemerkung" colnum="7" colwidth="3*" align="left"/>
                        <thead>
                            <row>
                                <entry>Warengruppe</entry>
                                <entry>Anzahl Instanzen</entry>
                                <entry>Minimum</entry>
                                <entry>Maximum</entry>
                                <entry>Median</entry>
                                <entry>Standardabweichung</entry>
                                <entry>Bemerkung</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>112</entry>
                                <entry>778</entry>
                                <entry>173</entry>
                                <entry>35.944</entry>
                                <entry>2.105</entry>
                                <entry>2.270</entry>
                                <entry>Romane enthalten außer Cover und Werbeanzeigen nur selten weitere Grafiken</entry>
                            </row>
                            <row>
                                <entry>182</entry>
                                <entry>119</entry>
                                <entry>28.544</entry>
                                <entry>211.806</entry>
                                <entry>104.961</entry>
                                <entry>35.015</entry>
                                <entry><indexterm><primary>Comic</primary><secondary>Besonderheiten</secondary>
                                    </indexterm>Manga-Comics bestehen in der Regel ausschließlich aus Grafiken</entry>
                            </row>
                            <row>
                                <entry>221</entry>
                                <entry>34</entry>
                                <entry>7.505</entry>
                                <entry>153.683</entry>
                                <entry>28.240</entry>
                                <entry>35.030</entry>
                                <entry>Bilderbücher enthalten nicht nur Grafiken, sondern mitunter auch Audiodateien (Vorlesefunktion)</entry>
                            </row>
                            <row>
                                <entry>455</entry>
                                <entry>61</entry>
                                <entry>446</entry>
                                <entry>100.271</entry>
                                <entry>16.740</entry>
                                <entry>17.056</entry>
                                <entry>Kochbücher enthalten sowohl Text als auch Abbildungen</entry>
                            </row>
                            <row>
                                <entry>973</entry>
                                <entry>135</entry>
                                <entry>272</entry>
                                <entry>95.548</entry>
                                <entry>4.504</entry>
                                <entry>11.375</entry>
                                <entry>Bei Sachbüchern über Politik und Gesellschaft ist die Varianz groß</entry>
                            </row>
                        </tbody>
                    </tgroup>
                    <caption><para>Größenangaben in Kilobyte, gerundet. Datenbasis: Teilkorpus mit insgesamt 4380 Traininginstanzen</para></caption>
                </table>
                <para>Die makroskopische Struktur eines Textes lässt sich durch einen Blick auf die Kapitel erhellen. Neben Anzahl und Länge
                    der Kapitel ist auch die Schachtelung interessant, denn ein Kapitel kann Unterkapitel, Unter-Unterkapitel usw. enthalten.
                    In dieser Betrachtung ist es hilfreich, die Kapitel als Baumstruktur zu betrachten mit dem Dokument als Wurzel, in das
                    alle Kapitel erster Ordnung eingehängt sind. Unterkapitel bilden dann die Kindelemente des Kapitelknotens, in dem sie enthalten sind.</para>
                <para>Aus dem EPUB-Inhaltsverzeichnis <indexterm class="endofrange" startref="idt_011"/>(NCX-Datei in EPUB 2, Navigation Document in EPUB 3)
                    wurden drei Attribute ermittelt:</para>
                <itemizedlist spacing="compact">
                    <listitem>
                        <para>die Anzahl der Kapitel erster Ebene (Hauptkapitel, <emphasis>number of top level chapters</emphasis>)</para>
                    </listitem>
                    <listitem>
                        <para>die Anzahl aller Kapiteleinträge, unabhängig von ihrer Position im Kapitelbaum (<emphasis>number of TOC elements</emphasis>)</para>
                    </listitem>
                    <listitem>
                        <para>die maximale Kapiteltiefe (Ermittlung des am tiefsten geschachtelten Unterkapitels, dann Ermittlung der Pfadlänge 
                            zu diesem Kapitel im Kapitelbaum)</para>
                    </listitem>
                </itemizedlist>
                <para>Die Zahl der Kapitel erster Ebene lag dabei im Korpus zwischen 0 und 921, die Zahl aller Kapiteleinträge zwischen 0 und 1253
                    und die maximale Kapiteltiefe betrug 0 bis 6 Ebenen.</para>
            </sect2>
            <sect2 xml:id="ch_LexSemProp"><!-- Lexikalisch-semantische Eigenschaften -->
                <title>Lexikalisch-semantische Eigenschaften</title>
                <sect3 xml:id="ch_LexSemProp_Wörter"> <!-- Wörter, Tokens, Lemmata -->
                    <title>Wörter, Tokens, Lemmata</title>
                    <para>Intuitiv verstehen wir Wörter in der deutschen Sprache als Einheiten, welche im Schriftbild durch Leerstellen voneinander getrennt 
                    werden (vgl. <biblioref linkend="bib_Buß02"/>, S. 750)<footnote><para>Die Einschränkung auf die deutsche Sprache ist hier insofern
                    wichtig, als es a) Sprachen gibt, die Wortgrenzen nicht durch Leerzeichen markieren, wie z.B. das Chinesische und b) Sprachen gibt,
                    welche mehrere Wörter unter Verwendung anderer Trennkennzeichen zusammenziehen, z.B. der verkürzte Artikel vor Wörtern, die mit einem
                    Vokal beginnen im Französischen: l'après-midi statt *le après-midi. Auf eine weitere Betrachtung von Sonderfällen wird hier verzichtet.
                    Stattdessen nehmen wir eine leichte Unschärfe des <indexterm><primary>Wort</primary></indexterm>Wortbegriffs bewusst in Kauf.</para></footnote>. Deutsche Texte lassen sich auf Basis dieser Regel leicht algorithmisch in einzelne 
                    Wörter auflösen. In der Informatik nennt man diesen Zerlegungsvorgang die <glossterm linkend="gt_tokenisierung">Tokenisierung</glossterm>
                    eines Textes.<indexterm><primary>Tokenisierung</primary></indexterm></para>
                <para>Ohne im Rahmen dieser Arbeit den <indexterm><primary>Bedeutung</primary></indexterm>Bedeutungsbegriff linguistisch diskutieren 
                    zu können, stellen wir fest, dass nach Gottlob Freges <indexterm><primary>Kompositionalitätsprinzip</primary>
                    </indexterm>Kompositionalitätsprinzip die „Bedeutung eines komplexen 
                    [sprachlichen] Ausdrucks eine Funktion der Bedeutungen seiner Teile und der Art ihrer syntaktischen Kombination 
                    ist“ (<biblioref linkend="bib_Buß02"/>, S. 361, Lemma „Kompositionalitätsprinzip“). Wenn wir die zu klassifizierenden 
                    Texte in leicht abgrenzbare kleine Bedeutungseinheiten zerlegen, verlieren wir also einen Teil ihrer Bedeutung (nämlich den 
                    Teil, der sich erst durch die Kombination von Worten und Sätzen ergibt), erhalten jedoch statistisch leichter handhabbare 
                    Elemente, die immer noch mit der Textbedeutung in Beziehung stehen. Forschungen haben ergeben, dass dieses vereinfachende Vorgehen, die
                    Textbedeutung über Einzelworte zu erschließen, in der automatischen Textklassifikation gute Ergebnisse erzielt und elaboriertere Verfahren
                    wenig zur Steigerung der Effektivität eines Klassifikators beitragen können. Das Indizieren und statistische Aufbereiten von
                    Mehrwort-Phrasen<footnote><para>Die Fachliteratur spricht in diesem Zusammenhang häufig von n-gram-Modellen, mit n = der Anzahl der 
                    im Zusammenhang indizierten oder sonstwie verarbeiteten Elementen. Bei unserem Vorgehen der Indizierung von Einzelworten ist also n = 1.</para>
                    </footnote> ist nicht per se besser als die Konzentration auf einzelne Wörter und muss noch weiter im Detail erforscht werden
                    (<biblioref linkend="bib_Seb02"/>, S. 10-11).</para>
                <para>Die somit legitimierte Tokenisierung eines Textes liefert in einer flektierenden Sprache wie dem Deutschen zunächst eine Vielzahl
                    von Wortformen. Ist man primär an der Bedeutung der Wörter interessiert, bietet sich eine Rückführung der Worte auf ihre jeweilige 
                    Grundform an. Bei der Wahl einer passenden Grundform hat man im Wesentlichen drei verschiedene Möglichkeiten: Trunkierung, 
                    Wortstämme oder Lemmata. Bei der <indexterm><primary>Trunkierung</primary></indexterm>Trunkierung werden lediglich Wortendungen 
                    entfernt („[du] liest“ -> <emphasis>lies</emphasis>). Bei der Zerlegung in Wortstämme (<glossterm xml:id="gt_stemming"> Stemming</glossterm><indexterm>
                        <primary>Stemming</primary></indexterm>) werden nicht nur die Flexionsendungen entfernt, sondern es findet auch eine 
                    Rückführung des Stamms auf die Grundform statt (z.&#160;B. „[du] liest“-> <emphasis>les</emphasis>). Die <indexterm><primary>Lemmatisierung</primary>
                    </indexterm>Lemmatisierung arbeitet ähnlich wie das Stemming, gibt jedoch die Grundform so an, wie sie als Eintrag in einem 
                    Wörterbuch aufgeführt wird, bei Verben also den Infinitiv („[du] liest“ -> <emphasis>lesen</emphasis>). Die Trunkierung ist 
                    sichtlich schwächer als sie beiden anderen Verfahren. Einen äquivalenten Algorithmus vorausgesetzt, sind Stemming und 
                    Lemmatisierung hingegen gleich stark. Jedoch sind in der Praxis Algorithmen, die den Namen „Stemming“ tragen (z.B. der 
                    Porter-Stemming-Algorithmus<footnote><para>Für eine vollständige Beschreibung dieses Algorithmus für die englische Sprache siehe 
                        <biblioref linkend="bib_BR99"/>, S. 433-436</para></footnote>) oft schwächer, als solche, welche sich „Lemmatisierer“ nennen.</para>
                <para>Der Nutzen von Stemming bzw. Lemmatisierung ist unter Fachleuten der automatischen Textklassifikation zwar umstritten, wird in 
                    der großen Mehrzahl von Untersuchungen in diesem Bereich jedoch angewandt, da Stemmingtechnologien sowohl die Dimensionalität des
                    Attributraums als auch die stochastische Abhängigkeit zwischen Tokens reduzieren (<biblioref linkend="bib_Seb02"/>, S. 12).</para>
                 <para>Wir haben uns für eine Analyse der Wörter unserer E-Book-Texte mit Hilfe der Lemmatisierungsfunktion des TreeTaggers<footnote>
                        <para><link xlink:href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"/> (Zugriff 11.2.2018).</para></footnote> 
                    entschieden, weil dieser bereits ein trainiertes Modell für die deutsche Sprache anbietet, wohingegen es kein frei verfügbares 
                    deutsches Modell für den Lemmatisierer aus dem <indexterm><primary>Apache openNLP</primary>
                    </indexterm>Apache-openNLP-Projekt gibt<footnote><para><link
                        xlink:href="https://opennlp.apache.org/docs/1.8.1/manual/opennlp.html#tools.lemmatizer"/> (Zugriff 5.2.2018). Da wir bereits für
                    Satzzerlegung, Tokenisierung und Wortartenerkennung ApacheNLP verwendet haben, wäre eine Einsatz des OpenNLP-Lemmatisierers 
                    wünschenswert gewesen, um die technische Abhängigkeit von verschiedenen Projekten und Softwareprodukten gering zu halten.</para></footnote>. Um später evaluieren zu können, welchen Einfluss eine falsche Lemmatisierung auf die
                    automatische Klassifikation der E-Books hat, wurde die Möglichkeit einer nachträglichen Lemma-Korrektur der TreeTagger-Ergebnisse
                    eingerichtet, welche per Kommandozeilenparameter ein- und ausgeschaltet werden kann. Diese Korrekturmöglichkeit basiert auf manuell
                    gepflegten Ersetzungslisten.</para>
                <para>Wie im Kapitel <xref linkend="ch_WGSdDB"/> erläutert, beziehen sich nicht alle unsere Warengruppen-Klassen auf die Bedeutung 
                    eines Textes, sondern mitunter auch auf Aspekte der Textentstehung (vor 1945 / nach 1945), der Zielgruppe (Erwachsene / Jugendliche, Fachleute / Laien) oder der 
                    Wirkungsart (ein Ratgeber leitet zu einer Handlung an, wohingegen ein Sachbuch informiert). Aus linguistischer Sicht haben wir
                    es hier mit einer Vermischung von <glossterm linkend="gt_semantik">Semantik</glossterm> als der Wissenschaft der Bedeutung von 
                    Ausdrücken innerhalb eines Sprachsystems und der <glossterm linkend="gt_pragmatik">Pragmatik</glossterm> als Wissenschaft der 
                    Bedeutungen von Äußerungen innerhalb bestimmter Sprechsituationen zu tun (<biblioref linkend="bib_Buß02"/>, S. 534, Lemma 
                    „Pragmatik“). Jedoch wird die statistische Aufbereitung von Worthäufigkeiten auch in der computergestützten 
                    Stilanalyse von Texten verwendet (<biblioref linkend="bib_All89"/>, S. 548) und es ist zumindest für einige der oben genannten
                    pragmatischen Unterscheidungen wahrscheinlich, dass sie sich auf stilistischer Ebene der Texte unterscheiden.</para>
                <para>Wir erwarten also von einer Analyse des Wortbestands der zu klassifizierenden E-Books, dass sie uns sowohl Hinweise auf die
                    Bedeutung als auch auf den Schreibstil der Texte gibt und somit die Klassifikation auf zwei Ebenen unterstützt. Es sei an dieser Stelle
                    aber auch darauf hingewiesen, dass es computerlinguistische Verfahren gibt, welche die Bedeutung eines Textes detaillierter und genauer 
                    erfassen können als eine reine Wort- und Lemma-Analyse. Denkbar sind etwa eine Anreicherung der Analyse durch Ermittlung von festen Fügungen, welche 
                    aus mehreren Wörtern bestehen, das Auszeichnen von Wörtern hinsichtlich morphologischer Eigenschaften oder die Gewichtung von Wörtern aufgrund 
                    ihrer Zugehörigkeit zu semantischen Typen (und nicht der Gewichtung auf Basis der Vorkommenshäufigkeit von Lemmata, wie hier vorgenommen). Die
                    hier genannten Beispiele stammen sämtlich aus <biblioref linkend="bib_NS03"/>, welche prognostisch behaupten, dass sich regelbasierte, 
                    „informationslinguistische“ Verfahren der automatischen Indexierung und Klassifikation gegenüber statistisch fundierten Verfahren als überlegen
                    erweisen werden. Der Vorteil des hier angewandten rein statistischen Vorgehens ist hingegen, dass er unabhängig von der Existenz ausgefeilt 
                    vorformulierter Wörterbücher, Thesauri und Regelsammlungen ist.</para>
                </sect3>
                <sect3 xml:id="ch_LexSemProp_Wortvektor"> <!-- Wortvektor und TF/IDF -->
                    <title>Wortvektor und TF/IDF</title>
                    <para>Formal können wir das Ergebnis unserer Textzerlegung in einzelne lemmatisierte Wörter als einen Wortvektor auffassen (auch Bag-of-Words-Modell oder
                    Unigram-Modell, vgl. <biblioref linkend="bib_RN16"/>, S. 866). Es sei d<subscript>i</subscript> ein E-Book-Dokument aus dem Korpus 
                    D mit 0 &lt; i ≤ |D| und das Vokabular W die Menge aller im gesamten Korpus vorkommenden Wörter (oder Lemmata). Die Funktion 
                    freq(w<subscript>j</subscript>) mit <inlineequation><mml:math><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:math>
                    </inlineequation> gibt die Anzahl des Vorkommens von Lemma w<subscript>j</subscript> im Dokument d<subscript>i</subscript> an. Dann ist der Wortvektor 
                    <inlineequation><mml:math display="inline"><mml:mrow><mml:mover><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>d</mml:mi>
                        <mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="true">⃗</mml:mo></mml:mover></mml:mrow></mml:math>
                    </inlineequation> von d<subscript>i</subscript> definiert als:</para>
                    <equation xml:id="formula_Wortvektor"> <!-- Formel Wortvektor -->
                        <mml:math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <mml:mrow>
                            <mml:mrow>
                                <mml:mover accent="true">
                                    <mml:msub>
                                        <mml:mi>v</mml:mi>
                                        <mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub>
                                    </mml:msub>
                                    <mml:mo stretchy="true">⃗</mml:mo>
                                </mml:mover>
                                <mml:mo stretchy="false">=</mml:mo>
                                <mml:mrow>
                                    <mml:mo fence="true" stretchy="true">(</mml:mo>
                                    <mml:mrow>
                                        <mml:mtable>
                                            <mml:mtr>
                                                <mml:mtd>
                                                    <mml:mrow>
                                                        <mml:mi mathvariant="italic">freq</mml:mi>
                                                        <mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">(</mml:mo>
                                                            <mml:mrow>
                                                                <mml:msub>
                                                                    <mml:mi>w</mml:mi>
                                                                    <mml:mn>1</mml:mn>
                                                                </mml:msub>
                                                            </mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">)</mml:mo>
                                                        </mml:mrow>
                                                    </mml:mrow>
                                                </mml:mtd>
                                            </mml:mtr>
                                            <mml:mtr>
                                                <mml:mtd>
                                                    <mml:mrow>
                                                        <mml:mi mathvariant="italic">freq</mml:mi>
                                                        <mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">(</mml:mo>
                                                            <mml:mrow>
                                                                <mml:msub>
                                                                    <mml:mi>w</mml:mi>
                                                                    <mml:mn>2</mml:mn>
                                                                </mml:msub>
                                                            </mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">)</mml:mo>
                                                        </mml:mrow>
                                                    </mml:mrow>
                                                </mml:mtd>
                                            </mml:mtr>
                                            <mml:mtr>
                                                <mml:mtd>
                                                    <mml:mn>...</mml:mn>
                                                </mml:mtd>
                                            </mml:mtr>
                                            <mml:mtr>
                                                <mml:mtd>
                                                    <mml:mrow>
                                                        <mml:mi mathvariant="italic">freq</mml:mi>
                                                        <mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">(</mml:mo>
                                                            <mml:mrow>
                                                                <mml:msub>
                                                                    <mml:mi>w</mml:mi>
                                                                    <mml:mi>n</mml:mi>
                                                                </mml:msub>
                                                            </mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">)</mml:mo>
                                                        </mml:mrow>
                                                    </mml:mrow>
                                                </mml:mtd>
                                            </mml:mtr>
                                        </mml:mtable>
                                    </mml:mrow>
                                    <mml:mo fence="true" stretchy="true">)</mml:mo>
                                </mml:mrow>
                            </mml:mrow>
                            <mml:mi>,&#160;</mml:mi>
                            <mml:mi>mit&#160;</mml:mi>
                            <mml:mrow>
                                <mml:mi mathvariant="italic">n&#160;</mml:mi>
                                <mml:mo stretchy="false">=&#160;</mml:mo>
                                <mml:mrow>
                                    <mml:mo fence="true" stretchy="false">|</mml:mo>
                                    <mml:mrow>
                                        <mml:mi>W</mml:mi>
                                    </mml:mrow>
                                    <mml:mo fence="true" stretchy="false">|</mml:mo>
                                </mml:mrow>
                            </mml:mrow>
                        </mml:mrow>
                    </mml:math> 
                    </equation>
                    <para>Statt der Funktion freq(w) kann alternativ auch contains(w) eingesetzt werden, deren
                        Wertebereich nicht die natürlichen Zahlen sind, sondern lediglich {0, 1} ist.</para>
                    <para>Die Mächtigkeit des Vokabulars W wird trotz der Lemmatisierung sehr groß. Sie lässt sich mittels des Heaps’schen Gesetzes 
                       abschätzen, welches besagt: <inlineequation><mathphrase>|W| = Kn<superscript>&#946;</superscript></mathphrase></inlineequation>.
                       Dabei ist n die Gesamtzahl der Wörter des Textes und die Parameter K und &#946; sind von Sprache und Textsorte abhängige 
                       Variablen. Da für gewöhnlich gilt: <inlineequation><mathphrase>0,4 &lt; &#946; &lt; 0,6</mathphrase></inlineequation>, wächst das
                       Vokabular sublinear mit der Textgröße (<biblioref linkend="bib_BR99"/>, S. 147)<footnote><para><biblioref linkend="bib_BLS14"/> zeigen
                           allerdings, dass das Heaps’sche Gesetz nicht mehr gilt bei sehr großen Korpora, deren Texte eine große Zeitspanne der Textentstehung
                       abdecken.</para></footnote>. Einige Besonderheiten bei der Zusammenstellung unseres Korpus sorgen dabei für ein besonders 
                       starkes Ansteigen von |W|:</para>
                    <itemizedlist spacing="compact">
                       <listitem>
                           <para>einige der Texte sind älteren Entstehungsdatums, so dass historische Wörter oder Wortformen in das Vokabular gelangen</para>
                       </listitem>
                       <listitem>
                           <para>das Korpus enthält medizinische Fachliteratur, welche über eine besonders hohe Zahl von Fachbegriffen verfügt</para>
                       </listitem>
                       <listitem>
                           <para>literarisch-künstlerische Texte zeichnen sich durch spielerischen Umgang mit Sprachmaterial aus, was Wortneuschöpfungen beinhaltet</para>
                       </listitem>
                       <listitem>
                           <para>insbesondere die Warengruppen 161, 162, 292, 862 und 914 enthalten sehr viel fremdsprachliches Material, so dass dem 
                               Korpusvokabular nicht nur deutsche Wörter und im Deutschen übliche Fremdwörter, sondern auch viele weitere fremdsprachige
                               Wörter eingefügt werden.</para>
                       </listitem>
                       <listitem>
                           <para>Dem TreeTagger-Lemmatisierer gelingt nicht in allen Fällen die Reduktion eines Wortes auf das zugehörige Lemma, so dass
                               trotz gegenteiliger Intention unterschiedliche Formen desselben Wortes im Vokabular koexistieren.</para>
                       </listitem>
                   </itemizedlist>
                   <para>Während die Anzahl der Attribute, welche unsere E-Books beschreiben, im Falle von <xref linkend="ch_FileProp"/> und 
                       <xref linkend="ch_GramProp"/> überschaubar ist, würde eine ungefilterte Verwendung des oben definierten Wortvektors 
                       <inlineequation><mml:math display="inline"><mml:mrow><mml:mover><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>d</mml:mi>
                           <mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="true">⃗</mml:mo></mml:mover></mml:mrow></mml:math>
                       </inlineequation> die Attributmenge auf eine Kardinalität von mehreren Millionen anwachsen lassen. Daraus würden sich praktische
                       Berechnungsprobleme ergeben, denn bei den von uns gewählten Verfahren müssen die Datensätze mitsamt ihrer Attribute im
                       Arbeitsspeicher gehalten werden. Mit der Größe der Attributmenge steigen die Anforderungen an die RAM-Kapazität der verwendeten
                       Rechner und die Dauer der Trainingsphase steigt mitunter in nicht mehr vertretbarem Maß. Uns ist aus diesem Grund daran 
                       gelegen, die Länge des Wortvektors zu verringern, ohne dabei seine Unterscheidungskraft bezüglich der zu lernenden Klassen 
                       zu verlieren. Um die große Wortmenge in unserem Korpus besser handhabbar zu machen, wird zunächst ein Index aller Wörter erstellt, 
                       eine Technik, die die Textklassifikationsgemeinde aus dem Forschungsfeld <indexterm><primary>Information Retrieval</primary>
                       </indexterm><glossterm
                           linkend="gt_information_retrieval">Information Retrieval</glossterm> übernommen hat (<biblioref linkend="bib_Seb02"/>, S. 10).</para>
                   <para>Es ist unmittelbar einsichtig, dass hochfrequente Funktionswörter wie die Artikel <emphasis>der, die, das</emphasis> oder 
                       Konjunktionen wie <emphasis>und, dass</emphasis> wenig Unterscheidungskraft bezüglich Textklassen haben. Solche Wörter werden in 
                       <indexterm><primary>Stopword</primary></indexterm>Stopwordlisten gesammelt und aus dem Wortvektor ausgeschlossen. Wir verwenden hierfür die deutsche 
                       Default-Stopwordliste der Java-Suchindexbibliothek <indexterm><primary>Lucene-Index</primary></indexterm>Lucene, welche in Lucene-Version 6.6
                       genau 231 Wörter enthält<footnote><para>Javadoc für die verwendete Lucene-Klasse: <link
                           xlink:href="https://lucene.apache.org/core/6_6_0/analyzers-common/org/apache/lucene/analysis/de/GermanAnalyzer.html"/>
                           (Zugriff 5.2.2018). Die Default-Stopwordliste des GermanAnalyzers enthält das Wort „daß“ nur in alter Rechtschreibung, so dass wir es auch in der 
                           Schreibweise „dass“ hinzugefügt haben. Außerdem noch die drei nicht enthaltenen Wörter „schon“, „mehr“ und das für unser Korpus spezifische Wort 
                           „cover“.</para></footnote>.</para>
                    <para>Auf dem Weg zu einem für das gesamte Korpus geltenden Wortvektor wird als Zwischenschritt eine dokumentspezifische Wortliste eingeführt. Pro Dokument
                        werden bis zu x Lemmata auf diese Liste gesetzt, wobei x ein Parameter ist, dessen Wert im Laufe der Untersuchungen zwischen 250 und 3000 schwankte &#x2012;
                        eine Evaluation des idealen x-Wertes wird in <xref linkend="ch_AttribEval_StringToWordVector"/> vorgenommen. Auf dem Weg vom Lucene-Index zur Wortliste
                        werden zunächst weitere Lemmata ausgeschlossen. Während Stopwords gar nicht erst in den Lucene-Index aufgenommen werden, gilt es nun, Schwellen für weitere 
                        für die Klassifikation unergiebige Wörter zu definieren.</para>
                   <para>Es ist leicht einsehbar, dass Unikate, also Wörter, die korpusübergeifend nur in einem einzigen Dokument vorkommen, keinen Beitrag zur
                       Klassifikation leisten können, zumindest dann, wenn es keine Klassen gibt, in deren Trainingsmenge sich nur ein 
                       einzelnes Textdokument befindet. In einem Bestand von <numberOfLuceneDocuments/> Dokumenten mit insgesamt <numberOfLuceneTerms/>
                       unterschiedlichen Lemmata betrug die Zahl der Lemmata, welche nur in einem Dokument vorkommen, <numberOfUniqueLuceneTerms/>.
                       Ein Ausschluss dieser Lexeme aus der Wortliste kann also die Menge der für den endgültigen Wortvektor zu prüfenden Wörter um mehr als die Hälfte reduzieren.
                       Um eine noch ideale Kappung am oberen (Stopwords) und unteren Ende (einzeln auftretende Wörter) zu erreichen, wurde statt mit absoluten Zahlen mit folgenden 
                       Formeln gearbeitet, welche mit der Korpusgröße moderat wachsende Schranken zum Ausschluss von für Klassifikationszwecke uninteressante Wörter bieten. |D|
                       bedeutet hier wieder die Anzahl der Dokumente im Korpus.</para>
                   <equation>
                       <mathphrase>s<subscript>u</subscript> = 2 + log<subscript>10</subscript> |D|</mathphrase>
                   </equation>
                   <equation>
                       <mathphrase>s<subscript>o</subscript> = |D| - 2 · log<subscript>10</subscript> |D|</mathphrase>
                   </equation>
                   <para>Worte, deren Frequenz unterhalb der unteren Schranke s<subscript>u</subscript> oder oberhalb der oberen Schranke
                       s<subscript>o</subscript> liegt, wurden von einer Aufnahme in die Wortliste ausgeschlossen. Gegenüber einer fixen unteren Schranke bietet
                       diese formelbasierte Variante den Vorteil, dass eine gewisse
                       Resistenz gegenüber <indexterm><primary>Duplikate</primary></indexterm> Duplikaten und Sammelbänden<footnote><para>Zur Definition 
                           von <emphasis>Duplikat</emphasis> und <emphasis>Sammelband</emphasis> siehe <xref linkend="ch_Korpus"/>.</para></footnote> in 
                       den Trainingsdaten erreicht wird: Sollte ein E-Book, das ein seltenes Wort enthält, mehrmals in den Trainingsdaten vorkommen, wird 
                       das seltene Wort eine niedrige absolute Schranke leichter überspringen als eine mit dem Korpus wachsende Schranke, denn mit 
                       wachsender Korpusgröße nimmt auch die Wahrscheinlichkeit des Auftretens von Duplikaten zu.</para>
                   <para>Die Verwendung einer solchen unteren Schranke im Wortvektor hilft nicht nur bei der Reduktion der Attributzahl und somit 
                       der Verringerung der Berechnungslast auf einem Computer, sondern kann auch einen Beitrag zur Vermeidung von <indexterm><primary>
                           Überanpassung</primary></indexterm><glossterm linkend="gt_überanpassung">Überanpassung</glossterm> leisten. Bei kleinen Klassen mit nur 
                       geringer Anzahl an Trainingsdaten könnte sich die Klassifikation auf ein zufällig in dieser kleinen Menge häufig vorkommendes Wort stützen, obwohl 
                       dieses Wort in einer größeren Trainingsmenge für dieselbe Klasse gar nicht mehr so häufig vorkäme.</para>
                   <para>Für die obere Schranke wurde zunächst eine prozentuale Angabe von 90% verwendet, d. h. Wörter, die in mehr als 90% aller
                       Dokumente auftreten, wurden ausgeschlossen. Ohne dies experimentell zu verifizieren, schien es aber so, dass dadurch bei 
                       wachsendem Korpus auch interessant erscheinende Wörter verloren gingen. Aus diesem Grund wurde auch hier auf eine logarithmische
                       Berechnung der Schranke gesetzt.</para>
                   <para>Aus allen Lemmata, welche die genannten Schranken überwinden, werden dann die erwähnten x wichtigsten pro Dokument aufgelistet. Unser Begriff der Wichtigkeit
                       definiert sich mit Hilfe des <indexterm><primary>TF/IDF</primary></indexterm>TF/IDF-Wertes jedes einzelnen Wortes im Dokument. Wichtig sind dann jeweils 
                       pro Dokument die x Worte mit dem höchtsen TF/IDF-Score.</para>
                   <para>TF/IDF ist ein im <indexterm><primary>Information Retrieval</primary></indexterm>Information Retrieval etabliertes Verfahren, die Relevanz von
                       Dokumenten bezüglich einer Suchanfrage zu 
                       ermitteln (<biblioref linkend="bib_BR99"/>, S. 29-30; <biblioref linkend="bib_KW14"/>, S. 194-197). TF steht für „term frequency“, 
                       also die Zahl, wie häufig ein betrachtetes Wort w in einem gegebenen Dokument d vorkommt. IDF bedeutet „inverse document frequency“ und
                       bildet die Häufigkeit des Wortes w im gesamten Dokumentenbestand D auf eine reelle Zahl ab, die umso größer ist, je seltener
                       das Wort ist. Mit docFreq(w) gleich der Anzahl der Dokumente, in denen das Wort <emphasis>w</emphasis> vorkommt, gilt:</para>
                   <equation xml:id="formula_idf"> <!-- IDF-Formel -->
                       <mml:math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                           <mml:mrow>
                               <mml:mrow>
                                   <mml:mrow>
                                       <mml:mi mathvariant="italic">idf</mml:mi>
                                       <mml:mo fence="true" stretchy="false">(</mml:mo>
                                       <mml:mrow>
                                           <mml:mi>w</mml:mi>
                                       </mml:mrow>
                                       <mml:mo fence="true" stretchy="false">)</mml:mo>
                                   </mml:mrow>
                                   <mml:mo stretchy="false">=</mml:mo>
                                   <mml:mn>1</mml:mn>
                                   <mml:mo stretchy="false">+</mml:mo>
                                   <mml:mi>ln</mml:mi>
                               </mml:mrow>
                               <mml:mrow>
                                   <mml:mo fence="true" stretchy="true">(</mml:mo>
                                   <mml:mrow>
                                       <mml:mfrac>
                                           <mml:mrow>
                                               <mml:mtext>|</mml:mtext>
                                               <mml:mi>D</mml:mi>
                                               <mml:mtext>|</mml:mtext>
                                           </mml:mrow>
                                           <mml:mrow>
                                               <mml:mn>1</mml:mn>
                                               <mml:mo>+</mml:mo>
                                               <mml:mi mathvariant="italic">docFreq</mml:mi>
                                               <mml:mrow>
                                                   <mml:mo fence="true" stretchy="false">(</mml:mo>
                                                   <mml:mrow>
                                                       <mml:mi>w</mml:mi>
                                                   </mml:mrow>
                                                   <mml:mo fence="true" stretchy="false">)</mml:mo>
                                               </mml:mrow>
                                           </mml:mrow>
                                       </mml:mfrac>
                                   </mml:mrow>
                                   <mml:mo fence="true" stretchy="true">)</mml:mo>
                               </mml:mrow>
                           </mml:mrow>
                       </mml:math>
                   </equation>
                    <para>Sowohl die Suchindexbibliothek <indexterm><primary>Lucene-Index</primary></indexterm><glossterm linkend="gt_lucene">Lucene</glossterm> als auch unser
                       Vorverarbeitungsprogramm <glossterm linkend="gt_avve">Avve</glossterm> fügen dem TF/IDF-Scoringwert noch einen 
                       Normalisierungsfaktor hinzu, der von der Dokumentlänge abhängt. Für die Art und Weise, wie wir das TF/IDF-Scoring verwenden,
                       nämlich zur Extraktion eines Wortvektors der x wichtigsten Wörter pro Dokument, wäre der Normalisierungsfaktor nicht nötig 
                       gewesen. Wichtig wird ein solcher Faktor erst dann, wenn man dokumentübergreifende Vergleiche von Wort- oder Phrasenscores
                       anstellen möchte. Jedenfalls lautet die gesamte Formel zur Berechnung der wichtigsten Wörter bzw. Lemmata pro Dokument, mit 
                       numTerms(d) gleich der Anzahl der Worte in Dokument <emphasis>d</emphasis>:</para>
                   <equation xml:id="formula_importancescore"> <!-- Importancescore-Formel -->
                       <mml:math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                           <mml:mrow>
                               <mml:mi mathvariant="italic">importancescore</mml:mi>
                               <mml:mo fence="true" stretchy="false">(</mml:mo>
                               <mml:mrow>
                                   <mml:mi>w</mml:mi>
                                   <mml:mtext>,</mml:mtext>
                                   <mml:mi>d</mml:mi>
                               </mml:mrow>
                               <mml:mo fence="true" stretchy="false">)</mml:mo>
                           </mml:mrow>
                           <mml:mo stretchy="false">=</mml:mo>
                           <mml:row>
                               <mml:mfrac>
                                   <mml:mrow>
                                       <mml:msqrt>
                                           <mml:mi mathvariant="italic">tf</mml:mi>
                                           <mml:mo fence="true" stretchy="false">(</mml:mo>
                                           <mml:mrow>
                                               <mml:mi>w</mml:mi>
                                           </mml:mrow>
                                           <mml:mo fence="true" stretchy="false">)</mml:mo>
                                       </mml:msqrt>
                                       <mml:mo>·</mml:mo>
                                       <mml:mi mathvariant="italic">idf</mml:mi>
                                       <mml:mo fence="true" stretchy="false">(</mml:mo>
                                       <mml:mrow>
                                           <mml:mi>w</mml:mi>
                                       </mml:mrow>
                                       <mml:mo fence="true" stretchy="false">)</mml:mo>
                                   </mml:mrow>
                                   <mml:mrow>
                                       <mml:msqrt>
                                           <mml:mi mathvariant="italic">numTerms</mml:mi>
                                           <mml:mrow>
                                               <mml:mo fence="true" stretchy="false">(</mml:mo>
                                               <mml:mrow>
                                                   <mml:mi>d</mml:mi>
                                               </mml:mrow>
                                               <mml:mo fence="true" stretchy="false">)</mml:mo>
                                           </mml:mrow>
                                       </mml:msqrt>
                                   </mml:mrow>
                               </mml:mfrac>
                           </mml:row>
                       </mml:math>
                   </equation>
                   <para>Mit dem angeführten Verarbeitungsschritt des Rankings der einzelnen Wörter haben wir pro Dokument eine neue Wortliste erzeugt, 
                       die vom Ausgangstext des E-Books in vieler Hinsicht abstrahiert. In technischer Hinsicht stellt diese Wortliste jedoch noch 
                       keinen Wortvektor dar, da diese Wortliste jeweils eine Eigenschaft eines einzelnes E-Books und nicht des ganzen Korpus ist. Eine 
                       genauere Schilderung der Verarbeitungsschritte in ihrer Ausführungsreihenfolge gibt <xref linkend="ch_Avvelauf"/>, doch muss an dieser 
                       Stelle aus systematischen Gründen bereits erwähnt werden, dass aus diesen gerankten Wortlisten für die Verarbeitung in Weka ein 
                       Wortvektor erstellt werden muss, wobei weitere Schwellenwerte greifen und die Größe des erzeugten Wortvektors weiter beschränkt
                       werden kann.</para>
                    <para>Dieser letzte Vorverarbeitungsschritt zum endgültigen Wortvektor <inlineequation><mml:math display="inline"><mml:mrow><mml:mover><mml:msub>
                        <mml:mi>v</mml:mi><mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mi></mml:msub><mml:mo stretchy="true">⃗</mml:mo></mml:mover>
                    </mml:mrow></mml:math></inlineequation> besteht in der Nutzung der Weka-Funktion <code>StringToWordVector</code>,
                       die aus allen gerankten Wortlisten im Trainings- und Testkorpus einen gemeinsamen Wortvektor erstellt und diesen zugleich in Attribute
                       der Trainings- und Testinstanzen umwandelt. <code>StringToWordVektor</code> arbeitet in der Standardkonfiguration unter Zurkenntnisnahme
                       der den Instanzen jeweils zugeordneten Klasse. Pro Klasse werden nur höchstens 1000 Wörter in den Vektor aufgenommen<footnote><para>Bei der
                           Zahl 1000 handelt es sich um einen Defaultwert von <code>StringToWordVektor</code>. Mit einem Überschreiben dieses Defaultwertes
                           wurde im Rahmen der vorliegenden Arbeit nicht experimentiert.</para></footnote>, und zwar jeweils die in dieser Klasse höchstfrequenten.
                       Darüber hinaus lässt sich eine <emphasis>minimum term frequency</emphasis> festlegen, die sich ebenfalls auf die Klassenzugehörigkeit 
                       bezieht: Setzt man diese <emphasis>minimum term frequency</emphasis> beispielsweise auf den Wert 2, so gelangen nur solche Wörter in 
                       den endgültigen Wortvektor, welche in mindestens einer Klasse mindestens zwei Mal vorkommen. Eine Evaluation der beiden Parameter 
                        <emphasis>x wichtigste Wörter pro Klasse</emphasis> und <emphasis>minimum term frequency pro Klasse</emphasis> wird in <xref linkend="ch_AttribEval_StringToWordVector"/>
                       gegeben.</para>
                </sect3>
                <sect3 xml:id="ch_LexSemProp_Hyperonym"> <!-- Hyperonyme -->
                    <title>Oberbegriffe (Hyperonyme)</title>
                    <para>In den vorangehenden Abschnitten haben wir unser Vorgehen mit Hilfe eines Bag-of-Words-Modells legitimiert &#x2012; und werden damit auch
                        gute Klassifikationsergebnisse erzielen, wie in den Kapiteln 4 bis 6 gezeigt wird. Das bedeutet dann aber auch, dass eine Klassifikation lediglich 
                        auf Basis der manifesten Terminologie, nicht jedoch auf Basis der im Text besprochenen Konzepte erfolgt. Problematisch kann an diesem traditionellen
                        Vorgehen sein, dass </para>
                    <orderedlist spacing="compact">
                        <listitem>
                            <para>zusammengehörige Wörter wie z.B. „Europäische Union“ getrennt betrachtet und damit in ihrer Bedeutung verfälscht werden</para>
                        </listitem>
                        <listitem>
                            <para>synonyme Wörter als unterschiedliche Texteigenschaften bewertet werden</para>
                        </listitem>
                        <listitem>
                            <para>Polyseme als ein Wort betrachtet werden</para>
                        </listitem>
                        <listitem>
                            <para>keine Verallgemeinerung erfolgt (z.B. „Rindfleisch“ und „Schweinefleisch“ nicht auf „Fleisch“ abgebildet werden).</para>
                        </listitem>
                    </orderedlist>
                    <para><biblioref linkend="bib_BH06"/> haben untersucht, wie man <indexterm><primary>Ontologie</primary></indexterm>Ontologien einsetzen kann, um den 
                        genannten Problemen in der Textklassifikation durch Maschinenlernen zu begegnen. Ontologien bestehen aus einer Menge von Konzeptknoten, welche in 
                        einer Subkonzept/Superkonzept-Beziehung zueinander stehen. Uns interessiert dabei vor allem die Auflösung von Oberbegriffen (<indexterm>
                            <primary>Hyperonym</primary></indexterm>Hyperonymen), um von konkreten Wörtern unserer Texte zu allgemeineren Begriffen zu kommen. Die Idee
                        besteht also darin, zu gegebenen Wörtern unserer Texte zunächst zu ermitteln, zu welchem Konzept ein konkretes Wort gehört und anschließend das
                        oder die Superkonzept(e) zu ermitteln. Wir folgen dabei dem von <biblioref linkend="bib_BH06"/> vorgeschlagenen Algorithmus, den wir aber an
                        einigen Stellen vereinfacht haben:</para>
                    <orderedlist spacing="compact">
                        <listitem>
                            <para>Auswahl von Termkandidaten: Da auch aus mehreren Wörtern bestehende Terme behandelt werden sollen, wird ein Abgleich von Phrasen mit einem
                                Lexikon durchgeführt und die jeweils längsten Übereinstimmungen als Kandidaten ausgewählt. Somit erhofft man sich, die jeweils spezifischsten
                                Konzepte herausarbeiten zu können. Als Algorithmus wird ein Fenster über den Text geschoben (bei <biblioref linkend="bib_BH06"/> ist die
                                ursprüngliche Fenstergröße 4, in der vorliegenden Untersuchung hingegen drei, da unsere Ontologie kaum viergliedrige Verbindungen enthält),
                                wobei das Fenster entweder um 1 verkleinert oder um eine Position weitergeschoben wird, wenn kein Treffer im Lexikon erzielt wurde.</para>
                        </listitem>
                        <listitem>
                            <para>Bei <biblioref linkend="bib_BH06"/> werden nur Terme aus Substantivphrasen gegen das Lexikon verglichen, um die Anzahl der notwendigen
                                Vergleiche gering zu halten. Da unsere Ontologie auch in größerem Umfang Verben enthält, haben wir in der vorliegenden Untersuchung auf diese
                                Einschränkung verzichtet.</para>
                        </listitem>
                        <listitem>
                            <para>Ergibt die Lexikonabfrage bei <biblioref linkend="bib_BH06"/> für die gerade zu untersuchende Phrase keinen Treffer, wird eine zweite Abfrage mit
                                der gestemmten Form derselben Phrase durchgeführt. Auf diesen Schritt wurde in der vorliegenden Studie verzichtet, um die Berechnungsdauer in akzeptablem
                                Rahmen zu halten. Denn auch wenn wir eine zweite Abfrage mit den ermittelten Lemmata durchgeführt hätten, wären Phrasen wie „Europäische Union“
                                nicht gefunden worden, denn die lemmatisierte Form lautet „Europäisch Union“.</para>
                        </listitem>
                        <listitem>
                            <para>Weder <biblioref linkend="bib_BH06"/> noch die vorliegende Studie streben eine Disambiguierung von Wörtern oder Phrasen aus dem Kontext heraus 
                                an. Wäre eine Ermittlung des jeweils passenden Konzepts angestrebt, so müsste sie an dieser Stelle erfolgen.</para>
                        </listitem>
                        <listitem>
                            <para>Die gefundenen Konzepte werden nun verallgemeinert, indem die Oberbegriffe eines Konzepts bis zu einer festgesetzten Stufe in den Attributvektor
                                aufgenommen und mitverwendet werden. Wir haben nur jeweils die Oberbegriffe der nächsthöheren Stufe verwendet und sind nicht rekursiv zu 
                                weiter höher gelegenen Konzeptstufen aufgestiegen. Unser verwendeter Thesaurus bot als weiteres Feature neben den Konzepten noch eine so
                                genannte Kategorie. Jedes Konzept (Synset) kann zu einer oder mehreren von 32 sehr allgemeinen Kategorien, wie z.&#160;B. „Physik“, „Medizin“ oder
                                „Botanik“ gehören. Während der Suche nach Oberbegriffen haben wir zusätzlich gezählt, welche dieser Kategorien wie häufig in einem Text
                                aufgerufen wurde.</para>
                        </listitem>
                    </orderedlist>
                    <para><biblioref linkend="bib_BH06"/> erzielten die besten Maschinenlern-Ergebnisse, wenn sie einen Attributvektor verwendeten, der sowohl 
                        term-basierte als auch konzept-basierten Attribute enthielt. Die Verbesserungen durch die Verwendung von Konzepten war in mehr als
                        der Hälfte der Fälle statistisch signifikant. Von der Zugabe der Konzepte konnten vor allem kleinere Klassen profitieren.</para>
                    <para>Die Studie von <biblioref linkend="bib_BH06"/> arbeitete mit englischsprachigen Texten und setzte als Ontologie WordNet der Princeton University
                        ein. Eine deutsche Variante namens GermaNet wird an der Universität Tübingen gepflegt<footnote><para><link xlink:href="http://www.sfs.uni-tuebingen.de/GermaNet/index.shtml"/>
                            (Zugriff 11.03.2018).</para></footnote>, konnte aber aufgrund restriktiver Lizenzbedingungen in der vorliegenden Studie nicht genutzt werden.
                        Stattdessen kam der OpenThesaurus zum Einsatz<footnote><para><link xlink:href="https://www.openthesaurus.de/about/download"/> (Zugriff am 11.03.2018).
                        </para></footnote>, der ca. 40.000 Konzepte (Synsets) und knapp 160.000 Wörter und Wortgruppen enthält.</para>
                    <para>Da sich die Berechnungsdauer der E-Book-Aufbereitung für unser Korpus von knapp 7.000 E-Books durch Hinzufügung der Oberbegriffe mehr als 
                        verdreifachte (von ca. 30 auf über 100 Stunden), wurde dieser Vorverarbeitungsschritt nicht in jedem neuen Versuchslauf mit durchgeführt, sondern
                        wurde über einen Kommanozeilenparameter an- oder ausgeschaltet. Inwiefern die Oberbegriffe zu einer Verbesserung der Klassifikation beitrugen,
                        wird in den Kapiteln 4 und 5 bei den jeweiligen Maschinenlernverfahren diskutiert.</para>
                </sect3>
            </sect2>
            <sect2 xml:id="ch_GramProp"><!-- Grammatische und stilistische Eigenschaften -->
                <title>Grammatische und stilistische Eigenschaften</title>
                <para>Wie bereits mehrfach erwähnt bedürfen wir für einige Klassenunterscheidungen nicht nur einer semantisch-inhaltlichen 
                    Analyse des Wortbestands, sondern auch einer Ermittlung von Zielgruppe (z.&#160;B. Romane für Erwachsene und für Jugendliche, 
                    Bücher für Laien und für Fachleute) oder Kommunikationssituation (z.&#160;B. Ratgeber / beratend und Sachbuch / informierend).
                    Wir erwarten, dass sich diese Unterscheidung mitunter in nicht ausreichender Weise im Wortbestand der so zu unterscheidenden 
                    Textklassen manifestiert und ziehen aus diesem Grund <indexterm class="startofrange" xml:id="idt_001"><primary>Stil</primary>
                    </indexterm>stilistische Merkmale hinzu.</para>
                <para>So ist etwa eine intuitive Erwartung bezüglich der <indexterm><primary>Satzlänge</primary></indexterm>Satzlänge von Texten, 
                    dass die durchschnittliche Satzlänge mit der 
                    Fachlichkeit eines Textes korreliert, indem Texte, die sich an ein allgemeineres Publikum richten, durchschnittlich kürzere 
                    Sätze aufweisen als Texte für ein akademisch gebildetes Publikum. Da die Unterscheidung zwischen Sach- und Fachbuch bei der 
                    angestrebten Warengruppenklassifikation eine Rolle spielt, bot sich die Aufnahme eines Attributes für die durchschnittliche 
                    Satzlänge, ausgedrückt in Wörtern pro Satz, an. Über das gesamte Trainingskorpus hinweg ergab sich für dieses Attribut ein 
                    Minimum von <wordsPerSentenceMinimum/> bis hin zu <wordsPerSentenceMaximum/> Wörtern pro Satz<footnote><para>Der Minimalwert trat in einem
                    E-Book auf, welches fast ausschließlich Grafiken enthält. Der Maximalwert stammt aus der E-Book-Ausgabe eines Theaterstücks, welches der
                    Autor ohne Punkte oder andere Satzzeichen verfasst hat. Die Satzerkennung von OpenNLP hat aufgrund der fehlenden Punkte sehr viele Satzenden
                    nicht erkannt.</para></footnote>. Der Median für dieses Attribut 
                    lag bei <wordsPerSentenceMean/> Wörtern pro Satz.</para>
                <para>In der theoretischen Stilstatistik (<biblioref linkend="bib_All89"/>) werden Stilmerkmale durch rein statistische Werte beschrieben,
                    z.B. Wort- und Satzlänge, Vokabelfülle, Prä- und Suffixe, Satzzahl, Häufigkeit grammatischer Kategorien<indexterm class="endofrange"
                        startref="idt_001"/>. Für die vorliegende Studie wurden folgende einfach berechenbare Attribute ermittelt:</para>
                <itemizedlist spacing="compact">
                    <listitem>
                        <para>durchschnittliche Satzlänge, ausgedrückt in Wörtern pro Satz</para>
                    </listitem>
                    <listitem>
                        <para>Quotient aus Lemmata zu Wörtern („lemmasToTokensRatio“) als Ausdruck für den morphologischen Formenreichtum des Dokuments</para>
                    </listitem>
                    <listitem>
                        <para>Anzahl der vorkommenden Wortarten</para>
                    </listitem>
                    <listitem>
                        <para>Textlänge, ausgedrückt in der Gesamtzahl der Wörter</para>
                    </listitem>
                    <listitem>
                        <para>Anzahl unterschiedlicher Wörter (Wortformen)</para>
                    </listitem>
                    <listitem>
                        <para><indexterm><primary>Vokabelfülle</primary></indexterm>Vokabelfülle, d. i. der Quotient aus der Gesamtzahl der Wörter und der
                            Zahl unterschiedlicher Wörter</para>
                    </listitem>
                    <listitem>
                        <para>durchschnittliche Wortlänge, in Zahl der Buchstaben</para>
                    </listitem>
                    <listitem>
                        <para>Quotient der Passivsätze in Bezug zur Gesamtzahl der Sätze</para>
                    </listitem>
                    <listitem>
                        <para>Quotienten für die relative Anzahl folgender Wortarten: Adjektive, Adverbien, Kardinalzahlen, Fremdwörter, Interjektionen,
                            Substantive, Namen, substituierende Demonstrativpronomen, attributive Demonstrativpronomen, substituierende Indefinitpronomen,
                            attributive Indefinitipronomen, Personalpronomen, substituierende Possessivpronomen, attributive Possessivpronomen, 
                            substituierende Relativpronomen, attributive Relativpronomen, Pronominaladverbien, Interrogativpronomen, Negationspartikeln,
                            Antwortpartikeln, Kompositionsglieder, finite Vollverben, imperative Vollverben, Partizipien II aus Vollverben, 
                            Hilfsverben, Modalverben, koordinierende Konjunktionen, subordinierende Konjunktionen</para>
                    </listitem>
                </itemizedlist>
                <para>Fast alle der oben genannten Eigenschaften basieren auf einer Erkennung von Wortarten. Für diese Wortartenerkennung wurden zwei
                    verschiedene, frei<footnote><para>Der TreeTagger ist nur in Binärdistributionen verfügbar, jedoch für akademische Zwecke kostenfrei 
                        lizenziert, für andere Zwecke ist eine kostenpflichtige Lizenz zu erwerben: 
                        <link xlink:href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/Tagger-Licence"/>, Zugriff am 11.2.2018.</para></footnote> verfügbare 
                    <indexterm><primary>POS-Tagging</primary></indexterm>POS-Tagger evaluiert. Im vorliegenden Projekt war es aus Kapazitätsgründen
                    nicht möglich, ein eigenes POS-Tagging-Modell basierend auf einem umfangreichen deutschen Sprachkorpus zu trainieren. Aus diesem
                    Grund kamen nur POS-Tagger in Frage, für welche bereits deutschsprachige Modelle erhältlich sind. Diese Bedingung trifft sowohl
                    auf den Helmut Schmids <indexterm><primary>TreeTagger</primary></indexterm>TreeTagger (<biblioref linkend="bib_Sch94"/>) als auch
                    auf den POS-Tagger aus dem Apache-OpenNLP-Baukasten<footnote><para><link
                        xlink:href="https://opennlp.apache.org/docs/1.8.1/manual/opennlp.html#tools.postagger"/> mit einem deutschen
                        POS-Modell unter <link xlink:href="http://opennlp.sourceforge.net/models-1.5/"/>, Zugriffe am 11.2.2018.</para></footnote> zu.
                    Die Evaluation der beiden verfügbaren POS-Tagger fand in einer frühen Projektphase auf einer kleinen zufälligen Stichprobe von 
                    lediglich sechs unterschiedlich langen Sätzen mit insgesamt 200 Wörtern statt. Die beiden Tagger lagen oft an unterschiedlichen
                    Stellen falsch, eine eindeutige Präferenz für eines der beiden Verfahren ließ sich nicht ableiten. Die Fehlerquote der beiden Tagger
                    lag bei 8,5% (17 Fehler auf 200 Wörter), wobei die Aussagekraft dieser Quote aufgrund der geringen Datenbasis natürlich sehr gering
                    ist. Da das Avve-Verarbeitungsprogramm in Java geschrieben ist und <indexterm><primary>Apache openNLP</primary></indexterm>Apache openNLP ebenfalls
                    als Java-Bibliothek vorliegt, konnte mit openNLP eine bessere Integration in die übrigen Verarbeitungsschritte erfolgen, so dass
                    aus pragmatischen Erwägungen eine Entscheidung zu Gunsten der Apache-Bibliothek getroffen wurde.</para>
                <para>In ähnlicher Weise wie bei der Lemmatisierung (siehe <xref linkend="ch_LexSemProp"/>) wurde auch für das POS-Tagging auf Basis
                    einer manuell gepflegten Ersetzungsliste die Möglichkeit einer Fehlerkorrektur im Anschluss an das openNLP-Tagging eingebaut.</para>
            </sect2>
            <sect2 xml:id="ch_AttribEval"><!-- Attributauswahl -->
                <title>Attributauswahl</title>
                <para><indexterm><primary>Attributauswahl</primary></indexterm>In der frühen Phase, in der eine Vielzahl verschiedener in Weka 
                    verfügbarer Maschinenlernalgorithmen durchprobiert wurden (siehe <xref linkend="app_Verfahrensauswahl"/>), wurde in der 
                    Regel keine Attribut&#173;selektion vorgenommen, d. h. die Zahl der Attribute wurde nicht reduziert, bevor die Daten in die 
                    Trainings- und Testphase geladen wurden. Eine Ausnahme stellt die bereits in <xref linkend="ch_LexSemProp_Wortvektor"/> erwähnte
                    Beschränkung der Wortvektorgröße dar. Bei der späteren Optimierung der Performance spielten darüber hinaus auch allgemeinere 
                    Überlegungen zur Attributauswahl eine Rolle, so dass hier ein kurzer Überblick über ausgewählte Möglichkeiten der Bewertung von 
                    Attributqualität gegeben werden soll.</para>
                <para>Attributauswahlverfahren können unabhängig vom Lernverfahren arbeiten (<biblioref linkend="bib_WFHP17"/> sprechen in diesem
                    Fall von <emphasis>Filterverfahren</emphasis>) oder es hingegen einbeziehen (<emphasis>Wrappermethode</emphasis> nach
                    <biblioref linkend="bib_WFHP17"/>, S. 290).
                    Problematisch können Attribute vor allem aus zwei Gründen sein: Einerseits ist es denkbar, dass Attribute in Bezug auf die
                    vorliegende Lernaufgabe irrelevant sind. Andererseits stellen mitunter redundante Attribute ein Problem dar. Darüber hinaus
                    kann die Anzahl der Attribute zu einem Problem werden, wenn sie dazu führt, dass die Lernaufgabe auf der vorhandenen Hardware
                    aufgrund zu großer Attributzahl nicht mehr ausgeführt werden kann. Neben dem Wechsel auf eine stärkere Hardware kann diesem 
                    Problem ebenfalls mit einer Reduktion der Attribute begegnet werden.</para>
                <sect3 xml:id="ch_AttribEval_Entropie"> <!-- Entropiebasierte Attributbewertung -->
                    <title>Entropiebasierte Attributbewertung</title>
                    <para>Ein Verfahren, sowohl irrelevante als auch redundante Attribute zu identifizieren, ist die Berechnung der Korrelation 
                        zwischen den Attributen mit Hilfe der <indexterm class="startofrange" xml:id="idt_015"><primary>Entropie</primary></indexterm>Entropiefunktion &#919;
                        (<xref linkend="formula_entropy"/>.
                    Die Attributmenge kann dann auf diejenigen Attribute eingeschränkt werden, welche eine hohe Korrelation mit der zu lernenden 
                    Klasse und gleichzeitig eine geringe Korrelation mit anderen Attributen aufweisen. Die Entropie ist in der Informationstheorie 
                    nach <indexterm><primary>Shannon, Claude</primary></indexterm>Claude Shannon das Maß für den mittleren Informationsgehalt eines Zeichens (vgl. <biblioref linkend="bib_SG17"/>, S. 141-144), 
                    wobei sich unsere Maschinenlernattribute auch als Zeichen im Sinne von Shannons Theorie betrachten lassen (<biblioref linkend="bib_WFHP17"/>,
                    S. 291). Mit den folgenden Formeln bewerten wir also den Informationsgehalt von Attributen in Bezug auf eine Klasse: Je 
                    größer die Entropie eines Attributs, desto mehr reduziert dieses Attribut die Unsicherheit bei der Klassifikation
                    eines Datensatzes. Die Entropieformel lautet, mit p<subscript>i</subscript> als Wahrscheinlichkeit für das Auftreten eines 
                    Attributwerts und <inlineequation><mml:math display="inline"><mml:mrow>
                        <mml:munderover>
                            <mml:mo stretchy="false">∑</mml:mo>
                            <mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow>
                            <mml:mi>n</mml:mi>
                        </mml:munderover>
                        <mml:msub>
                            <mml:mi>p</mml:mi>
                            <mml:mi>i</mml:mi>
                        </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                    </mml:mrow></mml:math></inlineequation>:</para>
                <equation xml:id="formula_entropy"> <!-- Entropieformel -->
                    <mml:math>
                        <mml:mrow>
                            <mml:mi>&#919;</mml:mi>
                            <mml:mrow>
                                <mml:mrow>
                                    <mml:mo fence="true" stretchy="false">(</mml:mo>
                                    <mml:mrow>
                                        <mml:mrow>
                                            <mml:msub>
                                                <mml:mi>p</mml:mi>
                                                <mml:mn>1</mml:mn>
                                            </mml:msub>
                                            <mml:mtext>,</mml:mtext>
                                            <mml:msub>
                                                <mml:mi>p</mml:mi>
                                                <mml:mn>2</mml:mn>
                                            </mml:msub>
                                            <mml:mtext>,...,</mml:mtext>
                                            <mml:msub>
                                                <mml:mi>p</mml:mi>
                                                <mml:mi>n</mml:mi>
                                            </mml:msub>
                                        </mml:mrow>
                                    </mml:mrow>
                                    <mml:mo fence="true" stretchy="false">)</mml:mo>
                                </mml:mrow>
                                <mml:mo stretchy="false">=</mml:mo>
                                <mml:mrow>
                                    <mml:mo stretchy="false">−</mml:mo>
                                    <mml:mrow>
                                        <mml:mo stretchy="false">∑</mml:mo>
                                        <mml:msub>
                                            <mml:mi>p</mml:mi>
                                            <mml:mi>i</mml:mi>
                                        </mml:msub>
                                    </mml:mrow>
                                </mml:mrow>
                            </mml:mrow>
                            <mml:msub>
                                <mml:mi>log</mml:mi>
                                <mml:mn>2</mml:mn>
                            </mml:msub>
                            <mml:mrow>
                                <mml:mo fence="true" stretchy="false">(</mml:mo>
                                <mml:mrow>
                                    <mml:msub>
                                        <mml:mi>p</mml:mi>
                                        <mml:mi>i</mml:mi>
                                    </mml:msub>
                                </mml:mrow>
                                <mml:mo fence="true" stretchy="false">)</mml:mo>
                            </mml:mrow>
                        </mml:mrow>
                    </mml:math>
                </equation>
                <para>Weka stellt auf Basis dieser Entropieformel eine Attributevaluationsfunktion <code>InfoGainAttributeEval</code> 
                    bereit, welche ein Attributranking zurückgibt, indem die Entropie der Klassen pro gegebenem Attribut von der „reinen“ 
                    Entropie der Klassen abgezogen wird, und somit der Informationszuwachs durch das jeweilige Attribut festgestellt wird:</para>
                <equation xml:id="formula_InfoGain"> <!-- InfoGain-Formel -->
                    <mml:math display="block">
                          <mml:mrow>
                              <mml:mtext>InfoGain</mml:mtext>
                              <mml:mo fence="true" stretchy="false">(</mml:mo>
                              <mml:mi>C</mml:mi>
                              <mml:mtext>,</mml:mtext>
                              <mml:mi>A</mml:mi>
                              <mml:mo fence="true" stretchy="false">)</mml:mo>
                              <mml:mo stretchy="false">=</mml:mo>
                              <mml:mi>&#919;</mml:mi>
                              <mml:mo fence="true" stretchy="false">(</mml:mo>
                              <mml:mi>C</mml:mi>
                              <mml:mo fence="true" stretchy="false">)</mml:mo>
                              <mml:mo fence="true" stretchy="false">-</mml:mo>
                              <mml:mi>&#919;</mml:mi>
                              <mml:mo fence="true" stretchy="false">(</mml:mo>
                              <mml:mi>C</mml:mi>
                              <mml:mtext>&#160;</mml:mtext>
                              <mml:mo fence="true" stretchy="false">|</mml:mo>
                              <mml:mtext>&#160;</mml:mtext>
                              <mml:mi>A</mml:mi>
                              <mml:mo fence="true" stretchy="false">)</mml:mo>
                        </mml:mrow>
                    </mml:math>
                </equation>
                <para>Weka <glossterm linkend="gt_diskretisierung">diskretisiert</glossterm> den Wertebereich numerischer Attribute vor der Berechnung von InfoGain<footnote>
                    <para>Vgl. hierzu den Source-Code der Klasse <code>InfoGainAttributeEval.java</code> aus Weka-Version 3.8.1, Zeile 275. Der Quellcode wird beim Download
                    dieser Weka-Version mitgeliefert.</para></footnote>. Anschließend wird eine Matrix aufgebaut, welche die Vorkommshäufigkeit <emphasis>w</emphasis> aller 
                    Attributwerte pro Klasse enthält und worin die Spalten jeweils für eine von n Klassen stehen und die Zeilen für jeweils einen der m möglichen 
                    Attributwerte:</para>
                <equation xml:id="formula_AttributeValuesPerClassInMatrix"> <!-- Attributwerte pro Klasse in Matrix -->
                    <mml:math display="block">
                        <mml:mrow>
                            <mml:mo fence="true" stretchy="true">(</mml:mo>
                            <mml:mrow>
                                <mml:mtable>
                                    <mml:mtr>
                                        <mml:mtd>
                                            <mml:mrow>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>2</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>3</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mtext>...</mml:mtext>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mi>n</mml:mi>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                            </mml:mrow>
                                        </mml:mtd>
                                    </mml:mtr>
                                    <mml:mtr>
                                        <mml:mtd>
                                            <mml:mrow>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mn>2</mml:mn>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mn>2</mml:mn>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>2</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mn>2</mml:mn>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>3</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mtext>...</mml:mtext>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mn>2</mml:mn>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mi>n</mml:mi>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                            </mml:mrow>
                                        </mml:mtd>
                                    </mml:mtr>
                                    <mml:mtr>
                                        <mml:mtd>
                                            <mml:mtext>...</mml:mtext>
                                        </mml:mtd>
                                    </mml:mtr>
                                    <mml:mtr>
                                        <mml:mtd>
                                            <mml:mrow>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mi>m</mml:mi>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mi>m</mml:mi>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>2</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mi>m</mml:mi>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mn>3</mml:mn>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mtext>...</mml:mtext>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>a</mml:mi>
                                                            <mml:mi>m</mml:mi>
                                                        </mml:msub>
                                                        <mml:msub>
                                                            <mml:mi>c</mml:mi>
                                                            <mml:mi>n</mml:mi>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:msub>
                                                <mml:mtext>&#160;</mml:mtext>
                                            </mml:mrow>
                                        </mml:mtd>
                                    </mml:mtr>
                                </mml:mtable>
                            </mml:mrow>
                            <mml:mo fence="true" stretchy="true">)</mml:mo>
                        </mml:mrow>
                    </mml:math> 
                </equation>
                <para>Kommt z.B. der zweite diskrete Attributwert in Klasse Nummer 1 insgesamt 5 Mal vor, so enthält die oben veranschaulichte 
                    Matrix an der Position <emphasis>a<subscript>a<subscript>2</subscript>c<subscript>1</subscript></subscript></emphasis>
                    den Wert 5. &#919;(C) lässt sich nun leicht spaltenweise berechnen: die Summe der Werte der ersten Spalte in der Matrix ergibt die 
                    Häufigkeit der ersten Klasse, die Summer der Werte der zweiten Spalte entspricht der Häufigkeit der zweiten Klasse, usw. Wekas
                    <emphasis>InfoGainAttributeEval</emphasis> rechnet intern mit der Vorkommenshäufigkeit <emphasis>w</emphasis> statt mit 
                    Wahrscheinlichkeitswerten <emphasis>p</emphasis> sowie mit dem natürlichen Logarithmus und nimmt erst anschließend eine Konvertierung 
                    nach <emphasis>p</emphasis> und den Logarithmus zur Basis 2 vor, was durch den Nenner in der folgenden Formel geschieht:</para>
                <equation xml:id="formula_ClassEntropyInInfoGainAttributeEval"><!-- Klassenentropie nach Wekas InfoGainAttributeEval -->
                    <mml:math display="block">
                        <mml:mrow>
                            <mml:mi>&#919;(C)</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mrow>
                                <mml:mfrac>
                                    <mml:mrow>
                                        <mml:mrow>
                                            <mml:mo fence="true" stretchy="true">(</mml:mo>
                                            <mml:mrow>
                                                <mml:mrow>
                                                    <mml:munderover>
                                                        <mml:mo stretchy="false">∑</mml:mo>
                                                        <mml:mrow>
                                                            <mml:mi>i</mml:mi>
                                                            <mml:mo stretchy="false">=</mml:mo>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:mrow>
                                                        <mml:mi>n</mml:mi>
                                                    </mml:munderover>
                                                    <mml:mrow>
                                                        <mml:mrow>
                                                            <mml:mrow>
                                                                <mml:mo stretchy="false">−</mml:mo>
                                                                <mml:msub>
                                                                    <mml:mi>w</mml:mi>
                                                                    <mml:mi>i</mml:mi>
                                                                </mml:msub>
                                                            </mml:mrow>
                                                            <mml:mo stretchy="false">⋅</mml:mo>
                                                            <mml:mi>ln</mml:mi>
                                                        </mml:mrow>
                                                        <mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">(</mml:mo>
                                                            <mml:mrow>
                                                                <mml:msub>
                                                                    <mml:mi>w</mml:mi>
                                                                    <mml:mi>i</mml:mi>
                                                                </mml:msub>
                                                            </mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">)</mml:mo>
                                                        </mml:mrow>
                                                    </mml:mrow>
                                                </mml:mrow>
                                            </mml:mrow>
                                            <mml:mo fence="true" stretchy="true">)</mml:mo>
                                        </mml:mrow>
                                        <mml:mo stretchy="false">+</mml:mo>
                                        <mml:mrow>
                                            <mml:mrow>
                                                <mml:mrow>
                                                    <mml:mo fence="true" stretchy="true">(</mml:mo>
                                                    <mml:mrow>
                                                        <mml:mrow>
                                                            <mml:munderover>
                                                                <mml:mo stretchy="false">∑</mml:mo>
                                                                <mml:mrow>
                                                                    <mml:mi>i</mml:mi>
                                                                    <mml:mo stretchy="false">=</mml:mo>
                                                                    <mml:mn>1</mml:mn>
                                                                </mml:mrow>
                                                                <mml:mi>n</mml:mi>
                                                            </mml:munderover>
                                                            <mml:msub>
                                                                <mml:mi>w</mml:mi>
                                                                <mml:mi>i</mml:mi>
                                                            </mml:msub>
                                                        </mml:mrow>
                                                    </mml:mrow>
                                                    <mml:mo fence="true" stretchy="true">)</mml:mo>
                                                </mml:mrow>
                                                <mml:mo stretchy="false">⋅</mml:mo>
                                                <mml:mi>ln</mml:mi>
                                            </mml:mrow>
                                            <mml:mrow>
                                                <mml:mo fence="true" stretchy="false">(</mml:mo>
                                                <mml:mrow>
                                                    <mml:mn>4</mml:mn>
                                                </mml:mrow>
                                                <mml:mo fence="true" stretchy="false">)</mml:mo>
                                            </mml:mrow>
                                        </mml:mrow>
                                    </mml:mrow>
                                    <mml:mrow>
                                        <mml:mrow>
                                            <mml:mrow>
                                                <mml:mo fence="true" stretchy="true">(</mml:mo>
                                                <mml:mrow>
                                                    <mml:mrow>
                                                        <mml:munderover>
                                                            <mml:mo stretchy="false">∑</mml:mo>
                                                            <mml:mrow>
                                                                <mml:mi>i</mml:mi>
                                                                <mml:mo stretchy="false">=</mml:mo>
                                                                <mml:mn>1</mml:mn>
                                                            </mml:mrow>
                                                            <mml:mi>n</mml:mi>
                                                        </mml:munderover>
                                                        <mml:msub>
                                                            <mml:mi>w</mml:mi>
                                                            <mml:mi>i</mml:mi>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                </mml:mrow>
                                                <mml:mo fence="true" stretchy="true">)</mml:mo>
                                            </mml:mrow>
                                            <mml:mo stretchy="false">⋅</mml:mo>
                                            <mml:mi>ln</mml:mi>
                                        </mml:mrow>
                                        <mml:mrow>
                                            <mml:mo fence="true" stretchy="false">(</mml:mo>
                                            <mml:mrow>
                                                <mml:mn>2</mml:mn>
                                            </mml:mrow>
                                            <mml:mo fence="true" stretchy="false">)</mml:mo>
                                        </mml:mrow>
                                    </mml:mrow>
                                </mml:mfrac>
                                <mml:mtext>&#160;, mit&#160;</mml:mtext>
                                <mml:mrow>
                                    <mml:msub>
                                        <mml:mi>w</mml:mi>
                                        <mml:mi>i</mml:mi>
                                    </mml:msub>
                                    <mml:mo stretchy="false">=</mml:mo>
                                    <mml:mrow>
                                        <mml:munderover>
                                            <mml:mo stretchy="false">∑</mml:mo>
                                            <mml:mrow>
                                                <mml:mi>k</mml:mi>
                                                <mml:mo stretchy="false">=</mml:mo>
                                                <mml:mn>1</mml:mn>
                                            </mml:mrow>
                                            <mml:mi>m</mml:mi>
                                        </mml:munderover>
                                        <mml:msub>
                                            <mml:mi>a</mml:mi>
                                            <mml:mrow>
                                                <mml:msub>
                                                    <mml:mi>a</mml:mi>
                                                    <mml:mi>k</mml:mi>
                                                </mml:msub>
                                                <mml:msub>
                                                    <mml:mi>c</mml:mi>
                                                    <mml:mi>i</mml:mi>
                                                </mml:msub>
                                            </mml:mrow>
                                        </mml:msub>
                                    </mml:mrow>
                                </mml:mrow>
                            </mml:mrow>
                        </mml:mrow>
                    </mml:math>
                </equation>
                <para>Die Berechnung der bedingten Entropie für &#919;(C | A) erfolgt dann nach der
                    folgenden Formel:</para>
                <equation xml:id="formula_bedingteEntropieFürKlassenBeiGegebenenAttributen"><!-- bedingte Klassenentropie bei gegebenen Attributwerten nach InfoGainAttributeEval -->
                    <mml:math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <mml:mrow>
                            <mml:mi>&#919;</mml:mi>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:mi>C</mml:mi>
                            <mml:mo stretchy="false">|</mml:mo>
                            <mml:mi>A</mml:mi>
                            <mml:mo stretchy="false">)</mml:mo>
                            <mml:mo>=</mml:mo>
                            <mml:mfrac>
                                <mml:mrow>
                                    <mml:mo stretchy="false">−</mml:mo>
                                    <mml:mrow>
                                        <mml:mrow>
                                            <mml:munderover>
                                                <mml:mo stretchy="false">∑</mml:mo>
                                                <mml:mrow>
                                                    <mml:mi>i</mml:mi>
                                                    <mml:mo stretchy="false">=</mml:mo>
                                                    <mml:mn>1</mml:mn>
                                                </mml:mrow>
                                                <mml:mi>m</mml:mi>
                                            </mml:munderover>
                                            <mml:mrow>
                                                <mml:mrow>
                                                    <mml:mrow>
                                                        <mml:mo stretchy="false">−</mml:mo>
                                                        <mml:msub>
                                                            <mml:mi>w</mml:mi>
                                                            <mml:mi>i</mml:mi>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                    <mml:mo stretchy="false">⋅</mml:mo>
                                                    <mml:mi>ln</mml:mi>
                                                </mml:mrow>
                                                <mml:mrow>
                                                    <mml:mo fence="true" stretchy="false">(</mml:mo>
                                                    <mml:mrow>
                                                        <mml:msub>
                                                            <mml:mi>w</mml:mi>
                                                            <mml:mi>i</mml:mi>
                                                        </mml:msub>
                                                    </mml:mrow>
                                                    <mml:mo fence="true" stretchy="false">)</mml:mo>
                                                </mml:mrow>
                                            </mml:mrow>
                                        </mml:mrow>
                                        <mml:mo stretchy="false">+</mml:mo>
                                        <mml:mrow>
                                            <mml:mo fence="true" stretchy="true">(</mml:mo>
                                            <mml:mrow>
                                                <mml:mrow>
                                                    <mml:munderover>
                                                        <mml:mo stretchy="false">∑</mml:mo>
                                                        <mml:mrow>
                                                            <mml:mi>j</mml:mi>
                                                            <mml:mo stretchy="false">=</mml:mo>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:mrow>
                                                        <mml:mi>n</mml:mi>
                                                    </mml:munderover>
                                                    <mml:mrow>
                                                        <mml:mrow>
                                                            <mml:msub>
                                                                <mml:mi>a</mml:mi>
                                                                <mml:mrow>
                                                                    <mml:msub>
                                                                        <mml:mi>a</mml:mi>
                                                                        <mml:mi>i</mml:mi>
                                                                    </mml:msub>
                                                                    <mml:msub>
                                                                        <mml:mi>c</mml:mi>
                                                                        <mml:mi>j</mml:mi>
                                                                    </mml:msub>
                                                                </mml:mrow>
                                                            </mml:msub>
                                                            <mml:mo stretchy="false">⋅</mml:mo>
                                                            <mml:mi>ln</mml:mi>
                                                        </mml:mrow>
                                                        <mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">(</mml:mo>
                                                            <mml:mrow>
                                                                <mml:msub>
                                                                    <mml:mi>a</mml:mi>
                                                                    <mml:mrow>
                                                                        <mml:msub>
                                                                            <mml:mi>a</mml:mi>
                                                                            <mml:mi>i</mml:mi>
                                                                        </mml:msub>
                                                                        <mml:msub>
                                                                            <mml:mi>c</mml:mi>
                                                                            <mml:mi>j</mml:mi>
                                                                        </mml:msub>
                                                                    </mml:mrow>
                                                                </mml:msub>
                                                            </mml:mrow>
                                                            <mml:mo fence="true" stretchy="false">)</mml:mo>
                                                        </mml:mrow>
                                                    </mml:mrow>
                                                </mml:mrow>
                                            </mml:mrow>
                                            <mml:mo fence="true" stretchy="true">)</mml:mo>
                                        </mml:mrow>
                                    </mml:mrow>
                                </mml:mrow>
                                <mml:mrow>
                                    <mml:mrow>
                                        <mml:mrow>
                                            <mml:mo fence="true" stretchy="true">(</mml:mo>
                                            <mml:mrow>
                                                <mml:mrow>
                                                    <mml:munderover>
                                                        <mml:mo stretchy="false">∑</mml:mo>
                                                        <mml:mrow>
                                                            <mml:mi>i</mml:mi>
                                                            <mml:mo stretchy="false">=</mml:mo>
                                                            <mml:mn>1</mml:mn>
                                                        </mml:mrow>
                                                        <mml:mi>n</mml:mi>
                                                    </mml:munderover>
                                                    <mml:msub>
                                                        <mml:mi>w</mml:mi>
                                                        <mml:mi>i</mml:mi>
                                                    </mml:msub>
                                                </mml:mrow>
                                            </mml:mrow>
                                            <mml:mo fence="true" stretchy="true">)</mml:mo>
                                        </mml:mrow>
                                        <mml:mo stretchy="false">⋅</mml:mo>
                                        <mml:mi>ln</mml:mi>
                                    </mml:mrow>
                                    <mml:mrow>
                                        <mml:mo fence="true" stretchy="false">(</mml:mo>
                                        <mml:mrow>
                                            <mml:mn>2</mml:mn>
                                        </mml:mrow>
                                        <mml:mo fence="true" stretchy="false">)</mml:mo>
                                    </mml:mrow>
                                </mml:mrow>
                            </mml:mfrac>
                            <mml:mtext>&#160;, mit&#160;</mml:mtext>
                            <mml:mrow>
                                <mml:msub>
                                    <mml:mi>w</mml:mi>
                                    <mml:mi>i</mml:mi>
                                </mml:msub>
                                <mml:mo stretchy="false">=</mml:mo>
                                <mml:mrow>
                                    <mml:munderover>
                                        <mml:mo stretchy="false">∑</mml:mo>
                                        <mml:mrow>
                                            <mml:mi>k</mml:mi>
                                            <mml:mo stretchy="false">=</mml:mo>
                                            <mml:mn>1</mml:mn>
                                        </mml:mrow>
                                        <mml:mi>n</mml:mi>
                                    </mml:munderover>
                                    <mml:msub>
                                        <mml:mi>a</mml:mi>
                                        <mml:mrow>
                                            <mml:msub>
                                                <mml:mi>a</mml:mi>
                                                <mml:mi>i</mml:mi>
                                            </mml:msub>
                                            <mml:msub>
                                                <mml:mi>c</mml:mi>
                                                <mml:mi>k</mml:mi>
                                            </mml:msub>
                                        </mml:mrow>
                                    </mml:msub>
                                </mml:mrow>
                            </mml:mrow>
                        </mml:mrow>
                    </mml:math>
                </equation>
                <para>Zur Veranschaulichung bietet es sich an, die Berechnungen an einem kleinen Beispiel zu veranschaulichen. Gegeben sei folgende
                    Attributwertmatrix:</para>
                <equation xml:id="formula_Attributwertmatrixbeispiel"><!-- Beispiel für eine Attributwertematrix -->
                    <mml:math display="block">
                        <mml:mrow>
                            <mml:mo fence="true" stretchy="true">(</mml:mo>
                            <mml:mrow>
                                <mml:mtable>
                                    <mml:mtr>
                                        <mml:mtd>
                                            <mml:mrow>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>0</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>1</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>0</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                            </mml:mrow>
                                        </mml:mtd>
                                    </mml:mtr>
                                    <mml:mtr>
                                        <mml:mtd>
                                            <mml:mrow>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>0</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>0</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>1</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                            </mml:mrow>
                                        </mml:mtd>
                                    </mml:mtr>
                                    <mml:mtr>
                                        <mml:mtd>
                                            <mml:mrow>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>2</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>0</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                                <mml:mn>0</mml:mn>
                                                <mml:mtext>&#160;</mml:mtext>
                                            </mml:mrow>
                                        </mml:mtd>
                                    </mml:mtr>
                                </mml:mtable>
                            </mml:mrow>
                            <mml:mo fence="true" stretchy="true">)</mml:mo>
                        </mml:mrow>
                    </mml:math>
                </equation>
                <para>Das bedeutet, dass insgesamt vier Instanzen vorliegen. Zwei Instanzen mit
                    Attributwert Nummer 3 (Zeile 3) gehören zur Klasse 1 (Spalte 1), also
                                <emphasis>a<subscript>a<subscript>3</subscript>c<subscript>1</subscript></subscript></emphasis>=2.
                    Eine Instanz mit Attributwert Nummer 1 (Zeile 1) ist der Klasse 2 (Spalte 2)
                    zugehörig, also
                                <emphasis>a<subscript>a<subscript>1</subscript>c<subscript>2</subscript></subscript></emphasis>=1.
                    Und eine Instanz mit Attributwert Nummer 2 gehört zur Klasse 3:
                                <emphasis>a<subscript>a<subscript>2</subscript>c<subscript>3</subscript></subscript></emphasis>=1.
                    Beim spaltenweisen Iterieren durch die &#919;(C)-Formel nimmt
                            <emphasis>w<subscript>i</subscript></emphasis> somit nacheinander die
                    Summenwerte 2, 1, 1 an. Der Zähler von <xref
                        linkend="formula_ClassEntropyInInfoGainAttributeEval"/> lautet dann <inlineequation>
                        <mathphrase>(-2 ln(2) - 1 ln(1) - 1 ln(1)) + 4 ln(4) &#x2248;
                            4,1822</mathphrase>
                    </inlineequation>, der Nenner <inlineequation>
                        <mathphrase>4 ln(2) &#x2248; 2,7726</mathphrase>
                    </inlineequation> und somit &#919;(C) &#x2248; 1,5084.</para>
                <para>Für &#919;(C|A) summieren wir zunächst über die erste Zeile der Matrix und erhalten somit <emphasis>w<subscript>1</subscript></emphasis> = 1. Wir
                    benötigen für diese Zeile aber auch die Entropie der Klassen für den gegebenen Attributwert, der sich ergibt aus <inlineequation>
                        <mathphrase>0 + 1 ln(1) + 0 = 0</mathphrase> </inlineequation><footnote><para>Der Logarithmus von 0 ist nicht definiert. In Termen der
                            Entropiefunktion, in denen die Variable mit 0 belegt ist, geht per definitionem dieser gesamte Term mit Wert 0 in die Gesamtberechnung
                            ein.</para></footnote>. Als Wert für die erste Zeile ergibt sich somit <inlineequation><mathphrase>-1 ln(1) + 0 = 0</mathphrase>
                    </inlineequation>. Das gleiche gilt für die zweite Zeile unserer Beispielmatrix. Für die dritte Zeile lautet die Summe über alle Werte
                            <emphasis>w<subscript>3</subscript></emphasis> = 2. Die „innere“ Entropie für die Zeile beträgt <inlineequation>
                        <mathphrase>2 ln (2) + 0 + 0 &#x2248; 1,3862</mathphrase></inlineequation>, der Gesamtbetrag für die Zeile schließlich <inlineequation>
                        <mathphrase>-2 ln (2) + 1,3862 = 0</mathphrase></inlineequation>. Damit ist der gesamte Zähler 0 und somit auch das Ergebnis des Bruchs.<indexterm
                        class="endofrange" startref="idt_015"/></para>
                <para>Wir setzen die ermittelten Werte in die InfoGain-Formel (<xref linkend="formula_InfoGain"/>) ein und erhalten <inlineequation>
                        <mathphrase> InfoGain(C,A) = 1,5084 - 0 = 1,5084</mathphrase>
                    </inlineequation>.</para>
                <para>Wendet man diese Attributrankingmethode auf die in unserem Trainingskorpus enthaltenen E-Books an, ergeben sich die folgenden zwanzig 
                    informationsträchtigsten Attribute:</para>
                <table frame="all" orient="port" pgwide="0" tocentry="1" xml:id="tab_InfoGainAttributes"> <!-- InfoGain der Attribute -->
                    <info>
                        <title>Die zwanzig Attribute mit dem höchsten InfoGain-Wert im Trainingskorpus</title>
                    </info>
                    <tgroup cols="3" align="left" colsep="1" rowsep="1">
                        <colspec colname="Attributbeschreibung" colnum="1" colwidth="4*" align="left"/>
                        <colspec colname="Attributname" colnum="2" colwidth="2.5*" align="left"/>
                        <colspec colname="InfoGain-Wert" colnum="3" colwidth="1*" align="left"/>
                        <thead>
                            <row>
                                <entry>Attributbeschreibung</entry>
                                <entry>Attributname</entry>
                                <entry>InfoGain-Wert</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row><entry>Quotient der finiten Verben</entry><entry>finiteMainVerbsRatio</entry><entry>1.1229</entry></row>
                            <row><entry>Quotient der Personalpronomen</entry><entry>personalPronounRatio</entry><entry>0.9729</entry></row>
                            <row><entry>Quotient der Nomen</entry><entry>ratioOfNouns</entry><entry>0.9409</entry></row>
                            <row><entry>Quotient von Kompositumsteilen</entry><entry>compoundPartRatio</entry><entry>0.9018</entry></row>
                            <row><entry>durchschnittliche Wortlänge</entry><entry>averageWordLength</entry><entry>0.8738</entry></row>
                            <row><entry>Quotient der Kardinalzahlen</entry><entry>ratioOfCardinals</entry><entry>0.8631</entry></row>
                            <row><entry>Quotient der Antwortpartikel</entry><entry>answerParticleRatio</entry><entry>0.7576</entry></row>
                            <row><entry>Quotient der Relativpronomen</entry><entry>attrRelPronRatio</entry><entry>0.7122</entry></row>
                            <row><entry>Quotient der attributiven Possessivpronomen</entry><entry>attrPossPronRatio</entry><entry>0.7122</entry></row>
                            <row><entry>Quotient der Infinitive</entry><entry>infinitiveMainVerbsRatio</entry><entry>0.6731</entry></row>
                            <row><entry>Quotient der Interrogativpronomen</entry><entry>interrogativePronounRatio</entry><entry>0.6725</entry></row>
                            <row><entry>Gesamtzahl der vorkommenden Wortarten</entry><entry>uniquePartsOfSpeech</entry><entry>0.5913</entry></row>
                            <row><entry>Quotient der Negationspartikel</entry><entry>negationParticleRatio</entry><entry>0.5857</entry></row>
                            <row><entry>Vorkommen des Wortes <emphasis>nicken</emphasis></entry><entry>w_nicken</entry><entry>0.5733</entry></row>
                            <row><entry>Vorkommen des Wortes <emphasis>schütteln</emphasis></entry><entry>w_schütteln</entry><entry>0.5594</entry></row>
                            <row><entry>Vorkommen des Wortes <emphasis>lächeln</emphasis></entry><entry>w_lächeln</entry><entry>0.5417</entry></row>
                            <row><entry>Quotient der Adjektive</entry><entry>ratioOfAdjectives</entry><entry>0.535</entry></row>
                            <row><entry>Anzahl der Abbildungen</entry><entry>numberOfImages</entry><entry>0.5234</entry></row>
                            <row><entry>Quotient der Passivsätze</entry><entry>ratioOfPassiveSentences</entry><entry>0.5233</entry></row>
                            <row><entry>Vorkommen des Wortes <emphasis>starren</emphasis></entry><entry>w_starren</entry><entry>0.5216</entry></row>
                            <row><entry>Quotient der Namen</entry><entry>ratioOfNamedEntities</entry><entry>0.5183</entry></row>
                        </tbody>
                    </tgroup>
                </table>
                <para>Im Ranking in <xref linkend="tab_InfoGainAttributes"/> ist deutlich ersichtlich, dass sich die grammatische Analyse des Textes, wie in 
                    <xref linkend="ch_GramProp"/> geschildert, lohnt. Das Vorkommen einiger bestimmter Wörter im Text (vgl. <xref linkend="ch_LexSemProp"/>) erreicht
                    jedoch ebenfalls hohe Rankingwerte, so dass sich von einer Kombination dieser unterschiedlichen Attributarten eine bessere Klassifikation erwarten 
                    lässt als wenn man einen reinen Wortvektorzugang oder ein rein auf grammatischen Kategorien basierendes Maschinentraining angestrebt hätte.</para>
                </sect3>
                <sect3 xml:id="ch_AttribEval_StringToWordVector"> <!-- Evaluation StringToWordVector -->
                    <title>Evaluation der besten Wortvektorgröße</title>
                    <para>Um zu einer guten Entscheidung zu gelangen, welche Wortvektorgröße ideal ist, wurde für die beiden Maschinenlernverfahren Naïve Bayes und SMO, welche
                        in den Kapiteln 4 und 5 näher vorgestellt werden, die Performance für verschieden große Wortvektoren überprüft. Dabei wurden 
                        unterschiedliche Werte für zwei Parameter der Wortvektorerstellung (vgl. <xref linkend="ch_LexSemProp_Wortvektor"/>) durchprobiert:</para>
                    <itemizedlist spacing="compact">
                        <listitem>
                            <para>Größe <emphasis>x</emphasis> der Liste der „wichtigsten“ Wörter pro Dokument</para>
                        </listitem>
                        <listitem>
                            <para><emphasis>minimum term frequency</emphasis> pro Klasse als Parameter der Weka-Funktion <code>StringToWordVector</code> (M-Parameter)</para>
                        </listitem>
                    </itemizedlist>
                    <para>Es wurden pro Durchlauf jeweils 4615 Trainingsinstanzen und 2292 Testinstanzen mit dem Basissatz an Attributen ohne Optimierungen verwendet.
                        Mit den Trainingsinstanzen wurden ein Naïve-Bayes- und ein SMO-Modell
                        gelernt und eine Evaluation auf Basis der Testinstanzen durchgeführt. Als
                        einziges Qualitätskriterium wurde beim Ausprobieren verschiedener Werte für
                        diese Parameter der prozentuale Anteil korrekt klassifizierter Testinstanzen
                        an der Gesamtzahl der Instanzen (Erfolgsquote) herangezogen. Die Ergebnisdarstellung in den
                        folgenden Streudiagrammen ordnet der aus den gewählten Parameterwerten
                        resultierenden Wortvektorgröße die Qualität des damit trainierten
                        Klassifikators zu. Die Markierungen der einzelnen Punkte setzen sich aus dem
                        Kürzel für den Algorithmus (NB bzw. SMO), der Länge der Liste wichtigster
                        Wörter pro Dokument und dem M-Parameter zusammen.</para>
                    <figure xml:id="abb3">
                        <info>
                            <title>Performance von Naïve Bayes bei steigender Wortvektorgröße</title>
                        </info>
                        <mediaobject>
                            <imageobject>
                                <imagedata format="SVG" fileref="../img/Abb3_NB-Wortvektor-Performance.svg" width="100%"/>
                            </imageobject>
                        </mediaobject>
                    </figure>
                    <figure xml:id="abb4">
                        <info>
                            <title>Performance von SMO bei steigender Wortvektorgröße</title>
                        </info>
                        <mediaobject>
                            <imageobject>
                                <imagedata format="SVG" fileref="../img/Abb4_SMO-Wortvektor-Performance.svg" width="100%"/>
                            </imageobject>
                        </mediaobject>
                    </figure>
                    <para>Es zeigt sich hier deutlich, dass die größten Wortvektoren bei kleinem
                            <emphasis>x</emphasis> und kleinem <emphasis>M</emphasis> entstehen,
                        dass diese Werte aber nicht die besten Klassifikatoren erzeugen.
                        Entscheidender als die Wahl von <emphasis>M</emphasis> scheint die Größe von
                            <emphasis>x</emphasis> zu sein. Beide Verfahren erzielen gute Ergebnisse
                        bei <emphasis>x</emphasis> = 1000, Naïve Bayes auch bei
                            <emphasis>x</emphasis> = 500, SMO auch bei <emphasis>x</emphasis> =
                        2000. Dass Naïve Bayes tendenziell kleinere Wortvektoren bevorzugt dürfte
                        darin begründet sein, dass hier die Annahme statistischer Unabhängigkeit der
                        einzelnen Attribute gemacht wird, dass diese Annahme bei zunehmender
                        Wortvektorgröße aber immer unhaltbarer wird.</para>
                    <para>Desweiteren zeigt sich bei dieser Evaluation eine leichte Tendenz dafür,
                        dass bei gleichem <emphasis>x</emphasis>-Wert die Performance für
                            <emphasis>M</emphasis> = 1 am schlechtesten und für
                            <emphasis>M</emphasis> = 2 am besten ist, so dass <emphasis>M</emphasis>
                        = 3 eine Mittelstellung einnimmt. Diese Tendenz ist bei Naïve Bayes etwas
                        stärker zu beobachten als bei SMO. Im Bereich der besten beobachteten Werte
                        schlägt <emphasis>M</emphasis> = 3 allerdings <emphasis>M</emphasis> = 2. </para>
                    <para>Auf Basis dieser Auswertung, in der allerdings keine Maßnahmen getroffen
                        wurden, <glossterm linkend="gt_überanpassung">Überanpassung</glossterm> an
                        die Trainingsdaten zu vermeiden, wurde für das Arbeiten mit beiden Verfahren
                        die Parameterkombination <emphasis>x</emphasis> = 1000,
                            <emphasis>M</emphasis> = 3 gewählt. Diese Wahl wurde hingegen nicht als
                        Fixwert verstanden, da <emphasis>M</emphasis> = 3 für besonders kleine
                        Trainingsklassen mit nur wenigen Instanzen ungünstig sein kann, denn es
                        könnte sein, dass nur wenige oder schlimmstenfalls keine Wörter in den
                        Instanzen dieser Klasse die Termfrequenzschwelle überschreiten.</para>
                </sect3>
            </sect2>
        </sect1>
        <sect1 xml:id="ch_Avvelauf"> <!-- Textvorverarbeitung zur Eigenschaftsanalyse (Avvelauf) -->
            <title>Textvorverarbeitung zur Eigenschaftsanalyse</title>
            <sidebar>
                <?dbfo sidebar-width="7.5cm"?>
                <?dbfo float-type="left"?>
                <figure xml:id="abb5"><!-- floatstyle="left"-->
                    <title>Beispiel für die Struktur eines Avve-Eingabeordners</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata format="png" fileref="../img/Abb5_Avve_Eingabeordner.png" width="6.8cm"><!-- width is used for FOP; for other FO processors this should be contentwidth="9"--></imagedata>
                        </imageobject>
                    </mediaobject>
                </figure>
            </sidebar>
            <para><indexterm class="startofrange" xml:id="idt_Avve01"><primary>Avve</primary></indexterm>Zur Aufbereitung der im Korpus vorliegenden 
                <glossterm linkend="gt_ebook">E-Books</glossterm> im <glossterm linkend="gt_epub">EPUB</glossterm>-Format für eine Weiterverarbeitung
                mit <glossterm linkend="gt_weka">Weka</glossterm>-Algorithmen wurde ein Java-Programm namens <glossterm
                    linkend="gt_avve">Avve</glossterm> geschrieben. Der Name ist ein Akronym für „Automatische Verschlagwortung für E-Books“, was darauf
                hindeutet,  dass ursprünglich eine Integration der Weka-Bibliotheken geplant war, so dass die Trainings-,
                Evaluations- und Anwendungsphase des Maschinenlernens vollständig über Avve
                abgebildet werden können. Dieses Ziel konnte aus Kapazitätsgründen noch nicht<!-- allgemein-->
                verwirklicht werden, so dass Avve aktuell vor allem der Vorverarbeitung und Aufbereitung der E-Book-Daten dient. Avve
                speichert Dateien im <indexterm><primary>ARFF</primary></indexterm>ARFF- oder 
                <indexterm><primary>XRFF</primary></indexterm>XRFF-Format, welche die speziellen, bevorzugten
                Eingabeformate für Weka sind. Eine Dokumentation des ARFF-Formates findet sich unter <link xlink:href="https://weka.wikispaces.com/ARFF"/>. 
                In diesem Abschnitt geben wir einen kurzen Überblick über die Aufgaben von Avve und die Reihenfolge der einzelnen Schritte. Für eine Dokumentation der
                Entwurfsprinzipien und Aufrufparameter sei auf den Anhang verwiesen (<xref linkend="app_Avve"/>).</para>
            <para>Avve erwartet als Eingabe eine Ordnerstruktur, in der die Trainings- oder Testdaten jeweils in einem Unterordner abgelegt sind, dessen 
                Ordnername der Klasse entspricht, zu der die Trainingsinstanz gehört. Eine Ordnerstruktur zum Lernen der <indexterm><primary>Warengruppe</primary>
                </indexterm>VLB-Warengruppen könnte also beispielsweise aussehen wie in <xref linkend="abb5"/> gezeigt.</para>
            <para>Für den Fall einer <indexterm><primary>Multilabel-Klassifikation</primary></indexterm>Multilabel-Klassifikation, wie wir sie bei
                Thema <indexterm><primary>Thema-Klassifikation</primary></indexterm> 
                vornehmen, kann der Ordnername in einer kommagetrennten Liste bestehen. Avve weist den E-Book-Dokumenten in einem solchen Ordner &#x2012; bei 
                Angabe eines entsprechenden Aufrufparameters &#x2012; alle der im Ordnernamen genannten Klassen zu.</para>
            <para>Avve läuft in zwei Iterationen über die Eingabedaten, da einige relevante Informationen erst vorhanden sind, sobald alle Dokumente bereits 
                einmal verarbeitet wurden. Die beiden Läufe sowie die anschließende weitere Datenaufbereitung in der Knowledge-Flow-Komponente von Weka werden
                in <xref linkend="abb6"/> veranschaulicht und im weiteren Text erläutert.</para>
            <figure xml:id="abb6" pgwide="0"><!-- Verarbeitungsschritte Avve und Weka-->
                <title>Verarbeitungsschritte Avve und Weka</title>
                <mediaobject>
                    <imageobject>
                        <imagedata format="svg" fileref="../img/Abb6_Verarbeitungsläufe.svg" width="17.5cm"><!-- width is used for FOP; for other FO processors this should be contentwidth="9"--></imagedata>
                    </imageobject>
                </mediaobject>
            </figure>
            <para>Im ersten Lauf werden die EPUB-Dateien in ein temporäres Verzeichnis entzippt und sowohl der Volltext als auch die in den EPUB-Metadaten 
                verpflichtend anzugebende Dokumenten-ID ermittelt. Verlage verwenden zu diesem Zweck häufig die <glossterm linkend="gt_isbn">ISBN</glossterm>. Da
                es unter Umständen vorkommen kann, dass zwei nicht ganz identische Textausgaben dieselbe ISBN aufweisen, wir für die Verwendung der ID im 
                <indexterm><primary>Lucene-Index</primary></indexterm>Lucene-Index aber für jedes Dokument eine eindeutige ID benötigen, hängt Avve die Dateigröße an die ermittelte ID an. Die heuristische Annahme ist,
                dass zwei unterschiedliche Bücher mit identischer ISBN sich in der Dateigröße unterscheiden müssen. Tun sie dies nicht, so handelt es sich mit 
                sehr großer Wahrscheinlichkeit um zwei Dateien mit dem gleichen Buchtext, so dass diese in der Lucene-Indizierung nicht unterschieden werden 
                müssen<footnote><para>Vgl. zu den Versuchen, Duplikate zu vermeiden, auch <xref linkend="ch_Korpus"/>.</para></footnote>.</para>
            <para>Der Volltext des E-Books befindet sich im EPUB innerhalb von XHTML-Dateien. Eine Extraktion des Volltextes meint nun im Wesentlichen den Teil
                der XHTML-Datei, welcher beim Öffnen dieser Datei in einem Browser angezeigt würde. Dabei muss darauf geachtet werden, dass Wörter nicht „aneinander kleben“
                bleiben und gegebenenfalls Zeilenumbrüche oder Leerzeichen eingefügt werden müssen, damit die spätere
                <glossterm linkend="gt_tokenisierung">Tokenisierung</glossterm> des Textes zuverlässig funktioniert.</para>
            <para>Im Zuge dieses Verarbeitungsschrittes wird auch der in den EPUB-Metadaten hinterlegte ISO-Sprachcode geprüft: E-Books, welche nicht mit dem Code
                für die deutsche Sprache markiert sind, werden ignoriert, da unser Maschinenlernsetup nur in einem Umfeld funktioniert, in dem der Großteil der 
                Texte monolingual ist. Mehrsprachige Bücher, wie sie in einigen Warengruppen vorkommen, werden verarbeitet, sofern sie in den EPUB-Metadaten 
                als ersten von möglicherweise mehreren Sprachcodes Deutsch angeben.</para>
            <para>Schließlich werden hier auch die ersten Attributwerte für die Weka-Algorithmen ermittelt, welche wir in <xref linkend="ch_FileProp"/> besprochen
                haben: Anzahl der Grafiken und der Kapitel sowie die Angaben zur Kapitelhierarchie.</para>

            <para>Der nächste Schritt des ersten Verarbeitungslaufs besteht in der Ausführung diverser Transformationen auf dem ermittelten Volltext. Einige dieser
                Transformationen sind hart codiert und werden in jedem Fall ausgeführt, andere nur dann, wenn ein entsprechender Kommandozeilenparameter beim 
                Avve-Aufruf übergeben wurde. Die Standardtransformationen sind:</para>
            <itemizedlist spacing="compact">
                <listitem>
                    <para>Satzerkennung: Der Volltext wird in einzelne Sätze zerlegt.</para>
                </listitem>
                <listitem>
                    <para><indexterm><primary>Tokenisierung</primary></indexterm>Tokenisierung: Die Sätze werden in einzelne Wörter zerlegt.</para>
                </listitem>
                <listitem>
                    <para>Entfernen von Interpunktion</para>
                </listitem>
                <listitem>
                    <para>Berechnung von Worthäufigkeiten: Hier werden alle vorkommenden Wortformen gezählt, da noch keine Lemmatisierung durchgeführt ist.
                        Hieraus wird später die Gesamtzahl der Wörter und die durchschnittliche Wortlänge berechnet.</para>
                </listitem>
                <listitem>
                    <para>Wortartenerkennung (Part-of-Speech-Tagging)</para>
                </listitem>
                <listitem>
                    <para>Entfernen von Zahlen: Alle im Text vorkommenden Zahlen werden durch # ersetzt.<footnote><para>Ohne den umgebenden Kontext haben 
                        Zahlen in einem Text oft wenig Bedeutung und werden deshalb beispielsweise auch in der Suchindizierung häufig ignoriert (<biblioref
                        linkend="bib_BR99"/>, S. 166). <biblioref linkend="bib_GS04"/> verzichten auch in ihrer Multilabel-Klassifikation auf alle Zahlen.</para></footnote></para>
                </listitem>
                <listitem>
                    <para>Lemmatisierung</para>
                </listitem>
            </itemizedlist>
            <para>Anschließend wird der lemmatisierte Text in einen <indexterm><primary>Lucene-Index</primary></indexterm><glossterm linkend="gt_lucene">Lucene-Index</glossterm>
                geschrieben. Dieser Index besteht aus zwei Feldern: einer Dokument-ID und einem Volltextfeld für den lemmatisierten Text. Alle Wörter werden im Index 
                in Kleinbuchstaben gespeichert und neben den <indexterm><primary>Stopword</primary></indexterm>Stopwords<footnote><para>Vgl. die Ausführungen zu Stopwords in
                    <xref linkend="ch_LexSemProp_Wortvektor"/>.</para></footnote> werden Wörter, die kürzer als 3 und länger als 80 Buchstaben sind, ausgeschlossen.</para>
            <para>Der erste Verarbeitungsdurchlauf endet nun damit, dass die für das E-Book-Dokument ermittelten Daten in einer temporären Datei auf Festplatte gesichert werden,
                denn es ist nicht zu erwarten, dass bei großer E-Book-Zahl diese Daten dauerhaft im Arbeitsspeicher eines heute handelsüblichen Computers gehalten werden
                können.</para>
            <para>Im zweiten Durchlauf wird dann aus jeder temporären Datei eine Weka-Datei im <indexterm><primary>XRFF</primary></indexterm>XRFF-Format generiert, wobei auf 
                Informationen im Lucene-Index zurückgegriffen wird, der nun, nach vollständigem ersten Durchlauf durch alle Eingabedateien, alle Lemmata des gesamten Korpus
                enthält und somit für dokumentübergreifende TF/IDF-Berechnungen zur Verfügung steht. Dies ist für die Erstellung der in <xref linkend="ch_AttribEval_StringToWordVector"/>
                besprochenen Liste der wichtigsten Wörter notwendig. Zuletzt werden dann die einzelnen XRFF-Dateien zu einem gemeinsamen XRFF-File für das gesamte Korpus 
                zusammengefasst. In diesem Schritt wird auch eine Liste aller Klassen zusammengestellt, welche von Weka als Enumeration aller möglichen Werte benötigt wird.</para>
            <para>Die beiden beschriebenen Avve-Durchläufe werden je einmal für die Trainingsdaten und einmal für die Testdaten durchgeführt, wobei beide Durchläufe in denselben
                Lucene-Index schreiben. Als Resultat erhält man jeweils eine XRFF-Datei für die Trainings- und eine für die Testdaten.<indexterm class="endofrange" startref="idt_Avve01"/></para>
            <figure xml:id="abb7" pgwide="1">
                <title>Aufbau des Wortvektors mit Hilfe von Weka-Knowledgeflow</title>
                <indexterm><primary>Weka</primary><secondary>Knowledgeflow</secondary></indexterm>
                <mediaobject>
                    <imageobject>
                        <imagedata format="png" fileref="../img/Abb7_KF_StringToWordVector.png" width="19cm"><!-- width is used for FOP; for other FO processors this should be contentwidth="9"--></imagedata>
                    </imageobject>
                </mediaobject>
            </figure>
            <para>Wie erwähnt erfolgt die Erstellung des Wortvektors auf Basis der gesamten Wortlisten aller Trainings- und Testinstanzen mit Hilfe der Weka-Komponente
                    <emphasis>Knowledgeflow</emphasis>. In Knowledgeflow lassen sich Verarbeitungspipelines aus Weka-Algorithmen grafisch aufbauen und speichern. Sie
                können anschließend sowohl in der Weka-GUI als auch via Kommandozeilenaufruf oder
                Einbettung in ein eigenes Programm ausgeführt werden. Die grafische Definition der
                Wortvektorerstellung ist in <xref linkend="abb7"/> gezeigt. Nach dem Laden werden
                Trainings- und Testdaten jeweils mit einem Flag angereichert und dann in dem
                    <emphasis>Appender</emphasis>-Element zusammengeführt. Da
                    <code>StringToWordVector</code> mit seinen <emphasis>M</emphasis>- und
                    <emphasis>W</emphasis>-Parametern auf Basis der zu lernenden Klassen arbeitet,
                wird das Klassenattribut, also in unserem Fall die VLB-Warengruppe, im
                <emphasis>Class&#173;Assigner</emphasis> gekennzeichnet. Nachdem der Wortvektor
                erzeugt ist, werden die Wortlisten, welche aus Weka-Sicht ein Stringattribut
                darstellen, gelöscht, denn die anschließenden Weka-Komponenten Naïve Bayes und SMO
                verweigern bei Eingabe von stringwertigen Attributen ihren Dienst. Vor dem Speichern
                der beiden Datensätze werden sie anhang des zuvor gesetzten Flags wieder in
                Trainigs- und Testdaten getrennt (Komponente <emphasis>TrainAndTestSetSplitter</emphasis> in der Abbildung oben).</para>
        </sect1>
    </chapter>
    <chapter xml:id="ch_Evaluation"><!-- Basiswerte zur qualitativen Einschätzung der Maschinenlernverfahren -->
        <info><title>Basiswerte zur qualitativen Einschätzung der Maschinenlernverfahren</title></info>
        <para>In der Erforschung von Maschinenlernverfahren ist es nicht nur üblich, sondern schlichtweg notwendig, die erzielten Ergebnisse statistisch zu 
            erfassen und qualitativ zu bewerten. Um für eine Vergleichbarkeit zu sorgen, haben sich Kennzahlen etabliert, von denen die wichtigsten kurz 
            vorgestellt werden. Eine Einschätzung der erzielten Klassifikationsqualität bedarf jedoch nicht nur der Kennzahlen, sondern auch der Vergleichswerte, 
            welche die Frage beantworten können: Wie gut klassifiziert das hier vorgestellte Verfahren für E-Books im Vergleich mit anderen Verfahren?</para>
        <para>Da mir bisher keine Studie mit gleicher Aufgabenstellung bekannt ist, gibt es auch keine bereits vorliegenden Vergleichswerte für die qualitative
            Einschätzung der vorliegenden Untersuchung. Um dieses Defizit auszugleichen, werden in diesem Kapitel einige statistische Basislinien gezogen: Welches
            qualitative Ergebnis würde eine zufällige Klassifikation der E-Books erbringen? Wie viel besser verhalten sich Naïve Bayes und SMO im Vergleich mit 
            den einfachstmöglichen Klassifikationsverfahren? Und wie hoch ist eigentlich die Fehlerquote von Menschen bei der Klassifikation? Wie weit sind 
            unsere automatisierten Verfahren von der E-Book-Klassifikation durch Menschen entfernt?</para>
        <sect1 xml:id="ch_Evaluation_Kennzahlen"> <!-- Evaluationskennzahlen -->
            <title>Allgemeine Kennzahlen der Klassifikationsqualität</title>
            <para>In den vorangehenden Kapiteln wurde bereits mehrmals deutlich, dass in der vorliegenden Untersuchung das Korpus im Verhältnis 2 : 1 in 
                eine Trainings- und eine Testmenge aufgeteilt wurde. Dies geschah aus dem Bewusstsein, dass wir keine zuverlässigen Qualitätsstatistiken erhalten,
                wenn wir ein und dieselben Daten sowohl für das Training der Klassifikatoren als auch für deren <indexterm><primary>Evaluation</primary></indexterm>
                Evaluation verwenden. Das Ziel bei der Klassifikation von Daten durch Maschinenlernen besteht ja gerade darin, ein Modell aufzubauen, welches zukünftige,
                noch nicht bekannte Daten möglichst gut klassifiziert. Würden wir unsere Klassifikationsmodelle mithilfe der Trainingsdaten evaluieren, könnten wir lediglich 
                eine Aussage darüber treffen, wie gut das Modell bereits bekannte Daten einordnet und dabei mit großer Wahrscheinlichkeit ein <indexterm><primary>
                    Überanpassung</primary></indexterm> <glossterm linkend="gt_überanpassung">überangepasstes</glossterm> Modell erzeugen. Durch eine von der Trainingsmenge
                unabhängige Testmenge werden eben diese zukünftigen, noch unbekannten Daten simuliert bzw. repräsentiert.</para>
            <para>Unser Vorgehen bedarf aber auch aus einer anderen Perspektive noch der Rechtfertigung. Gerade wenn die Menge der Trainingsdaten klein ist &#x2012; und
                das trifft aus Sicht des Data Minings auf unser Korpus von lediglich <numberOfInstances/> E-Books auf <numberOfClasses/> Klassen zu &#x2012; würde
                sich das Trainieren mit zehnfacher <indexterm><primary>Kreuzvalidierung</primary></indexterm>Kreuzvalidierung anbieten, einem Standardverfahren bei 
                der Bewertung von Maschinenlernen mit kleinen Trainingsmengen (<biblioref linkend="bib_WFHP17"/>, S. 167-168; <biblioref linkend="bib_Ert16"/>, S. 232-233).
                Bei diesem Verfahren werden zehn etwa gleich große und im Bezug auf die Klassen etwa gleich verteilte Schichten gebildet (<emphasis>Stratification</emphasis>), von denen in jedem Durchgang 
                9 Schichten zum Trainieren des Klassifikators und eine Schicht zum Testen verwendet wird. Dieses Verfahren wird zehnmal wiederholt, bis jede Schicht genau
                einmal als Testmenge gedient hat. Aus den Evaluationsergebnissen aus jedem dieser zehn Läufe wird ein Durchschnittswert gebildet, welcher dann als
                Gesamtergebnis betrachtet wird.</para>
            <para>Diese Kreuzvalidierung mit Bildung von Schichten gleicher Klassenverteilung wird vor allem dann statistisch sehr robust und aussagekräftig, wenn sie 
                nicht nur einmal, sondern 10 Mal durchgeführt wird. Hier wurde dennoch von ihrer Verwendung abgesehen, da wir an einer zumindest stichprobenhaften 
                manuellen Überprüfung einzelner falsch klassifizierter Datensätze interessiert sind. Der Hintergrund dieses Wunsches ist es, dass wir aufgrund der
                Abgrenzungsschwierigkeiten der VLB-Warengruppen (vgl. <xref linkend="ch_WGSdDB"/>) der Ansicht sind, dass es neben eindeutig richtigen und falschen
                Klassifikationen auch Grenzfälle gibt, in denen zwei oder sogar mehr Warengruppen als richtig gelten können. Eine Überprüfung, ob eine objektiv, d. h. auf 
                die vorgegebene Warengruppe bezogen, falsche Klassifikation vielleicht doch „fast richtig“ ist, sollte die Überprüfung der Klassifikationsergebnisse 
                in der Testmenge nicht zu sehr erschwert werden<footnote><para>Unter Aufbau eines sehr ausführlichen Loggingverhaltens wäre eine solche Überprüfung 
                wohl auch bei Verwendung von zehnfacher Kreuzvalidierung möglich gewesen, doch wäre der Speicherbedarf beträchtlich geworden. Schon bei dem Versuch,
                das gelernte SMO-Modell eines einzelnen Durchlaufs in Weka durch Aufruf der entsprechenden <code>toString()</code>-Methode auszugeben, scheiterte daran,
                dass die maximale Stringlänge in Java von ca. 2 GB überschritten wurde.</para></footnote>.</para>
            <sect2 xml:id="ch_Evaluation_Kennzahlen_Erfolgsquote">
                <title>Erfolgsquote</title>
                <para>Das einfachste und naheliegendste statistische Gütekriterium bei der Klassifikation ist die Erfolgsquote. Mit S = Anzahl der korrekt 
                    klassifizierten Datensätze und N = Gesamtzahl der Testdatensätze ergibt sich die beobachtete Erfolgsquote aus der Gleichung <inlineequation>
                        <mathphrase>f = S / N</mathphrase></inlineequation>. Diese Quote wurde bereits in <xref linkend="ch_AttribEval_StringToWordVector"/> verwendet.
                    Mit allgemeinen statistischen Gesetzmäßigkeiten lassen sich aus dieser Quote auch Aussagen über Konfidenzintervalle ableiten, z.&#160;B. „Mit x-prozentiger 
                    Wahrscheinlichkeit liegt die Quote korrekt klassifizierter Daten zwischen y Prozent und z Prozent“ (<biblioref linkend="bib_WFHP17"/></para>
            </sect2>
            <sect2 xml:id="ch_Evaluation_Kennzahlen_Relevanz-Sensitivität">
                <title>Trefferquote und Genauigkeit</title>
                <para><indexterm class="startofrange" xml:id="idt_016"><primary>Genauigkeit</primary></indexterm><indexterm class="startofrange" xml:id="idt_017">
                    <primary>Trefferquote</primary>
                </indexterm>Wenn wir die Testmenge nicht als Ganzes, sondern jede Klasse C<subscript>i</subscript> einzeln betrachten, lassen sich Aussagen darüber treffen, für welche
                    Klassen der gelernte Klassifikator bessere und für welche Klassen er schlechtere Entscheidungen trifft. Wir unterteilen dazu die Testmenge bezüglich 
                    C<subscript>i</subscript> in folgende Teilmengen:</para>
                <itemizedlist spacing="compact">
                    <listitem>
                        <para>TP („true positives“) als Menge der Datensätze, welche tatsächlich zur Klasse C<subscript>i</subscript> gehören und vom Klassifikator auch 
                            als solche gekennzeichnet wurden,</para>
                    </listitem>
                    <listitem>
                        <para>TN („true negatives“) als Menge der Datensätze, welche nicht zur Klasse C<subscript>i</subscript> gehören und vom Klassifikator auch nicht
                            als solche gekennzeichnet wurden,</para>
                    </listitem>
                    <listitem>
                        <para>FP („false positives“) als Menge der Datensätze, welche nicht zur Klasse C<subscript>i</subscript> gehören, vom Klassifikator aber als solche
                            gekennzeichnet wurden,</para>
                    </listitem>
                    <listitem>
                        <para>FN („false negatives“) als Menge der Datensätze, welche zur Klasse C<subscript>i</subscript> gehören, vom Klassifikator aber 
                            fälschlicherweise nicht als solche gekennzeichnet wurden.</para>
                    </listitem>
                </itemizedlist>
                <para>Die Trefferquote (engl. <indexterm><primary>Recall</primary></indexterm>Recall) 
                    gibt den Quotienten von korrekt klassifizierten Datensätzen der Klasse C<subscript>i</subscript> an der Gesamtzahl der Datensätze der Klasse
                    C<subscript>i</subscript> an: </para>
                <equation><!-- Formel Trefferquote / Recall -->
                    <mml:math>
                        <mml:mi>T</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mfrac>
                            <mml:mrow>
                                <mml:mi>TP</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                                <mml:mi>TP</mml:mi>
                                <mml:mo>+</mml:mo>
                                <mml:mi>FN</mml:mi>
                            </mml:mrow>
                        </mml:mfrac>
                    </mml:math>
                </equation>
                <para>Die Aussagekraft der Trefferquote ist also die erwartete Wahrscheinlichkeit, dass ein zufällig ausgewähltes Dokument der Klasse C<subscript>i</subscript>
                    auch tatsächlich als ein solches klassifiziert wird. Die Trefferquote ist somit ein Maß der <indexterm><primary>Vollständigkeit</primary>
                    </indexterm>Vollständigkeit des gelernten Klassifikators in Bezug auf die Klasse C<subscript>i</subscript> (<biblioref linkend="bib_Seb02"/>, S. 33).</para>
                <para>Die Genauigkeit (engl. <indexterm><primary>Precision</primary></indexterm>Precision)
                    ist definiert als Quotient der korrekt klassifizierten Datensätze der Klasse C<subscript>i</subscript> und der Gesamtheit der vom Klassifikator als
                    zu Klasse C<subscript>i</subscript> gehörig angesehenen Datensätze:</para>
                <equation><!-- Formel Genauigkeit / Precision -->
                    <mml:math>
                        <mml:mi>G</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mfrac>
                            <mml:mrow>
                                <mml:mi>TP</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                                <mml:mi>TP</mml:mi>
                                <mml:mo>+</mml:mo>
                                <mml:mi>FP</mml:mi>
                            </mml:mrow>
                        </mml:mfrac>
                    </mml:math>
                </equation>
                <para>Die Genauigkeit drückt damit die Wahrscheinlichkeit aus, dass die Entscheidung eines Klassifikators, welcher ein zufällig ausgewähltes Dokument 
                    als zur Klasse C<subscript>i</subscript> gehörig klassifiziert, korrekt ist (<biblioref linkend="bib_Seb02"/>, S. 33).</para>
                <para>Mit den vier Konzepten lässt sich natürlich durch entsprechende Summierung auch wieder die Erfolgsquote definieren:</para>
                <equation> <!-- Erfolgsquote aus TP, TF, FN, FP -->
                    <mml:math>
                        <mml:mi>f</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mfrac>
                            <mml:mrow>
                                <mml:mi>TP</mml:mi>
                                <mml:mo>+</mml:mo>
                                <mml:mi>TN</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                                <mml:mi>TP</mml:mi>
                                <mml:mo>+</mml:mo>
                                <mml:mi>TN</mml:mi>
                                <mml:mo>+</mml:mo>
                                <mml:mi>FP</mml:mi>
                                <mml:mo>+</mml:mo>
                                <mml:mi>FN</mml:mi>
                            </mml:mrow>
                        </mml:mfrac>
                    </mml:math>
                </equation>
                <para>Die beiden Maße Trefferquote (Recall) und Genauigkeit (Precision) lassen sich über verschiedene Formeln zu einem gemeinsamen Maß kombinieren.
                    So ist etwa das <indexterm><primary>F-Maß</primary></indexterm>F-Maß das gewichtete harmonische Mittel aus Genauigkeit und Trefferquote:</para>
                <equation>
                    <mathphrase>F = 2 · G · T / (G + T)</mathphrase>
                </equation>
                <para>Will man nun nicht nur eine Aussage bezüglich der Genauigkeit und Trefferquote einzelner Klassen treffen, sondern die Qualität des 
                    Klassifikators bezogen auf alle Klassen ausdrücken, so kann man Durchschnittswerte für die genannten Kriterien auf zwei Arten berechnen: Mittels
                    <indexterm class="startofrange" xml:id="idt_006"><primary>Micro-Averaging</primary></indexterm>Micro-Averaging und <indexterm class="startofrange" 
                        xml:id="idt_007"><primary>Macro-Averaging</primary></indexterm>Macro-Averaging. Beim Micro-Averaging werden zunächst die TP-, FP- und FN-Werte aller
                    Klassen summiert und dann in die Genauigkeits- und Trefferquotenformeln eingesetzt, beim Macro-Averaging wird zunächst für jede Klasse ein „lokaler“
                    Genauigkeits- und Trefferquotenwert berechnet und anschließend der Durchschnitt aller Genauigkeitswerte bzw. Trefferquoten genommen.<indexterm class="endofrange" startref="idt_016"/></para>
                <equation> <!-- Micro-averaged Genauigkeit / Precision -->
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <msub>
                            <mi>G</mi>
                            <mtext>micro</mtext>
                        </msub>
                        <mo>=</mo>
                        <mfrac>
                            <mrow>
                                <munder>
                                    <mo>&#x2211;<!-- ∑ --></mo>
                                    <mi>c</mi>
                                </munder>
                                <mi>T</mi>
                                <msub>
                                    <mi>P</mi>
                                    <mi>c</mi>
                                </msub>
                            </mrow>
                            <mrow>
                                <munder>
                                    <mo>&#x2211;<!-- ∑ --></mo>
                                    <mi>c</mi>
                                </munder>
                                <mi>T</mi>
                                <msub>
                                    <mi>P</mi>
                                    <mi>c</mi>
                                </msub>
                                <mo>+</mo>
                                <munder>
                                    <mo>&#x2211;<!-- ∑ --></mo>
                                    <mi>c</mi>
                                </munder>
                                <mi>F</mi>
                                <msub>
                                    <mi>P</mi>
                                    <mi>c</mi>
                                </msub>
                            </mrow>
                        </mfrac>
                    </math>
                </equation>
                <equation> <!-- Mico-averaged Trefferquote / Recall -->
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <mspace linebreak="newline" />
                        <msub>
                            <mi>T</mi>
                            <mtext>micro</mtext>
                        </msub>
                        <mo>=</mo>
                        <mfrac>
                            <mrow>
                                <munder>
                                    <mo>&#x2211;<!-- ∑ --></mo>
                                    <mi>c</mi>
                                </munder>
                                <mi>T</mi>
                                <msub>
                                    <mi>P</mi>
                                    <mi>c</mi>
                                </msub>
                            </mrow>
                            <mrow>
                                <munder>
                                    <mo>&#x2211;<!-- ∑ --></mo>
                                    <mi>c</mi>
                                </munder>
                                <mi>T</mi>
                                <msub>
                                    <mi>P</mi>
                                    <mi>c</mi>
                                </msub>
                                <mo>+</mo>
                                <munder>
                                    <mo>&#x2211;<!-- ∑ --></mo>
                                    <mi>c</mi>
                                </munder>
                                <mi>F</mi>
                                <msub>
                                    <mi>N</mi>
                                    <mi>c</mi>
                                </msub>
                            </mrow>
                        </mfrac>
                    </math>
                </equation>
                <equation> <!-- Macro-averaged Genauigkeit / Precision -->
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <msub>
                            <mi>G</mi>
                            <mtext>macro</mtext>
                        </msub>
                        <mo>=</mo>
                        <mfrac>
                            <mrow>
                                <munder>
                                    <mo>&#x2211;<!-- ∑ --></mo>
                                    <mi>c</mi>
                                </munder>
                                <msub>
                                    <mi>G</mi>
                                    <mi>c</mi>
                                </msub>
                            </mrow>
                            <mrow>
                                <mo>|</mo>
                                <mi>C</mi>
                                <mo>|</mo>
                            </mrow>
                        </mfrac>
                    </math>
                </equation>
                <equation> <!-- Macro-averaged Trefferquote / Recall -->
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <msub>
                            <mi>T</mi>
                            <mtext>macro</mtext>
                        </msub>
                        <mo>=</mo>
                        <mfrac>
                            <mrow>
                                <munder>
                                    <mo>&#x2211;<!-- ∑ --></mo>
                                    <mi>c</mi>
                                </munder>
                                <msub>
                                    <mi>T</mi>
                                    <mi>c</mi>
                                </msub>
                            </mrow>
                            <mrow>
                                <mo>|</mo>
                                <mi>C</mi>
                                <mo>|</mo>
                            </mrow>
                        </mfrac>
                    </math>
                </equation>
                <para>Beide Verfahren können recht unterschiedliche Werte liefern, insbesondere, wenn die Klassenhäufigkeit ungleich verteilt ist. Macro-Averaging
                    ist sensibler
                    dafür, ob auch kleine Klassen mit wenigen Trainingsinstanzen gut gelernt wurden (<biblioref linkend="bib_Seb02"/>, S. 33).
                    <indexterm class="endofrange" startref="idt_007"/></para>
                <para>Für den hier vorliegenden Fall eines VLB-Warengruppen-Klassifikators mit mehreren Zielklassen werden beim Micro-Averaging
                    <emphasis>alle</emphasis> falsch klassifizierten Testinstanzen aus Sicht aller Klassen gezählt, so dass sich ergibt:</para>
                <equation> <!-- FP = FN bei Microaveraging -->
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <munder>
                            <mo>&#x2211;<!-- ∑ --></mo>
                            <mi>c</mi>
                        </munder>
                        <mi>F</mi>
                        <msub>
                            <mi>P</mi>
                            <mi>c</mi>
                        </msub>
                        <mo>=</mo>
                        <munder>
                            <mo>&#x2211;<!-- ∑ --></mo>
                            <mi>c</mi>
                        </munder>
                        <mi>F</mi>
                        <msub>
                            <mi>N</mi>
                            <mi>c</mi>
                        </msub>
                    </math>
                </equation>
                <para>Daraus folgt wiederum unmittelbar, dass in diesem Fall G<subscript>micro</subscript> = T<subscript>micro</subscript>. Außerdem entsprechen 
                    diese beiden Micro-average-Werte wiederum genau der Erfolgsquote f, denn:</para>
                <equation> <!-- S = Summe(FP) + Summe (TP) -->
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <munder>
                            <mo>&#x2211;<!-- ∑ --></mo>
                            <mi>c</mi>
                        </munder>
                        <mi>F</mi>
                        <msub>
                            <mi>P</mi>
                            <mi>c</mi>
                        </msub>
                        <mo>+</mo>
                        <munder>
                            <mo>&#x2211;<!-- ∑ --></mo>
                            <mi>c</mi>
                        </munder>
                        <mi>T</mi>
                        <msub>
                            <mi>P</mi>
                            <mi>c</mi>
                        </msub>
                        <mo>=</mo>
                        <mi>N</mi>
                    </math>
                </equation>
                <para>Es erübrigt sich somit, bei der Gesamtevaluation eines Trainings- und Testlaufs zum Lernen von Warengruppen die Micro-Averaged-Berechnungen
                    durchzuführen, da das Ergebnis bereits an der einfacher zu berechnenden Erfolgsquote f abgelesen werden kann. Interessant kann es aber unter 
                    Umständen sein, Genauigkeit und Trefferquote für alle Klassen außer der Mehrheitsklasse micro-averaged zu berechnen und mit den entsprechenden
                    Werten für die Mehrheitsklasse zu vergleichen.<indexterm class="endofrange" startref="idt_006"/><indexterm class="endofrange" startref="idt_017"/></para>
            </sect2>
        </sect1>
        <sect1 xml:id="ch_Evaluation_EBookKlassifikation"> <!-- Vergleichswerte für die E-Book-Klassifikation -->
            <title>Vergleichswerte für die E-Book-Klassifikation</title>
            <para>Wenn für ein Klassifikationsvorhaben keine Benchmark-Werte aus früheren Untersuchungen zur Verfügung stehen, kann man als unterste 
                 Qualitätslinie die Klassifikation durch einen Zufallsgenerator einziehen. Selbstverständlich muss der zu überprüfende gelernte
                 Klassifikator besser sein als ein Zufallsgenerator.</para>
            <para>Da die Wahrscheinlichkeitsverteilung unserer Klassen im Voraus bekannt ist, sollte eine <indexterm><primary>Zufallsklassifikation</primary>
                 </indexterm>zufällige Klassifikation gleich verteilte 
                 Klassifikationen generieren. Wenn Warengruppe A 60%, Warengruppe B 35% und Warengruppe C 5% der Daten ausmachen würde, ergäbe sich für 
                 die tatsächliche Wahrscheinlichkeitsverteilung P(WG = A) = 0,6, P(WG = B) = 0,35 und P(WG = C) = 0,05. Sei RWG die zufällig gesetzte Warengruppe
                 eines zufälligen Klassifikators, so gilt ebenfalls P(RWG = A) = 0,6, P(RWG = B) = 0,35 und P(RWG = 0,05).</para>
            <para>Die Wahrscheinlichkeit, dass der zufällige Klassifikator die richtige Warengruppe wählt, ist P(RWG = WG), was wiederum bedeutet:</para>
            <equation>
                <mathphrase>
                    P(RWG = WG) = P(RWG = A)P(WG = A) + P(RWG=B)P(WG=B) + P(RWG=C)P(WG=C)
                </mathphrase>
            </equation>
            <para>Für unser oben gegebenes Beispiel wäre die Erfolgsquote eines zufälligen Klassifikators also:</para>
            <equation>
                <mathphrase>
                    P(RWG = WG) = 0,6 * 0,6 + 0,35 * 0,35 + 0,05 * 0,05 = 0,485
                </mathphrase>
            </equation>
            <para>Wendet man dieses Prinzip auf die Verteilung der <numberOfClasses/> VLB-Warengruppen in unserem Korpus an, so ergibt sich als Erfolgsquote 
                eines zufälligen Klassifikators der äußerst geringe Wert von <randomClassifierSuccessRate/>. Dieser Wert ist so niedrig, dass er als 
                Benchmark für eine ernst zu nehmende Maschinenlernanwendung nicht in Betracht kommt. Eine streng deterministische und äußerst einfache 
                Klassifikationsregel erbringt ein sehr viel besseres Ergebnis und stellt damit eine bessere Basislinie für die Erfolgsquote unserer trainierten 
                Klassifikatoren dar: <indexterm><primary>Zero Rule</primary></indexterm>Zero Rule, kurz 0R (vgl. <biblioref linkend="bib_Bro16"/>).
                Bei 0R vergibt der Klassifikator einfach für 
                jede Instanz die frequenteste Klasse. Die Erfolgsquote des 0R-Klassifikators entspricht dann immer genau dem prozentualen Anteil dieser 
                häufigsten Klasse. Ist z.&#160;B. die Warengruppe 112 mit einem Anteil von 17,9% aller Instanzen die häufigste Warengruppe im Korpus, so würde
                0R alle Instanzen der Warengruppe 112 korrekt klassifizieren (und die Instanzen aller anderen Warengruppen falsch) und käme damit insgesamt
                auf eine Erfolgsquote von ebenfalls 17,9%. Es ist somit offensichtlich, dass 0R eine bessere Basislinie für unseren Klassifikationserfolg 
                darstellt.</para>
            <para>Eine weitere Grundlinie der Klassfikationsqualität, die noch etwas über 0R liegt, lässt sich mit <indexterm><primary>One Rule</primary>
                </indexterm>One Rule (1R) ziehen (<biblioref linkend="bib_WFHP17"/>, S. 93-94). Der 1R-Algorithmus baut <glossterm linkend="gt_entscheidungsbaum">Entscheidungsbäume</glossterm>
                basierend auf einem einzigen Attribut der zu klassifizierenden Daten. Er probiert alle Attribute durch und evaluiert dabei die Erfolgsquote.
                Am Ende entscheidet er sich für das Attribut mit der höchsten Erfolgsquote. Der Aufbau des Entscheidungsbaumes pro Attribut geschieht wie folgt: 
                Numerische Attribute werden diskretisiert bzw. in Intervalle aufgeteilt, wobei eine Mindestgröße der Buckets vorgegeben wird, so dass sich z.&#160;B. in 
                jedem Intervall des Attributs mindestens sechs Instanzen befinden müssen<footnote><para>So erläutert es die Javadoc-Dokumentation der Weka-Klasse OneR. <biblioref 
                    linkend="bib_Hol93"/> spricht hingegen davon, dass sich in einem Attributintervall mindestens sechs Instanzen derselben Klasse befinden müssen (und 
                darüber hinaus möglicherweise weitere Instanzen anderer Klassen). Diese Variante scheint in Weka nicht implementiert zu sein.</para></footnote>. Der Entscheidungsbaum erstellt dann auf jeder Stufe jeweils einen Blatt-Kindknoten für die am 
                häufigsten vorkommende Klasse und einen weiteren Entscheidungsknoten auf der nächstniedrigen Stufe für die restlichen Daten.</para>
            <para>Bezogen auf unsere E-Book-Daten evaluiert 1R das Attribut für die Quote finiter Verben als besten Performer und erreicht damit Erfolgsquoten von 
                über 23%. Man beachte, dass sich dieses Attribut auch in unserer Attributbewertung mittels InfoGain in <xref linkend="ch_AttribEval_Entropie"/> als das
                informationshaltigste herausgestellt hat, obwohl das Attributranking in 1R nicht auf Basis der Entropie, sondern auf Basis der Fehlerquote berechnet
                wird (<biblioref linkend="bib_Hol93"/>, S. 2). Der von 1R aufgebaute Entscheidungsbaum sieht ausschnittsweise so aus wie in <xref linkend="abb8"/>
                gezeigt.</para>
            <para>Die drei hier vorgestellten Verfahren zur Berechnung eines Vergleichswertes stellen keine besondere Herausforderung dar; die Verfahren Naïve Bayes und 
                SMO erreichen auf unserem E-Book-Korpus auch ohne besondere Optimierungen deutlich bessere Erfolgsquoten. Eine bessere Benchmark stellt deshalb eine
                Zahl dar, welche wir bei der Vorstellung unseres Korpus in <xref linkend="ch_Korpus"/> bereits erwähnt haben: <modifiedInstancesRatio/> der Trainings-
                und Testinstanzen waren entweder eindeutig falsch klassifiziert oder waren inkonsistent in Bezug auf eine einheitliche Klassifikationsweise. Wir
                können diesen Wert als Fehlerquote bei der Klassifikation durch Menschen ansehen, der idealerweise auch von unserer Maschinenklassifikation erreicht
                werden sollte.</para>
            <figure xml:id="abb8">
                <title>Ausschnitt aus dem 1R-Entscheidungsbaum für VLB-Warengruppen</title>
                <mediaobject>
                    <imageobject>
                        <imagedata format="svg" fileref="../img/Abb8_OneR_Entscheidungsbaum.svg" width="16cm"><!-- width is used for FOP; for other FO processors this should be contentwidth="9"--></imagedata>
                    </imageobject>
                </mediaobject>
            </figure>
        </sect1>
    </chapter>
    <chapter xml:id="ch_NaiveBayes"> <!-- WG-Klassifikation mit Naïve Bayes -->
        <info>
            <title>WG-Klassifikation mit Naïve Bayes</title>
        </info>
        <sect1 xml:id="ch_NB_intro"> <!-- Darstellung des Verfahrens-->
            <title>Darstellung des Verfahrens</title>
            <para><indexterm class="startofrange" xml:id="idt_005"><primary>Naïve-Bayes-Klassifikator</primary></indexterm>Naïve Bayes ist ein probabilistisches 
                Maschinenlernverfahren, welches über eine lange Tradition im Data Mining verfügt und insbesondere in der Textklassifikation häufig eingesetzt 
                wird (z.&#160;B. <biblioref linkend="bib_Ert16"/>, S. 239; <biblioref linkend="bib_WFHP17"/>, S. 158). Das Verfahren erwies sich in unserer Sondierungsphase
                als dasjenige mit der zweitbesten Erfolgsquote und bewies somit, dass es seinem Ruf als als guter Textklassifikator immer noch gerecht wird.</para>
            <para>Wie der Name vermuten lässt, basiert das 
                Naïve-Bayes-Verfahren auf der Bayesschen Statistik. Bezogen auf die Dokumentklassifikation lässt sich die Bayessche Annahme wie folgt ausdrücken:
                Gesucht wird die <indexterm><primary>Wahrscheinlichkeit</primary><secondary>a posteriori</secondary></indexterm>A-posteriori-Wahrscheinlichkeit, dass 
                ein Dokument zu einer bestimmten Klasse von Dokumenten gehört. Bekannt ist die A-priori-Wahrscheinlichkeit, welche in der Wahrscheinlichkeitsverteilung
                der Klassen besteht (siehe <xref linkend="app_WG_dist"/>, deren Prozentwerte sich unmittelbar in die <indexterm><primary>Wahrscheinlichkeit</primary><secondary>a priori</secondary>
                </indexterm>A-priori-Wahrscheinlichkeit umrechnen lassen, indem man die Prozentzahlen als Dezimalzahlen zwischen 0 und 1 darstellt). Die A-priori-Wahrscheinlichkeit
                drückt somit die Wahrscheinlichkeit aus, dass ein zu klassifizierendes Dokument zu einer bestimmten Klasse gehört, bevor wir überhaupt einen Blick auf
                das Dokument geworfen haben (<biblioref linkend="bib_SP03"/>). Bekannt ist bzw. gelernt wird darüber hinaus eine Korrelation zwischen <indexterm><primary>Attribut</primary>
                </indexterm>Attributwerten und Dokumentklassen. Die Attributwerte von Dokumenten lassen sich für jedes Dokument berechnen und können dann unmittelbar
                „beobachtet“ werden. Sie stellen somit ein Datum oder eine <indexterm><primary>Evidenz</primary></indexterm>Evidenz in der Bayesschen Terminologie dar.</para>
            <para>Werfen wir einen Blick auf bzw. in das Dokument, erhalten wir konkrete Evidenzen und möchten berechnen, wie wahrscheinlich es ist, dass ein 
                Dokument der Klasse c genau diese Evidenzen aufweist: <inlineequation><mml:math><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>c</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>|</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">⃗</mml:mo></mml:mover><mml:mo>)</mml:mo>
                </mml:mrow></mml:math></inlineequation> drückt die bedingte A-Posteriori-Wahrscheinlichkeit einer Klasse c bei bekanntem bzw. gegebenem Attributvektor
                <inlineequation><mml:math><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">⃗</mml:mo></mml:mover></mml:math></inlineequation>.
                Wir können so für jede Klasse c<subscript>i</subscript> aus C einen Wahrscheinlichkeitswert
                berechnen und erhalten für das zu klassifizierende Dokument einen Vektor von Wahrscheinlichkeiten, dessen Länge der Anzahl der Klassen entspricht.
                Indem man die A-priori-Wahrscheinlichkeit mit der Evidenzwahrscheinlichkeit multipliziert, erhält man im Bayesschen Modell die letztlich gesuchte
                A-posteriori-Wahrscheinlichkeit:</para>
            <equation xml:id="formula_Bayes_Regel"> <!-- Formel der Bayes-Regel -->
                <mml:math>
                    <mml:mrow>
                        <mml:mi>P</mml:mi>
                        <mml:mo>(</mml:mo>
                        <mml:mi>c</mml:mi>
                        <mml:mo>|</mml:mo>
                        <mml:mover accent="true">
                            <mml:mi>x</mml:mi>
                            <mml:mo stretchy="true">⃗</mml:mo>
                        </mml:mover>
                        <mml:mo>)</mml:mo>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                        <mml:mfrac>
                            <mml:mrow>
                                <mml:mi>P</mml:mi>
                                <mml:mo>(</mml:mo>
                                <mml:mi>c</mml:mi>
                                <mml:mo>)</mml:mo>
                                <mml:mi>P</mml:mi>
                                <mml:mo>(</mml:mo>
                                <mml:mover accent="true">
                                    <mml:mi>x</mml:mi>
                                    <mml:mo stretchy="true">⃗</mml:mo>
                                </mml:mover>
                                <mml:mo>|</mml:mo>
                                <mml:mi>c</mml:mi>
                                <mml:mo>)</mml:mo>
                            </mml:mrow>
                            <mml:mrow>
                                <mml:mi>P</mml:mi>
                                <mml:mo>(</mml:mo>
                                <mml:mover accent="true">
                                    <mml:mi>x</mml:mi>
                                    <mml:mo stretchy="true">⃗</mml:mo>
                                </mml:mover>
                                <mml:mo>)</mml:mo>
                            </mml:mrow>
                        </mml:mfrac>
                    </mml:mrow>
                </mml:math>
            </equation>
            <para>Zugewiesen wird dem zu klassifizierenden Dokument dann diejenige Klasse, welche sich in der vorstehend
                skizzierten Berechnung als die wahrscheinlichste erweist (<biblioref linkend="bib_ER03"/>):</para>
            <equation> <!-- Naïve Bayes, Basisformel -->
                <mml:math display="block">
                    <mml:mrow>
                        <mml:mi>arg</mml:mi>
                        <mml:munder>
                            <mml:mi>max</mml:mi>
                            <mml:mrow>
                                <mml:mi>c</mml:mi>
                                <mml:mo stretchy="false">∈</mml:mo>
                                <mml:mi>C</mml:mi>
                            </mml:mrow>
                        </mml:munder>
                        <mml:mi>P</mml:mi>
                        <mml:mrow>
                            <mml:mo fence="true" stretchy="false">(</mml:mo>
                            <mml:mrow>
                                <mml:mrow>
                                    <mml:mi>c</mml:mi>
                                    <mml:mtext>&#160;</mml:mtext>
                                    <mml:mo>|</mml:mo>
                                    <mml:mtext>&#160;</mml:mtext>
                                    <mml:mover accent="true">
                                        <mml:mi>x</mml:mi>
                                        <mml:mo stretchy="true">⃗</mml:mo>
                                    </mml:mover>
                                </mml:mrow>
                            </mml:mrow>
                            <mml:mo fence="true" stretchy="false">)</mml:mo>
                        </mml:mrow>
                    </mml:mrow>
                </mml:math>
            </equation>
            <para>Die <emphasis>argmax</emphasis>-Funktion gibt denjenigen Wert c zurück, für den P maximal wird. <emphasis>C</emphasis> ist hierbei die
                Menge der Klassen und <inlineequation><mml:math><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">⃗</mml:mo></mml:mover>
                </mml:math></inlineequation> der Vektor aller Attributwerte des zu klassifizierenden Dokuments.</para>
            <para>Nachdem das Bayessche am Naïve-Bayes-Verfahren geklärt ist, bleibt noch zu erläutern, was daran „naiv“ ist. Der Naïve-Bayes-Algorithmus behandelt alle
                Attribute eines Datensatzes als gleichwertig und <glossterm linkend="gt_stochastisch_unabhängig">stochastisch unabhängig</glossterm>
                <indexterm><primary>Unabhängigkeit</primary><secondary>stochastische</secondary></indexterm>voneinander. Denn nur unter der Bedingung der 
                Attributunabhängigkeit ist es erlaubt, die Wahrscheinlichkeiten der Attribute bzw. Evidenzen miteinander zu multiplizieren
                (<biblioref linkend="bib_WFHP17"/>, S. 99; <biblioref linkend="bib_JL95"/>, S. 339):</para>
            <equation xml:id="formula_wahrscheinlichkeit_attributvektor"> <!-- Wahrscheinlichkeit eines Vektors unabhängiger Attribute -->
                <mml:math>
                    <mml:mstyle displaystyle="false" scriptlevel="0">
                        <mml:mi>P</mml:mi>
                        <mml:mo>(</mml:mo>
                        <mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">⃗</mml:mo></mml:mover>
                        <mml:mo>)</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:apply>
                            <mml:product/>
                            <mml:bvar>
                                <mml:ci>i</mml:ci>
                            </mml:bvar>
                            <mml:lowlimit>
                                <mml:ci>1</mml:ci>
                            </mml:lowlimit>
                            <mml:uplimit>
                                <mml:ci>n</mml:ci>
                            </mml:uplimit>
                            <mml:apply>
                                <mml:ci type="fn">P</mml:ci>
                                <mml:msub>
                                    <mml:ci>x</mml:ci>
                                    <mml:ci>i</mml:ci>
                                </mml:msub>
                            </mml:apply>
                        </mml:apply>
                        <mml:mtext>&#160;, mit n = Anzahl der Attribute</mml:mtext>
                    </mml:mstyle>
                </mml:math>
            </equation>
            <para>Die Annahme der Unabhängigkeit der Attribute trifft mit Blick auf „aus dem Leben gegriffene“ Datensätze jedoch meist
                nicht zu (<biblioref linkend="bib_WFHP17"/>, S. 96; <biblioref linkend="bib_DPHS98"/>) und ist aus diesem Grund naiv.
                Das Naïve-Bayes-Verfahren ist dennoch seit Jahrzehnten in der Textklassifikationspraxis sehr beliebt, da es trotz der vermuteten oder
                tatsächlichen Missachtung des Unabhängigkeitspostulats gute Ergebnisse auch im
                Vergleich mit anderen Maschinenlernverfahren erzielt (<biblioref linkend="bib_JL95"/>, S. 388). Jedoch lässt sich gewöhnlich bei 
                Anstieg der Zahl nicht unabhängiger Attribute eine Verschlechterung des Klassifikators beobachten.</para>
            <para>Da der Wertebereich unserer Attribute nicht diskret-nominell, sondern kontinuierlich ist, lässt sich die Wahrscheinlichkeit eines
                Attributwerts nicht so leicht berechnen, wie die Wahrscheinlichkeit der nominellen Warengruppen-Klassen. Die Naïve-Bayes-Implementierung von 
                Weka nimmt in der Defaulteinstellung für jedes Attribut eine Gaußsche <indexterm><primary>Normalverteilung</primary></indexterm>Normalverteilung an
                und berechnet die <indexterm><primary>Wahrscheinlichkeitsdichte</primary></indexterm>Wahrscheinlichkeitsdichte auf der 
                Basis dieser Annahme. Hierzu werden in der Trainingsphase für jede Klasse und jedes Attribut der Median und die Standardabweichung berechnet. Ein 
                Auszug aus diesem Berechnungsergebnis wird in <xref linkend="tab_NB_Median_und_Stddev"/> gezeigt.<indexterm class="endofrange" startref="idt_005"/></para>
            <table frame="all" orient="port" pgwide="0" tocentry="1" xml:id="tab_NB_Median_und_Stddev"> <!-- Median und Standardabweichung einiger Attributwerte und Klassen -->
                <info>
                    <title>Median (&#956;) und Standardabweichung (&#963;) einiger Attributwerte im Naïve-Bayes-Klassifikator</title>
                </info>
                <tgroup cols="4" align="left" colsep="1" rowsep="1">
                    <colspec colname="attribut" colwidth="3*"/>
                    <colspec colname="wg112" colwidth="2*"/>
                    <colspec colname="wg250" colwidth="2*"/>
                    <colspec colname="wg481" colwidth="2*"/>
                    <thead>
                        <row>
                            <entry>Attribut</entry>
                            <entry>WG 112</entry>
                            <entry>WG 250</entry>
                            <entry>WG 481</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>Dateigröße (fileSizeInBytes)</entry>
                            <entry><para>&#956; = 2107906,23</para><para>&#963; = 2221825,88</para></entry>
                            <entry><para>&#956; = 5366943,70</para><para>&#963; = 7932747,43</para></entry>
                            <entry><para>&#956; = 2704132,29</para><para>&#963; = 2889005,16</para></entry>
                        </row>
                        <row>
                            <entry>Zahl der Kapitel erster Ebene (numberOfTopLevelChapters)</entry>
                            <entry><para>&#956; = 32,1979</para><para>&#963; = 30,429</para></entry>
                            <entry><para>&#956; = 30,5049</para><para>&#963; = 21,9742</para></entry>
                            <entry><para>&#956; = 27,7169</para><para>&#963; = 25,533</para></entry>
                        </row>
                        <row>
                            <entry>Zahl der Wörter pro Satz (wordsPerSentence)</entry>
                            <entry><para>&#956; = 18,4885</para><para>&#963; = 42,6971</para></entry>
                            <entry><para>&#956; = 12,4994</para><para>&#963; = 2,6502</para></entry>
                            <entry><para>&#956; = 18,9956</para><para>&#963; = 3,9949</para></entry>
                        </row>
                        <row>
                            <entry>Vokabelfülle (vocabularyRichness)</entry>
                            <entry><para>&#956; = 6,6046</para><para>&#963; = 1,7905</para></entry>
                            <entry><para>&#956; = 5,6898</para><para>&#963; = 1,4976</para></entry>
                            <entry><para>&#956; = 5,8278</para><para>&#963; = 1,3475</para></entry>
                        </row>
                        <row>
                            <entry>durchschnittliche Wortlänge (averageWordLength)</entry>
                            <entry><para>&#956; = 5,0921</para><para>&#963; = 0,1873</para></entry>
                            <entry><para>&#956; = 5,0361</para><para>&#963; = 0,2579</para></entry>
                            <entry><para>&#956; = 5,4322</para><para>&#963; = 0,26</para></entry>
                        </row>
                        <row>
                            <entry>Quote der Passivsätze (ratioOfPassiveSentences)</entry>
                            <entry><para>&#956; = 0,1297</para><para>&#963; = 0,0596</para></entry>
                            <entry><para>&#956; = 0,0829</para><para>&#963; = 0,0316</para></entry>
                            <entry><para>&#956; = 0,1462</para><para>&#963; = 0,0455</para></entry>
                        </row>
                        <row>
                            <entry>Quote der Adjektive (ratioOfAdjectives)</entry>
                            <entry><para>&#956; = 0,0667</para><para>&#963; = 0,0092</para></entry>
                            <entry><para>&#956; = 0,0632</para><para>&#963; = 0,0102</para></entry>
                            <entry><para>&#956; = 0,0769</para><para>&#963; = 0,0108</para></entry>
                        </row>
                        <row>
                            <entry>Vorkommen des Wortes „Abend“ (w_abend)</entry>
                            <entry><para>&#956; = 0,2995</para><para>&#963; = 0,458</para></entry>
                            <entry><para>&#956; = 0,25</para><para>&#963; = 0,433</para></entry>
                            <entry><para>&#956; = 0,0116</para><para>&#963; = 0,1667</para></entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <para><biblioref linkend="bib_JL95"/> weisen darauf hin, dass die Annahme der Normalverteilung für kontinuierliche Attributwerte nicht in allen
                Anwendungsbereichen zutrifft. Sie stellen als Alternative die Berechnung der Wahrscheinlichkeitsdichte auf Basis eines Kernel-Estimators vor 
                und nennen das resultierende Verfahren <indexterm><primary>Flexible Bayes</primary></indexterm>Flexible Bayes. Weka bietet diese Variante innerhalb 
                der <code>NaiveBayes</code>-Klasse über den Parameter <emphasis>K</emphasis> an. Auf unseren Korpusdaten ergibt sich bei Verwendung des 
                Kernel-Estimators anstelle der Normalverteilungsannahme allerdings eine um mehr als 2% geringere Erfolgsquote, so dass wir in allen weiteren
                Versuchen bei den Defaultwerten und der Annahme der Normalverteilung geblieben sind.</para>
        </sect1>
        <sect1 xml:id="ch_NB_results"> <!-- Darstellung der Ergebnisse -->
            <title>Darstellung der Ergebnisse</title><!-- Darstellung der Ergebnisse -->
            <para>Unsere Untersuchungen begannen mit einem kleinen Trainings- und Testkorpus, welches dann mit der Zeit immer mehr erweitert wurde. Da mit
                dem Fortschreiten der Untersuchungen nicht nur die Zahl der Attribute schwankte, sondern auch auch die Anzahl der Klassen mit der 
                Erweiterung des Korpus wuchs, sind die Ergebnisse der Zwischenschritte nicht oder nur sehr bedingt untereinander vergleichbar. Die 
                Erfolgsquote von Naive Bayes lag jedoch im Vergleich mit anderen Klassifikationsverfahren stets im vorderen Bereich und schwankte &#x2012; mit
                einer auffälligen Ausnahme &#x2012; zwischen 55% und 64%.</para>
            <table frame="all" orient="port" pgwide="0" tocentry="1" xml:id="tab_NB_bei_wachsendem_Korpus"> <!-- Median und Standardabweichung einiger Attributwerte und Klassen -->
                <info>
                    <title>Erfolgsquote von Naïve Bayes bei wachsender Korpusgröße</title>
                </info>
                <tgroup cols="7" align="left" colsep="1" rowsep="1">
                    <colspec colname="nummer" colwidth="1*"/>
                    <colspec colname="train" colwidth="2*"/>
                    <colspec colname="test" colwidth="2*"/>
                    <colspec colname="attribute" colwidth="2*"/>
                    <colspec colname="klassen" colwidth="2*"/>
                    <colspec colname="erfolgsquote" colwidth="2*"/>
                    <colspec colname="bemerkungen" colwidth="4*"/>
                    <thead>
                        <row>
                            <entry>Nummer</entry>
                            <entry>Trainingsinstanzen</entry>
                            <entry>Testinstanzen</entry>
                            <entry>Attribute</entry>
                            <entry>Klassen</entry>
                            <entry>Erfolgsquote</entry>
                            <entry>Bemerkungen</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>1</entry>
                            <entry>920</entry>
                            <entry>keine</entry>
                            <entry>13650</entry>
                            <entry>79</entry>
                            <entry>62,28%</entry>
                            <entry>Test durch 10-fache Kreuzvalidierung</entry>
                        </row>
                        <row>
                            <entry>2</entry>
                            <entry>1183</entry>
                            <entry>374</entry>
                            <entry>15768</entry>
                            <entry>114</entry>
                            <entry>63,37%</entry>
                            <entry>Erstmals Validierung mit dezidierter Testmenge</entry>
                        </row>
                        <row>
                            <entry>3</entry>
                            <entry>1491</entry>
                            <entry>374</entry>
                            <entry>15890</entry>
                            <entry>99</entry>
                            <entry>63,90%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>4</entry>
                            <entry>1892</entry>
                            <entry>630</entry>
                            <entry>19675</entry>
                            <entry>114</entry>
                            <entry>58,89%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>5</entry>
                            <entry>1987</entry>
                            <entry>675</entry>
                            <entry>19819</entry>
                            <entry>105</entry>
                            <entry>61,25%</entry>
                            <entry>Erstmals mit manuell korrigierten Trainingsdaten</entry>
                        </row>
                        <row>
                            <entry>6</entry>
                            <entry>2058</entry>
                            <entry>675</entry>
                            <entry>21271</entry>
                            <entry>142</entry>
                            <entry>57,04%</entry>
                            <entry>WG 111 und 112 hier erstmals nicht auf WG 110 abgebildet</entry>
                        </row>
                        <row>
                            <!-- zweistellige Klassen -->
                            <entry>7</entry>
                            <entry>2058</entry>
                            <entry>675</entry>
                            <entry>14657</entry>
                            <entry>46</entry>
                            <entry>64,74%</entry>
                            <entry>wie Nr. 6, aber dreistellige WG-Codes auf zwei Stellen gekürzt</entry>
                        </row>
                        <row>
                            <!-- mit controlled vocabulary -->
                            <entry>8</entry>
                            <entry>2259</entry>
                            <entry>822</entry>
                            <entry>13738</entry>
                            <entry>143</entry>
                            <entry>45,86%</entry>
                            <entry>mit Verwendung eines kontrollierten Vokabulars</entry>
                        </row>
                        <row>
                            <entry>9</entry>
                            <entry>3353</entry>
                            <entry>1084</entry>
                            <entry>23454</entry>
                            <entry>153</entry>
                            <entry>55,07%</entry>
                            <entry></entry>
                        </row>
                        <?dbfo-need height="4cm" ?>
                        <row>
                            <!-- wie 9, aber auf zweistellige Klassen gekürzt -->
                            <entry>10</entry>
                            <entry>3353</entry>
                            <entry>1084</entry>
                            <entry>23454</entry>
                            <entry>47</entry>
                            <entry>59,69%</entry>
                            <entry>Daten identisch mit Nr. 9, aber WG-Codes auf zweistellige Codes gekürzt</entry>
                        </row>
                        <row>
                            <!-- wie 9, aber auf niedrigfrequente Klassen entfernt -->
                            <entry>11</entry>
                            <entry>2927</entry>
                            <entry>934</entry>
                            <entry>23454</entry>
                            <entry>50</entry>
                            <entry>59,21%</entry>
                            <entry>Korpus wie Nr. 9, aber niedrigfrequente Klassen entfernt</entry>
                        </row>
                        <row>
                            <entry>12</entry>
                            <entry>4380</entry>
                            <entry>2016</entry>
                            <entry>26713</entry>
                            <entry>157</entry>
                            <entry>58,48%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>13</entry>
                            <entry>4612</entry>
                            <entry>2292</entry>
                            <entry>23669</entry>
                            <entry>149</entry>
                            <entry>59,16%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>28a</entry>
                            <entry>4615</entry>
                            <entry>2292</entry>
                            <entry>16270</entry>
                            <entry>149</entry>
                            <entry>59,42%</entry>
                            <entry>Referenzlauf für Optimierungsversuche</entry>
                        </row>
                        <row>
                            <entry>28b</entry>
                            <entry>4615</entry>
                            <entry>2292</entry>
                            <entry>16270</entry>
                            <entry>149</entry>
                            <entry>57,11%</entry>
                            <entry>Mit Kernel-Estimator-Option (Flexible Bayes)</entry>
                        </row>
                        <row>
                            <entry>28c</entry>
                            <entry>4615</entry>
                            <entry>2292</entry>
                            <entry>16270</entry>
                            <entry>149</entry>
                            <entry>49,65%</entry>
                            <entry>Mit Option Supervised Discretization</entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <para>Neben den im Vergleich zu anderen Verfahren guten Erfolgsquoten zeichnete sich Naïve Bayes durch kurze Laufzeiten, vor allem in der
                Trainingsphase aus. Die Trainingsdauer lag für die in <xref linkend="tab_NB_bei_wachsendem_Korpus"/> aufgelisteten Trainingsläufe auf handelsüblichen 
                Bürocomputern mit 64-Bit-Betriebssystemen, 8 GB RAM und Intel-Core-i5-4300U-Prozessor oder Intel-Core-i5-2520M-Prozessor stets bei unter 2 Minuten.
                Erst wenn die Attributmenge eine Mächtigkeit von 50.000 erreichte, stieg die Trainingsdauer auf 3 Minuten und ein Umstieg auf Computer mit größerem
                RAM-Speicher wurde erforderlich.</para>
            <para>Bis zu Trainingslauf Nummer 4 wurden ausschließlich die von Verlagen vergebenen Warengruppen verwendet, danach erfolgte die manuelle Durchsicht und
                Korrektur dieser Klassen, wie in <xref linkend="ch_Korpus"/> erläutert.</para>
            <para>Bis zu Trainingslauf Nummer 5 wurden die beiden Warengruppen 111 (Romane und Erzählungen von Autorinnen und Autoren, deren Hauptwerk vor
                1945 verfasst wurde) und 112 (Romane und Erzählungen von Autorinnen und Autoren, deren Hauptwerk nach 1945 verfasst wurde) auf den
                Elternknoten 110 abgebildet, weil diese etwas „esoterische“ Warengruppen-Unterscheidung bei kleiner Trainingsmenge wohl kaum zu erlernen ist.
                Mit steigender Größe der Trainingsmenge schien diese Maßnahme dann nicht mehr notwendig.</para>
            <para>In den Trainingsläufen Nummer 7 und 10 wurden die Warengruppencodes auf zwei Stellen gekürzt, also alle Warengruppen auf ihren Elternknoten 
                abgebildet. Die Erfolgsquote stieg dadurch signifikant (im Vergleich zu den Läufen 6 und 9 mit denselben Trainings- und Testdaten), allerdings
                um den Preis der geringeren Aussagekraft der Klassifikation. Ein ähnlicher Effekt bezüglich der Erfolgsquote ließ sich durch das Entfernen
                niedrigfrequenter Klassen aus der Trainings- und Testmenge erzielen (Nummer 11), was natürlich insgesamt zu einer kleineren Zahl an Klassen 
                führte.</para>
            <para>Die Läufe Nummer 14 bis 27 dienten der Attributevaluation, wie sie in <xref linkend="ch_AttribEval"/> beschrieben wurden, und sind aus diesem
                Grund in <xref linkend="tab_NB_bei_wachsendem_Korpus"/> nicht mehr aufgeführt. Mit Lauf 28a wurde ein stabiles Ergebnis ohne Einschränkungen bei
                den Klassen erreicht. Dieser Lauf dient uns nun als Referenz für die im folgenden Abschnitt geschilderten Optimierungsversuche.</para>
            <para>Die genannte Erfolgsquote im Referenzlauf von 59,4241% entspricht der nach Klassendistribution gewichteten TP-Rate (vgl. <xref
                linkend="ch_Evaluation_Kennzahlen_Relevanz-Sensitivität"/>) von 0,594. Die ebenso gewichtete FP-Rate betrug in diesem Lauf 0,017. Die 
                Genauigkeit (Precision) lag durchschnittlich bei 0,597, die Trefferquote (Recall) wieder bei 0,594. Betrachtet man die Micro-Averaged-Genauigkeit
                und Trefferquote
                getrennt für die größte Klasse (Warengruppe 112: 0,7993 Genauigkeit, 0,5047 Trefferquote) und alle anderen Klassen (0,5047 Genauigkeit, 0,5523
                Trefferquote), so zeigt sich, dass die große Klasse deutlich genauer gelernt werden konnte.</para>
            <para>Interessant ist nun noch ein Blick darauf,
                welche Warengruppen besonders gut und welche besonders schlecht erkannt wurden. Das höchstmögliche F-Maß von 1,0, welches nur erreicht wird,
                wenn sowohl Genauigkeit als auch Trefferquote ebenfalls 1,0 betragen, wurde vor allem für einige niedrigfrequente Klassen erreicht: Warengruppe
                162 (zweisprachige Ausgaben deutsch/französisch mit nur 2 Trainingsinstanzen und einer Testinstanz), 292 (Kinder- und Jugendbuch Fremdsprachen mit
                12 Trainings- und 6 Testinstanzen), 412 (Ratgeber Handarbeit / Textiles mit 10 Trainings- und 5 Testinstanzen), 415 (Ratgeber Fotografieren und
                Filmen mit 6 Trainings- und 3 Testinstanzen), 914 (Sprachführer mit 6 Trainings- und 3 Testinstanzen). Die hohe F-Rate ist in diesem Fall
                nicht unbedingt als Erfolg zu werten. Vielmehr ist bei dieser geringen Trainingsdatenbasis die Gefahr der <indexterm><primary>Überanpassung</primary>
                </indexterm>Überanpassung sehr groß. So stammen die E-Books der Warengruppe 292 alle von demselben Verlag und verteilen sich auf drei Buchreihen: eine 
                Reihe mit Kinderkrimis, eine mit Liebesgeschichten für jugendliche Mädchen und eine für Jungen im Jugendalter. Alle drei Reihen dienen dazu,
                mit Hilfe unterhaltsamer Geschichten Englisch zu lernen. Würde sich in der Testmenge ein ebenfalls in diese Warengruppe passendes Buch für 
                Französisch oder Spanisch befinden, würde dieses vom trainierten Modell mit hoher Wahrscheinlichkeit falsch klassifiziert.</para>
            <para>Am anderen Ende der F-Maß-Skala stehen die Klassen mit einem F-Maß von 0, was bedeutet, dass keine Instanzen der Testmenge für diese Warengruppe
                korrekt klassifiziert wurden. Auch hier gilt, dass es sich durchweg um Klassen mit wenigen bis sehr wenigen E-Books im Korpus handelt: 114 (Märchen, 
                Sagen, Legenden mit insgesamt 6 Titeln), 119 (Aphorismen mit 6 Titeln), 361 (Reiseerzählungen Deutschland, 18 Titel), 363 (Reiseerzählungen 
                Afrika 9 Titel), 364 (Reiseerzählungen Naher Osten, 6 Titel), 366 (Reiseerzählungen Nordamerika, 12 Titel), 367 (Reiseerzählungen Südamerika,
                6 Titel), 368 (Reiseerzählungen Australien/Ozeanien, 6 Titel), 411 (kreatives Gestalten, 6 Titel), 422 (Naturführer mit insgesamt 3 Titeln), 
                426 (Angeln und Jagd, 3 Titel), 449 (sonstige Sportarten, 12 Titel), 454 (Länderküchen, 15 Titel), 460 (Gesundheit, 6 Titel), 
                472 (Esoterik, 6 Titel), 476 (östliche Weisheit, 3 Titel), 479 (Spiritualität: Sonstiges, 6 Titel), 485 (praktische Anleitungen, 9 Titel),
                496 (Ratgeber Geld, Bank, Börse, 21 Titel), 521 (Fachbuch Philosophie allgemein, 9 Titel), 522 (antike Philosophie, 3 Titel), 525 (Philosophie des 19. 
                Jahrhunderts, 6 Titel), 533 (theoretische Psychologie, 6 Titel), 535 (Psychoanalyse, 9 Titel), 543 (Fachbuch parktische Theologie, 3 Titel), 
                544 (Fachbuch Judentum, 3 Titel), 551 (Fachbuch Geschichte allgemein, 3 Titel), 553 (Fachbuch Geschichte: Antike, 3 Titel), 555 (Fachbuch Geschichte: 
                Neuzeit bis 1914, 3 Titel), 557 (Zeitgeschichte 1945 bis 1989, 3 Titel), 559 (Kulturgeschichte, 9 Titel), 561 (allgemeine und vergleichende 
                Sprachwissenschaft, 3 Titel), 563 (deutsche Sprach- und Literaturwissenschaft, 9 Titel), 574 (Schulpädagogik, 6 Titel), 585 (Innenarchitektur, Design,
                3 Titel), 610 (Fachbuch Naturwissenschaften allgemein, 3 Titel), 646 (theoretische Physik, 3 Titel), 691 (Fachbuch Medizin: Allgemeines, 9 Titel), 710
                (Sozialwissenschaften allgemein, 3 Titel), 722 (Soziologische Theorien, 9 Titel), 733 (politische Theorien und Ideengeschichte, 6 Titel), 737 (vergleichende
                und internationale Politikwissenschaft, 3 Titel), 781 (Fachbuch Wirtschaft allgemein, 3 Titel), 782 (Fachbuch Volkswirtschaft, 3 Titel), 783 (Fachbuch
                Betriebswirtschaft, 9 Titel), 786 (einzelne Wirtschaftszweige und Branchen, 6 Titel), 862 (Selbstlernmaterialien Sprache, 9 Titel), 915 (Jahrbücher, 6 Titel),
                916 (Listenbücher, 6 Titel), 921 (Religion und Philosophie: Biografien, 12 Titel), 925 (Sachbuch Religion allgemein, 6 Titel), 937 (Sachbuch
                Spiritualität, 12 Titel), 942 (Sachbuch Geschichte allgemein, 6 Titel), 944 (Sachbuch Vor- und Frühgeschichte, 9 Titel), 945 (Sachbuch Geschichte: 
                Mittelalter, 6 Titel), 946 (Sachbuch Geschichte Neuzeit bis 1918, 6 Titel), 953 (Sachbuch Bildende Kunst, 6 Titel), 956 (Sachbuch Literatur allgemein, 
                9 Titel), 963 (Sachbuch klassische Musik, 6 Titel), 967 (Sachbuch TV allgemein, 3 Titel), 974 (Sachbuch Wirtschaft allgemein, 12 Titel), 976 (Sachbuch
                Volkswirtschaft, 12 Titel), 983 (Sachbuch Astronomie, 6 Titel) und 985 (Sachbuch Natur und Gesellschaft, 9 Titel).</para>
            <para>64 von 149 Klassen konnten augenscheinlich in unserem Korpus gar nicht gelernt werden. Das trainierte Naïve-Bayes-Modell enthält natürlich auch 
                Werte, welche zu einer Zuweisung von entsprechenden E-Book-Datensätzen zu einer der 64 oben aufgeführten Warengruppen führen würden, aber auch diese
                sind vermutlich überangepasst an die Trainingsmenge, wohingegen die Testmenge von der Trainingsmenge so weit abweicht, dass sie vom Modell nicht 
                korrekt erfasst werden. Es lässt sich jedenfalls feststellen, dass Trainingsmengen von zwei bis zwölf Titeln pro Klasse offensichtlich zu klein sind, um
                funktionierende Warengruppenmodelle zu erlernen, eine Erkenntnis, die natürlich wenig erstaunlich ist, sondern sich mit der Intuition eines jeden
                „Maschinentrainers“ decken dürfte. Eine Prognose darüber, ob dieses Problem bei ausreichender Vergrößerung der Trainingsmenge verschwinden würde, oder
                ob darüber hinaus auch inhärente Abgrenzungsschwierigkeiten zwischen den einzelnen Warengruppen das Erlernen eines trennscharfen Modells verhindern,
                lässt sich auf der vorliegenden Datenmenge leider nicht treffen.</para>
            <para>Interessant ist aber auch noch ein Blick auf die konkreten Fehlklassifikationen in einer Warengruppe. Analysieren wir eine dieser schlecht gelernten
                Klassen im Detail: Für Warengruppe 361 (Reiseberichte und -erzählungen über Deutschland) standen 12 Trainings- und 6 Testtitel zur Verfügung. Diese
                Testtitel wurden in der Evaluationsphase fälschlicherweise einsortiert in die Warengruppen 111 (Romane und Erzählungen mit Hauptwerk des Autors vor
                1945), 185 (Humor), 312 (Reiseführer Deutschland), 362 (Reiseberichte und -erzählungen über Europa). Aber gibt es in Anbetracht der auch bei
                Klassifikation durch Menschen auftretenden <indexterm><primary>Indexierungsinkonsistenz</primary></indexterm>Indexierungsinkonsistenz wirklich nur die
                beiden Zustände „richtig klassifiziert“ und „falsch klassifiziert“? Die drei als 111 und 185 klassifizierten Bücher entsprechen zwar nicht der
                vorgegebenen Klassifikation und werden deshalb in der Evaluation als Fehler gewertet, sind aber durchaus auch vertretbar:</para>
            <itemizedlist spacing="compact">
                <listitem>
                    <para>Das als Warengruppe 111 klassifizierte Werk gilt als ein „Reiseroman“ und erschien erstmals 1904.</para>
                </listitem>
                <listitem>
                    <para>Die beiden als Warengruppe 185 klassifizierten Werke enthalten humorvolle Betrachtungen einer Autorenlesereise durch Deutschland
                        sowie mehrerer Almaufenthalte als landwirtschaftliche Aushilfskraft.</para>
                </listitem>
            </itemizedlist>
            <para>Die Abweichungen in Richtung Warengruppe 312 sind hingegen weniger stimmig: Im Falle eines Buches über Leipzig mit Schilderungen von 
                Rundgängen gerade noch vertretbar, im Falle eines Buches über die Mark Brandenburg, welches laut Werbetext einen „poetischen Genuss“ 
                darstellt, das Erzählerische somit klar in den Vordergrund stellt, aber nicht mehr. Und schließlich liegt der Harz zwar auch in Europa
                (Warengruppe 362), doch hätte das Buch über den Harz nach den Konventionen der Warengruppenklassifikation nach Deutschland (361) gehört.</para>
            <para>Aus dieser beispielhaften Detailanalyse lässt sich ermitteln, dass eigentlich unterschiedliche Grade von Korrektheit anzusetzen wären: Eine 
                Bewertungsskala der Warengruppenklassifikation könnte etwa so aussehen:</para>
            <itemizedlist spacing="compact">
                <listitem>
                    <para>richtig</para>
                </listitem>
                <listitem>
                    <para>auch vertretbar</para>
                </listitem>
                <listitem>
                    <para>eher nicht vertretbar, aber auch nicht völlig falsch</para>
                </listitem>
                <listitem>
                    <para>falsch</para>
                </listitem>
            </itemizedlist>
            <para>Eine Auswertung nach diesem Schema wäre aber nicht mehr durch einzelne Forscher zu leisten, sondern müsste auf intersubjektiven 
                Einschätzungen auf Basis eines zuvor festgelegten Kriterienkatalogs aufbauen. Das Thema wird in <xref linkend="ch_sum_neue_Evaluation"/>
                noch einmal aufgegriffen werden.</para>
            <para>Betrachten wir abschließend noch die Qualität unseres Naïve-Bayes-Modells im Bereich der frequentesten Warengruppen. In der belletristischen
                Gegenwartsliteratur (WG 112) wurden 227 E-Books aus der Testmenge von 414 Titeln korrekt klassifiziert. Das entspricht einer TP-Rate von 0,552.
                Die Genauigkeit (Precision) liegt bei 0,799, das F-Maß bei 0,653. Falsch klassifizierte WG-112-Bücher fanden sich am häufigsten in der 
                Jugendliteratur (WG 260, 71 Exemplare), bei den Krimis (WG 121, 27 Exemplare), in der Fantasyliteratur (WG 132, 15 Exemplare), beim Humor (WG 185,
                13 Exemplare) und bei den Kinderbüchern (WG 250, 12 Exemplare). Zwischen allen genannten Warengruppen sind fließende Grenzen anzunehmen, so dass dieses 
                Ergebnis nicht überrascht. Auch ein Blick auf die <emphasis>false positives</emphasis> unterstreicht diese Affinität: 12 Krimis (WG 121) und 10
                Jugendbücher (WG 260) wurden der Belletristik-WG 112 zugewiesen.</para>
            <para>Einen Überblick über die Klassifikationsqualität der zehn häufigsten Warengruppen gibt <xref linkend="tab_NB_frequente_WG"/>. Das höchste
                F-Maß erreichen hier die <indexterm><primary>Comic</primary><secondary>Besonderheiten</secondary></indexterm>Manga-Comics (WG 182). Gemeinsam mit 
                den Comics (WG 181) und den Bilderbüchern (WG 211) stellen die E-Books dieser Warengruppe insofern eine Besonderheit dar, als sie technisch 
                betrachtet fast ausschließlich aus Grafikdateien bestehen, der Wortvektor somit fast leer ist. Die gute Erfolgsquote wird hier wohl aufgrund dieser
                Besonderheit erreicht. Würde man den in den Sprechblasen der Comics enthaltenen Text mit Optical Character Recognition (<glossterm
                    linkend="gt_ocr">OCR</glossterm>) verfügbar machen, 
                würde vermutlich das F-Maß sinken, aber auch die Überanpassung an technische Gegebenheiten, welche mit dem Buchtext nichts zu tun haben.</para>
            <para>Ein hohes F-Maß weisen darüber hinaus noch die Warengruppen 693 (klinische Medizin) und 121 (Krimis und Thriller) auf. Bei der relativ großen 
                Datenbasis von 37 bzw. 189 Testinstanzen deutet dies darauf hin, dass unser erlerntes Naïve-Bayes-Modell diese Warengruppen gut von anderen 
                abgrenzen kann. Die Gefahr einer Überanpassung scheint hier weniger groß zu sein als bei den niedrig frequenten Klassen oder bei Warengruppen
                mit sehr speziellen technischen Dateieigenschaften.</para>
            <table frame="all" orient="port" pgwide="0" tocentry="1" xml:id="tab_NB_frequente_WG"> <!-- Klassifikationsqualität in den frequentesten Warengruppen -->
                <info>
                    <title>Naïve-Bayes-Klassifikationsqualität in den frequentesten Warengruppen</title>
                </info>
                <tgroup cols="6" align="left" colsep="1" rowsep="1">
                    <colspec colname="warengruppe" colwidth="1*"/>
                    <colspec colname="anzahl" colwidth="1*"/>
                    <colspec colname="tp" colwidth="1*"/>
                    <colspec colname="precision" colwidth="1*"/>
                    <colspec colname="fmeasure" colwidth="1*"/>
                    <colspec colname="bemerkungen" colwidth="4*"/>
                    <thead>
                        <row>
                            <entry>Warengruppe</entry>
                            <entry>Testinstanzen</entry>
                            <entry>TP-Rate</entry>
                            <entry>Genauigkeit (Precision)</entry>
                            <entry>F-Maß</entry>
                            <entry>False negatives</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>112</entry><!-- Warengruppe -->
                            <entry>411</entry><!-- Testinstanzen -->
                            <entry>0,552</entry><!-- TP-Rate -->
                            <entry>0,799</entry><!-- Genauigkeit -->
                            <entry>0,653</entry><!-- F-Maß -->
                            <entry>71-mal WG 260, 27-mal WG 121, 15-mal WG 132, 13-mal WG 185, 12-mal WG 250</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>121</entry>
                            <entry>189</entry>
                            <entry>0,794</entry>
                            <entry>0,781</entry>
                            <entry>0,787</entry>
                            <entry>19-mal WG 260, 12-mal WG 112</entry>
                        </row>
                        <row>
                            <entry>260</entry><!-- Warengruppe -->
                            <entry>185</entry><!-- Testinstanzen -->
                            <entry>0,595</entry><!-- TP-Rate -->
                            <entry>0,491</entry><!-- Genauigkeit -->
                            <entry>0,538</entry><!-- F-Maß -->
                            <entry>25-mal WG 132, 25-mal WG 250, 10-mal WG 112</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>250</entry><!-- Warengruppe -->
                            <entry>94</entry><!-- Testinstanzen -->
                            <entry>0,734</entry><!-- TP-Rate -->
                            <entry>0,575</entry><!-- Genauigkeit -->
                            <entry>0,645</entry><!-- F-Maß -->
                            <entry>8-mal WG 260, 6-mal WG 132, 4-mal WG 112</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>132</entry><!-- Warengruppe -->
                            <entry>90</entry><!-- Testinstanzen -->
                            <entry>0,756</entry><!-- TP-Rate -->
                            <entry>0,548</entry><!-- Genauigkeit -->
                            <entry>0,636</entry><!-- F-Maß -->
                            <entry>12-mal WG 260, 6-mal WG 121</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>481</entry><!-- Warengruppe -->
                            <entry>86</entry><!-- Testinstanzen -->
                            <entry>0,767</entry><!-- TP-Rate -->
                            <entry>0,606</entry><!-- Genauigkeit -->
                            <entry>0,677</entry><!-- F-Maß -->
                            <entry>2-mal WG 185</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>973</entry><!-- Warengruppe -->
                            <entry>72</entry><!-- Testinstanzen -->
                            <entry>0,528</entry><!-- TP-Rate -->
                            <entry>0,392</entry><!-- Genauigkeit -->
                            <entry>0,450</entry><!-- F-Maß -->
                            <entry>10-mal WG 971, 7-mal WG 972, 6-mal WG 185</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>182</entry><!-- Warengruppe -->
                            <entry>59</entry><!-- Testinstanzen -->
                            <entry>1,0</entry><!-- TP-Rate -->
                            <entry>0,881</entry><!-- Genauigkeit -->
                            <entry>0,937</entry><!-- F-Maß -->
                            <entry></entry><!-- FN -->
                        </row>
                        <row>
                            <entry>185</entry><!-- Warengruppe -->
                            <entry>47</entry><!-- Testinstanzen -->
                            <entry>0,702</entry><!-- TP-Rate -->
                            <entry>0,423</entry><!-- Genauigkeit -->
                            <entry>0,528</entry><!-- F-Maß -->
                            <entry>2-mal WG 121</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>693</entry><!-- Warengruppe -->
                            <entry>37</entry><!-- Testinstanzen -->
                            <entry>0,811</entry><!-- TP-Rate -->
                            <entry>0,909</entry><!-- Genauigkeit -->
                            <entry>0,857</entry><!-- F-Maß -->
                            <entry></entry><!-- FN -->
                        </row>
                    </tbody>
                </tgroup>
            </table>
        </sect1>
        <sect1 xml:id="ch_NB_improve"> <!-- Verbesserung und Boosting -->
            <title>Verbesserungsversuche / Boosting</title>
            <para>In diesem Abschnitt werden Versuche dargestellt, die Erfolgsquote des oben vorgestellten Naïve-Bayes-Klassifikators zu verbessern. Im Falle des
                Ensemble Learnings setzt der Verbesserungsversuch auf algorithmischer Ebene an. Eine zweite Kategorie von Optimierungen versucht, in der Phase
                der Datenaufbereitung und Vorverarbeitung Verbesserungspotential zu ermitteln. Zum Abschluss geben wir in <xref 
                    linkend="ch_NB_improve_überblick"/> eine Zusammenfassung der Ergebnisse.</para>
            <sect2 xml:id="ch_NB_improve_ensemble"> <!-- Ensemble Learning -->
                <title>Ensemble Learning</title>
                <para>Eine im Maschinenlernen etablierte Methode zur Verbesserung von Klassifikatoren ist der Einsatz von Ensemble Learning (<biblioref
                    linkend="bib_RN16"/>, S. 748-52, <biblioref linkend="bib_WFHP17"/>, S. 479-99)<indexterm class="startofrange" xml:id="idt_014"><primary>Ensemble Learning</primary></indexterm>.
                    In der Weka-Bibliothek finden sich gängige Ensemble-Lernverfahren im Paket der „Metalerner.“ Bei allen bisher diskutierten Lernverfahren 
                    handelt es sich um Basislerner, mit deren Hilfe versucht wird, ein Modell zu generieren, welches eine bestimmte Anwendungsdomäne in möglichst guter
                    Weise abbildet. In einer etwas anthropomorphisierenden Art und Weise sagen <biblioref linkend="bib_BGSV09"/> (S. 2), dass ein Basislerner Erfahrung 
                    bezüglich einer bestimmten Lernaufgabe sammelt, wohingegen ein Metalerner Erfahrung bezüglich unterschiedlicher Anwendungen von Basislernern
                    aufbaut. Das uns hier interessierende Ensemble Learning ist eine Art des Metalernens, bei dem mehrere Modelle von unterschiedlichen Basislernern
                    gleichzeitig verwendet werden. Diese Modelle können entweder von unterschiedlichen Lernalgorithmen erzeugt werden, oder von demselben Algorithmus 
                    auf jeweils unterschiedlichen Teilmengen der Trainingsmenge (<biblioref linkend="bib_BGSV09"/>, S. 8-9).
                </para>
                <para>Zwei sehr populäre Ensemble-Lerner sind <indexterm><primary>Bagging</primary></indexterm>Bagging und <indexterm><primary>Boosting</primary></indexterm>
                    Boosting (vorgestellt u. a. in <biblioref linkend="bib_BGSV09"/>, <biblioref linkend="bib_RN16"/>,
                    <biblioref linkend="bib_WFHP17"/>). Bagging randomisiert die Trainingsmenge durch ein Resampling-Verfahren. Mit den verschiedenen Samples der 
                    Trainingsmenge werden dann unterschiedliche Modelle trainiert, aus welchen in der Testphase durch ein gemeinsames Votum die Klasse für jedes Testdokument
                    bestimmt wird. Bagging zeitigt vor allem dann positive Effekte, wenn der verwendete Basislerner unstabil ist, also den zu erlernenden Sachverhalt 
                    nicht in besonders sicherer Weise voraussagt und das Sampling für eine gewisse Varianz in den erlernten Modellen sorgt. Da das Resampling der 
                    Trainingsmenge auf einer Randomisierung beruht, lässt sich bei steigender Zahl von
                    Iterationen statistisch ein besserer Effekt erwarten als bei wenigen Iterationen. Damit steigen aber natürlich auch die Anforderungen an die 
                    Berechnungszeit. Bei Bagging mit 10 Iterationen stieg auf unserem Korpus die Dauer der Trainingsphase in der Weka-Implementierung zwar nur um den Faktor
                    8,36 bis 9,05, die Evaluationsphase hingegen um den Faktor 11,19 bis 16,0. Die Erfolgsquote schwankte dabei von -1,09 bis +1,5 
                    Prozentpunkte um die Erfolgsquote des Basislerners herum, wobei der Verbesserungseffekt von +1,5 nur erreicht wurde bei einem Testlauf, in dem die 
                    niedrig frequenten Klassen entfernt wurden (2927 Trainingsinstanzen, 934 Testinstanzen, 23454 Attribute, 50 Klassen). Da sich im „normalen“ Korpus
                    die Erfolgsquote stets verschlechterte, die Auswertungszeit sich hingegen stark verlängerte, wurden keine weiteren Versuche mit der Kombination
                    Bagging und Naïve Bayes unternommen. Der Basislerner scheint zu stabil zu sein, um durch Bagging gute Effekte zu erzielen. Es ist allerdings nicht
                    auszuschließen, dass bei Erhöhung der Anzahl der Iterationen positive Effekte durch Bagging erzielt werden können.</para>
                <para>Beim Bagging werden in den verschiedenen Iterationen jeweils neue Klassifikatoren generiert, welche unabhängig von den vorherigen
                    Läufen sind. Beim Boosting hingegen baut jede Iteration auf dem vorherigen Lauf auf, kennt also insbesondere auch das Evaluationsergebnis seines
                    Vorgängers. Der populäre Algorithmus <indexterm class="startofrange" xml:id="idt_012"><primary>AdaBoost</primary></indexterm>AdaBoost.M1 arbeitet
                    mit Gewichtung von Trainingsinstanzen. Zu Beginn haben alle Trainingsdatensätze das
                    gleiche Gewicht, bzw. in der Anwendung auf Naïve Bayes das Gewicht, welches der Klassendistribution entspricht. In den folgenden Läufen werden dann
                    jeweils die zuvor falsch klassifizierten Datensätze mit einem höheren Gewicht versehen, wodurch ein Anreiz geschaffen wird, im folgenden Durchlauf das 
                    Modell darauf hin zu optimieren, dass die zuvor falsch klassifizierten Datensätze nun besser gelernt werden. Zunächst ließen sich in den
                    Weka-Standardimplementierungen von Naïve Bayes und AdaBoost.M1 nur leichte Effekte beobachten: Der geboostete Klassifikator 
                    verfügte in der frühen Untersuchungsphase mit kleinem Korpus stets über dieselbe Erfolgsquote wie das Modell des Basislerners. AdaBoost brach in der 
                    Regel nach wenigen (4-7) Iterationen ab, da der Schwellwert für die höchste erlaubte Fehlerquote überschritten wurde. Die Beobachtung, dass 
                    Standard-AdaBoost in der Anwendung auf Standard-Naïve-Bayes kaum Effekte zeigt, deckt sich mit <biblioref linkend="bib_TZ03"/>, die einen neue Variante
                    von Naïve Bayes einführen, welche die Vorteile von AdaBoosting auch für Naïve-Bayes-Lernen verfügbar machen. Aus Zeitgründen wurde auf eigene Experimente
                    mit dieser Variante verzichtet. Mit wachsender Korpusgröße und damit einhergehend dem Einsatz von Computern mit größerem RAM-Speicher konnte die 
                    Erfolgsquote des Basisklassifikators doch deutlich um etwa 1,5 Prozentpunkte gesteigert werden (siehe <xref
                        linkend="tab_NB_mit_EnsembleLearning"/>.</para> 
               <para>Dass sich 
                    mit AdaBoost.M1 und Naïve Bayes bei kleinem Korpus kein positiver Effekt erzielen lässt, könnte daran liegen, dass der Basisklassifikator zu stabil ist 
                    und sich durch Variation der Gewichte keine alternativen Klassifikatoren generieren lassen, welche für die Klassen mit hoher Fehlerrate der vorhergehenden
                    Klassifikation bessere Modelle erzeugen. Eine andere Hypothese lautet, dass zu viele kleine Klassen mit sehr hoher Fehlerrate vorhanden sind, welche sich
                    auf Grundlage der geringen Anzahl an Trainingsdaten für diese Klassen gar nicht optimieren lassen. Diese Interpretation wird durch die frühen 
                    Abbrüche von AdaBoost.M1 aufgrund zu hoher Fehlerquote gestützt.</para>
                <para>Verwendet man AdaBoost.M1 nicht mit der Neugewichtung (Weighting) der Trainingsklassen, sondern mit einem 
                    probabilistischen Resampling der Trainingsmenge, wie es auch bei Bagging zum Einsatz kommt, konnten auch die frühen Abbrüche vermieden werden. Die
                    Steigerung der Klassifikationsqualität durch AdaBoost.M1 wird jedoch mit einer starken Verlängerung der Trainingsdauer erkauft (bei 20 Iterationen und
                    Resampling um das 1200-fache, eine ähnliche Laufzeit würde vermutlich auch bei Weighting erreicht, wenn kein früher Abbruch erfolgte) und ist somit mit 
                    steigender Größe der Trainingsmenge zunehmend weniger praktikabel. </para>
                <table frame="all" xml:id="tab_NB_mit_EnsembleLearning"><!-- Ergebnisse Naïve Bayes mit Bagging und Boosting -->
                    <title>Ergebnisse Naïve Bayes mit Ensemble Learning</title>
                    <tgroup cols="5">
                        <colspec colname="Nummer" colnum="1" colwidth="1*" align="left"/>
                        <colspec colnum="2" colwidth="2*" align="left" colname="Verfahren"/>
                        <colspec colnum="3" colwidth="1*" align="left" colname="Iterationen"/>
                        <colspec colname="Laufzeit" colnum="4" colwidth="2*" align="left"/>
                        <colspec colnum="5" colwidth="1*" align="left" colname="Erfolgsquote"/>
                        <thead>
                            <row>
                                <entry>Nummer</entry>
                                <entry>Verfahren</entry>
                                <entry>Anzahl Iterationen</entry>
                                <entry>Laufzeit</entry>
                                <entry>Erfolgsquote</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>28a</entry>
                                <entry>Naïve Bayes ohne Ensemble Learning<indexterm class="endofrange" startref="idt_014"/></entry>
                                <entry>1</entry>
                                <entry>53s (Training), 18m 16s (Evaluation)</entry>
                                <entry>59,42%</entry>
                            </row>
                            <row>
                                <entry>28f</entry>
                                <entry>Naïve Bayes mit Bagging</entry>
                                <entry>10</entry>
                                <entry>3m 6s (Training), 6h 2m 7s (Evaluation)</entry>
                                <entry>58,33%</entry>
                            </row>
                            <row>
                                <entry>28d</entry>
                                <entry>Naïve Bayes mit AdaBoost.M1 (mit Resampling)</entry>
                                <entry>20</entry>
                                <entry>17h 43m 32s (Training), 4h 13m 50s (Evaluation)</entry>
                                <entry>60,38%</entry>
                            </row>
                            <row>
                                <entry>28e</entry>
                                <entry>Naïve Bayes mit AdaBoost.M1 (mit Weighting)<indexterm class="endofrange" startref="idt_012"/></entry>
                                <entry>Abbruch nach 5.</entry>
                                <entry>5h 46m 15s (Training), 2h 32m 8s (Evaluation)</entry>
                                <entry>60,91%</entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
            </sect2>
            <sect2 xml:id="ch_NB_improve_attributes"> <!-- Optimierungspotential in der Datenaufbereitung -->
                <title>Optimierungspotential in der Datenaufbereitung</title>
                <sect3 xml:id="ch_NB_improve_cv"> <!-- Kontrolliertes Vokabular -->
                    <title>Hinzufügen eines kontrollierten Vokabulars zum Wortvektor</title>
                    <para>Unter den Voraussetzungen der Vorverarbeitungsschritte, welche in <xref linkend="ch_LexSemProp"/> geschildert wurden, um zu einem
                        einerseits kleinen, andererseits aussagekräftigen Wortvektor zu kommen, kann es passieren, dass Wörter, welche eine signifikante Korrelation
                        mit einer Klasse aufweisen, wegoptimiert werden. So wird etwa in Romanen das Wort „Roman“ häufig als Untertitel angegeben, es gelangt aber
                        wegen eines zu geringen TF/IDF-Wertes nicht in unseren Wortvektor. Um nun sicherzustellen, dass explizite Erwähnungen von Wörtern, welche
                        eine Warengruppe beschreiben, im Text der E-Books als Lernattribut nicht verlorengehen, wurde ein kontrolliertes Vokabular angefertigt, 
                        welches lemmatisierte Formen der Wörter aus dem offiziellen Warengruppendokument <biblioref linkend="bib_WGS06"/> enthält. Auf diese Art und Weise
                        wurden 510 Wörter ermittelt<footnote><para>Die vollständige kontrollierte Wortliste ist im Quellcode von Avve einsehbar unter <link
                            xlink:href="https://github.com/sermo-de-arboribus/Avve/blob/version-1.0/Avve/src/main/resources/controlledvocabulary/vlb_wg_cv.txt"/> (Zugriff 3.4.2018).</para>
                        </footnote>, welche durch Verwendung eines Parameter-Flags in der Avve-Vorverarbeitung der E-Books zugeschaltet werden können.
                        In die XRFF-Dateien wird dann für jede dieser kontrollierten Vokabeln die Anzahl der Vorkommen dieses Lemmas im untersuchten E-Book ausgegeben.</para>
                    <para>Die Erfolgsquote des Naïve-Bayes-Lerners lag mit dem kontrollierten Vokabular 1,13 Prozentpunkte unter derjenigen des Referenzklassifikators
                        (siehe <xref linkend="tab_NB_mit_CV"/>). Eine nahe liegende Vermutung ist, dass die Voraussetzung der Unabhängigkeit der Attribute voneinander nun
                        stärker verletzt wird, denn es kommt zu Attributdopplung wie z.&#160;B. <emphasis>cv_beruflich</emphasis> (Vorkommen des Wortes „beruflich“, gezählt auf
                        Basis des kontrollierten Vokabulars) und <emphasis>w_beruflich</emphasis> (Vorkommen des Wortes „beruflich“, gezählt aufgrund der Wortvektorberechnung). Doch
                        auch wenn man von den 69 offensichtlich verdoppelten Attributen jeweils eines entfernt<footnote><para>Hier zeigt sich exemplarisch der Vorteil, der
                            sich aus der Verwendung der XML-Dateivariante <indexterm><primary>XRFF</primary></indexterm> gegenüber dem <indexterm><primary>ARFF</primary>
                            </indexterm>ARFF-Format ergibt: Man kann in der XML-Welt die Lokalisierungssprache XPath und/oder
                        die Abfragesprache <indexterm><primary>XQuery</primary></indexterm> nutzen und sowohl übersichtliche als auch leicht verständliche deklarative Abfragen formulieren, die im ARFF-Format schwieriger
                        zu bewerstelligen wären. Sind alle Wortattribute aus dem kontrollierten Vokabular mit "cv_" präfigiert und alle normalen Wortvektorattribute mit "w_",
                        lassen sich die <indexterm><primary>Duplikate</primary></indexterm>Duplikate in XQuery (inklusive Attributindex, den man zum späteren Entfernen der Attribute benötigt) finden:</para>
                            <para><code>for $cvat in //attribute[starts-with(@name, 'cv_')]</code></para>
                            <para><code>let $cvatname := substring-after($cvat/@name, 'cv_')</code></para>
                            <para><code>let $cvatindex := 1 + count(//attribute[@name eq concat('w_', $cvatname)]/preceding-sibling::attribute)</code></para>
                            <para><code>where //attribute[@name eq concat('w_', $cvatname)]</code></para>
                            <para><code>return concat(local-name(//attribute[@name eq concat('w_', $cvatname)]), '[', $cvatindex, ']', ': ', $cvatname)</code></para>
                        </footnote>, gelangt man zu demselben Ergebnis. Dennoch bleibt die
                        Hypothese, dass sich bei Zugabe des kontrollierten Vokabulars die Unabhängigkeit der Attribute insgesamt verringert, plausibel.</para>
                    <para>Es wurde auch ein Versuch unternommen, bei dem die Wortvektorattribute komplett entfernt wurden und nur mit dem kontrollierten Vokabular
                        trainiert und evaluiert wure. Diese Variante performte deutlich schlechter, die Erfolgsquote lag fast bei der Hälfte. Unter Umständen würde
                        statt der unmittelbaren Verwendung von Wörtern aus der offiziellen Warengruppenbeschreibung ein von Experten verfasstes kontrolliertes Vokabular
                        besser funktionieren, doch wäre hier zunächst einmal zu klären, was eine VLB-Warengruppenexpertin überhaupt ausmacht.</para>
                    <table frame="all" xml:id="tab_NB_mit_CV"><!-- Ergebnisse Naïve Bayes mit kontrolliertem Vokabular -->
                        <title>Ergebnisse Naïve Bayes mit kontrolliertem Vokabular</title>
                        <tgroup cols="5">
                            <colspec colname="Nummer" colnum="1" colwidth="1*" align="left"/>
                            <colspec colnum="2" colwidth="2*" align="left" colname="Verfahren"/>
                            <colspec colnum="3" colwidth="1*" align="left"
                                colname="Attributanzahl"/>
                            <colspec colname="Laufzeit" colnum="4" colwidth="2*" align="left"/>
                            <colspec colnum="5" colwidth="1*" align="left"
                                colname="Erfolgsquote"/>
                            <thead>
                                <row>
                                    <entry>Nummer</entry>
                                    <entry>Verfahren</entry>
                                    <entry>Attributanzahl</entry>
                                    <entry>Laufzeit</entry>
                                    <entry>Erfolgsquote</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>28a</entry>
                                    <entry>Naïve Bayes ohne Optimierungen</entry>
                                    <entry>16270</entry>
                                    <entry>53s (Training), 18m 16s (Evaluation)</entry>
                                    <entry>59,42%</entry>
                                </row>
                                <row>
                                    <entry>29a</entry>
                                    <entry>Naïve Bayes mit kontrolliertem Vokabular</entry>
                                    <entry>16867</entry>
                                    <entry>54s (Training), 19m 4s (Evaluation)</entry>
                                    <entry>58,29%</entry>
                                </row>
                                <row>
                                    <entry>29b</entry>
                                    <entry>Naïve Bayes mit kontrolliertem Vokabular und entfernten Attribut-Duplikaten</entry>
                                    <entry>16798</entry>
                                    <entry>54s (Training), 18m 41s (Evaluation)</entry>
                                    <entry>58,29%</entry>
                                </row>
                                <row>
                                    <entry>30</entry>
                                    <entry>Naïve Bayes mit kontrolliertem Vokabular, aber ohne Wortvektor</entry>
                                    <entry>553</entry>
                                    <entry>2s (Training), 37s (Evaluation)</entry>
                                    <entry>31,77%</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </sect3>
                <sect3 xml:id="ch_NB_improve_fw"> <!-- Ausschließen von Wörtern aus Fremdsprachen -->
                    <title>Ausschließen von Wörtern aus Fremdsprachen</title>
                    <para>Aufgrund des Vorhandenseins von Sprachführern (Warengruppe 914), zweisprachigen Literaturausgaben (Warengruppen 161 und 162),
                        Sprachlehrbüchern (Warengruppe 862 für Erwachsene, Warengruppe 292 für Kinder) sowie der zufälligen Tatsache, dass in unserem
                        Korpus auch Sachbücher zur Bildenden Kunst (Warengruppe 953) sehr häufig zweisprachig englisch/deutsch verfasst sind, gelangt eine
                        sehr große Zahl von Wörtern aus fremden Sprachen in den Lucene-Index und bei Überschreiten der in <xref linkend="ch_LexSemProp_Wortvektor"/>
                        geschilderten Schwellen auch in den Attributindex.<indexterm><primary>Fremdsprachen</primary><secondary>Indexierung von Wörtern in</secondary>
                        </indexterm></para>
                    <para>Zur Untersuchung der Frage, ob diese Tatsache förderlich, hinderlich oder ergebnisneutral bezüglich der Klassifikationsqualität ist, 
                        bietet Avve über einen Kommandozeilenparameter die Möglichkeit, die <indexterm><primary>Indexierung</primary></indexterm>Indexierung von 
                        Wörtern in Fremdsprachen zu unterbinden. Dies setzt allerdings voraus, dass der <indexterm><primary>POS-Tagging</primary></indexterm>
                        Wortarten-Tagger solche Wörter korrekt erkennt. Die diesbezügliche Trefferquote des in unserer Studie verwendeten TreeTaggers lässt allerdings 
                        sehr zu wünschen übrig. Der TreeTagger verwendet das Stuttgart-Tübingen Tag Set (STTS, <biblioref linkend="bib_STST99"/>), welches für 
                        fremdsprachliches Material das Tag <emphasis>FM</emphasis> vorsieht. Eine Auflistung beobachteter Fehlklassifikationen gibt <xref
                            linkend="tab_falsch_klassifizierte_FM_Tags"/>. Eine Auszählung korrekt klassifizierter FM-Wörter fand aus Zeitgründen nicht statt, so
                        dass wir keine exakten Angaben zur Genauigkeit (Precision) und Trefferquote (Recall) des TreeTaggers in Bezug auf FM geben können. Es lässt 
                        sich aber festhalten, dass fremdsprachiges Wortmaterial sehr häufig als Name und häufig als Adjektiv, Adverb oder Präposition klassifiziert wurde.</para>
                    <table xml:id="tab_falsch_klassifizierte_FM_Tags"> <!-- Auszählung von 188 beaobachteten FM-Fehlklassifikationen des TreeTaggers -->
                        <title>Auszählung von 188 beaobachteten FM-Fehlklassifikationen des TreeTaggers</title>
                        <tgroup cols="3">
                            <colspec colname="Tag" colnum="1" colwidth="3*"/>
                            <colspec colname="Anzahl" colnum="2" colwidth="1*"/>
                            <colspec colname="Prozent" colnum="3" colwidth="1*"/>
                            <thead>
                                <row>
                                    <entry>Falsches POS-Tag</entry>
                                    <entry>Anzahl</entry>
                                    <entry>Prozentanteil</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row><entry>NE (Namen)</entry><entry>64</entry><entry>34,04%</entry></row>
                                <row><entry>ADJA (attributives Adjektive)</entry><entry>27</entry><entry>14,36%</entry></row>
                                <row><entry>ADV (Adverbien)</entry><entry>17</entry><entry>9,04%</entry></row>
                                <row><entry>APPR (Präpositionen)</entry><entry>14</entry><entry>7,45%</entry></row>
                                <row><entry>VVFIN (finite Verben)</entry><entry>14</entry><entry>7,45%</entry></row>
                                <row><entry>NN (appellative Nomen)</entry><entry>12</entry><entry>6,38%</entry></row>
                                <row><entry>$. (satzbeendende Interpunktion)</entry><entry>10</entry><entry>5,32%</entry></row>
                                <row><entry>ADJD (prädikatives Advjektiv)</entry><entry>8</entry><entry>4,26%</entry></row>
                                <row><entry>KON (Konjunktion)</entry><entry>5</entry><entry>2,66%</entry></row>
                                <row><entry>ART (Artikelwort)</entry><entry>3</entry><entry>1,60%</entry></row>
                                <row><entry>XY (Nichtwort)</entry><entry>3</entry><entry>1,60%</entry></row>
                                <row><entry>$( (satzinternes Satzzeichen)</entry><entry>3</entry><entry>1,60%</entry></row>
                                <row><entry>VVINF (Infinitiv)</entry><entry>2</entry><entry>1,06%</entry></row>
                                <row><entry>VVPP (Partizip Perfekt eines Vollverbs)</entry><entry>2</entry><entry>1,06%</entry></row>
                                <row><entry>$, (Komma)</entry><entry>2</entry><entry>1,06%</entry></row>
                                <row><entry>APPRART (Präposition mit Artikel)</entry><entry>1</entry><entry>0,53%</entry></row>
                                <row><entry>KOKOM (Vergleichspartikel)</entry><entry>1</entry><entry>0,53%</entry></row>
                            </tbody>
                        </tgroup>
                    </table>
                    <para>Für unseren Versuch, Fremdwörter von der Indexierung auszunehmen bedeutet die hohe Zahl an falschen POS-Tags, dass unser Filter nur sehr
                        grob und unzureichend funktionieren wird, denn viele Fremdwörter, die vom TreeTagger nicht als solche erkannt wurden, gehen durch den Filter
                        hindurch. Die Größe des Lucene-Indexes im Referenzlauf ohne jegliche Optimierungsversuche lag bei 2.971.631 Termen. Bei Anwendung der hier
                        beschriebenen Filterung war der Index um 15.056 Terme bzw. um 0,51% kleiner.</para>
                    <para>Daraus ergab sich dann auch nach Aufbereitung des Attributvektors eine verkleinerte Attributmenge von 15.902 Attributen gegenüber 16.270
                        Attributen, also einer Verringerung um 2,26%. Die Trainings- und Evaluationszeit für das fremdwortbereinigte Korpus sank geringfügig von ca. 
                        20 auf ca. 19 Minuten und die Erfolgsquote bei der Klassifikation stieg um 0,22 Prozentpunkte von 59,42% auf 59,64%. Da Training und Testen 
                        jeweils auf exakt denselben Korpusdaten beruhen und keinerlei randomisierende Prozesse angewandt wurden, kann ohne Überprüfung der statistischen
                        Signifikanz von einer tatsächlichen Verbesserung des Klassifikators für unser Korpus ausgegangen werden. Allerdings ist der Grund für diese
                        Verbesserung nicht zwingend in der Reduktion der Fremdwörter im Index zu suchen. Der Verbesserungseffekt könnte sich auch aufgrund von
                        Überanpassung, etwa an die Fehler im TreeTagger-Modell für das <emphasis>FM</emphasis>-Tag, oder von Reduktion der Attributabhängigkeiten ergeben 
                        haben.</para>
                </sect3>
                <sect3 xml:id="ch_NB_improve_nolig"> <!-- Auflösen von Ligaturen -->
                    <title>Auflösen von Ligaturen</title>
                    <para>Viele E-Books werden von Buchverlagen bzw. den von diesen beauftragten Setzereien aus dem Buchsatzprozess heraus erzeugt, sind 
                        quasi ein Nebenprodukt des Drucklayouts. Eine typografische Besonderheit ist dabei, dass für die Buchstabenkombinationen fl, fi, ff, ffi
                        und ffl häufig <glossterm linkend="gt_ligatur">Ligaturen</glossterm> eingesetzt werden. Für Schriftbild und Lesbarkeit mag das 
                        positive Effekte haben, für das Durchsuchen oder Indizieren von E-Book-Inhalten sind sie hingegen hinderlich, wenn diese Ligaturen nicht „rein
                        grafisch“ durch Zusammenrückung generiert werden, sondern wenn dafür eigene Unicode-Zeichen verwendet werden<footnote><para>Das Unicode-Konsortium
                            hat die Ligaturen für die genannten Buchstabenkombinationen aus Gründen der Kompatibilität mit zuvor bestehenden Zeichensätzen definiert, lehnt
                            es aber ab, zukünftig weitere Ligaturzeichen in den Standard aufzunehmen (Quelle: <link xlink:href="http://www.unicode.org/faq/ligature_digraph.html"/>,
                            Zugriff 10.03.2018).</para></footnote>. Das Vorhandensein solcher Unicode-Ligaturzeichen verhindert dann z.&#160;B., dass ﬂießen und f‌ließen auf dasselbe
                        Wort im Lucene-Index abgebildet werden. Ein Avve-Kommandozeilenparameter kann gesetzt werden, um die genannten Ligaturzeichen in ihre Grundbuchstaben
                        aufzulösen. Orthografisch standardisierte Ligaturen wie z.&#160;B. das scharfe ß oder das französische œ werden hingegen nicht ersetzt.</para>
                    <para>Das Auflösen von Ligaturen führte interessanterweise nicht zur erwarteten Verringerung, sondern zu einer Vergrößerung des Lucene-Indexes um ca. 0,056%
                        auf 2.973.309 Terme. Auch die Größe der Attributmenge stieg um 89 Attribute (bzw. 0,55%) an. Die Erfolgsquote stieg ebenfalls, und zwar im selben
                        Umfang wie beim oben beschriebenen Ausschließen von Fremdwörtern: von 59,42% auf 59,64%. Auch wenn die Beobachtungen bei Indexgröße und 
                        Attributvektorlänge kontraintuitiv sind, so entspricht das Ergebnis wieder der Intuition, die zu diesem Experiment geführt hat: Eine Vereinheitlichung
                        von Schreibweisen gruppiert zusammengehörige Lemmata, stärkt ggf. deren TF-IDF-Score und sorgt somit für einen aussagekräftigeren Wortvektor.</para>
                </sect3>
                <sect3 xml:id="ch_NB_improve_urlnorm"> <!-- Auflösung von URLs -->
                    <title>Entfernen von URLs</title>
                    <para>URLs enthalten häufig ungewöhnliche Buchstabenkombinationen, welche nicht als Worte aufgefasst werden können. Werden in einem Buch etwa
                        Links auf Youtube-Videos angegeben, gelangen solche sinnlosen Buchstabenkombinationen aber als Wörter in den Lucene-Index. Bei der Klassifikation von 
                        Webseiten ist es selbstverständlich sehr interessant, wohin diese verlinkt. Bei Buchtexten, auch bei E-Books, welche in der großen Mehrzahl auf
                        gedruckten Buchausgaben basieren, ist das Ziel angegebener Links vermutlich weniger relevant. Um diese Vermutung zu überprüfen, wurde Avve ein
                        Kommandozeilenparameter zur Normalisierung von URLs hinzugefügt. Ist dieser Parameter gesetzt, so wird mit einem einfachen regulären Ausdruck nach
                        URLs gesucht, die mit „http“, „https“ und „ftp“ beginnen. Der komplette auf das Protokoll folgende URL-String wird entfernt, der Protokollname hingegen 
                        beibehalten, so dass die Information, dass an dieser Stelle im Text ein Link angegeben ist, erhalten bleibt.</para>
                    <para>Die Größe des Lucene-Indexes verringerte sich nach Anwendung der beschriebenen Maßnahme um ca. 0,11% auf 2.968.462 Terme. Nach Berechnung von
                        TF-IDF-Werten und Anwendung der in <xref linkend="ch_LexSemProp_Wortvektor"/> beschriebenen Filtermaßnahmen stieg die Größe des Attributvektors auf
                        Basis des URL-bereinigten Korpus auf 16.350 Attribute (von 16.270, also ein Anstieg von 0,49%). Die Erfolgsquote verbesserte sich deutlicher als 
                        bei den vorgenannten Maßnahmen von 59,42% auf 59,90%.</para>
                    <para>Einige Wörter, die nun aus dem Attributvektor herausgefallen sind, sind etwa Zeitungskurznamen oder Presseagenturen („nytimes“, „faz“, „wiwo“,
                        „reuters“), Namen von Universitäten („vassar“) oder Firmen („capstone“) und generische Begriffe wie „article“. Wörter, die es aufgrund der nun 
                        neuen Gewichtung über die <code>StringToWordVector</code>-Schwelle (vgl. <xref linkend="ch_AttribEval_StringToWordVector"/>) geschafft haben, sind
                        „diffamieren“ oder „systems“.</para>
                </sect3>
                <sect3 xml:id="ch_NB_improve_hyperonym"> <!-- Hyperonyme -->
                    <title>Verwendung von Oberbegriffen (Hyperonymen)</title>
                    <para>Die in der Aufbereitungsphase sehr rechenintensive und in <xref linkend="ch_LexSemProp_Hyperonym"/> erläuterte Hinzufügung 
                        von <indexterm><primary>Hyperonym</primary></indexterm>Hyperonymen zum Attributvektor brachte nur eine sehr geringfügige
                        Verbesserung der Erfolgsquote von 59,42% auf 59,59% und ist somit zumindest in der untersuchten Implementierungsvariante nicht 
                        ökonomisch sinnvoll. Der Attributvektor wuchs übrigens von 16.270 auf 25.912 Einheiten. Auch hier kann man wieder leicht spekulieren,
                        dass erhöhte stochastische Abhängigkeit zwischen den Attributen die angestrebte Verbesserung der Klassifikationsqualität ausgebremst
                        haben könnte.</para>
                    <para>Interessant ist im Zusammenhang mit der Untersuchung von Hyperonymverwendung noch die Frage, ob eine Ersetzung des kompletten 
                        Wortvektors durch die Hyperonyme zu einem guten Klassifikator führt. Wurde der Wortvektor aus der Attributliste entfernt, blieben
                        2.165 Attribute übrig. Ein Naïve-Bayes-Klassifikator ließ sich so in 7 Sekunden erzeugen und in 2 Minuten und 24 Sekunden mit einer
                        Testmenge von 2.305 E-Book-Datensätzen evaluieren (im Vergleich dazu: der erwähnte vollständige Attributvektor mit 25.912 Attributen 
                        benötigte 1 Minute 36 Sekunden in der Trainingsphase und etwas über eine halbe Stunde zur Evaluation). Die Erfolgsquote dieses rein
                        hyperonymbasierten Klassifikators lag mit 47,27% deutlich niedriger.</para>
                </sect3>
                <sect3 xml:id="ch_NB_improve_pclem_corrections"><!-- POS-Tagging- und Lemmakorrektor -->
                    <title>Korrektur der Wortartenerkennung und der Lemmatisierung</title>
                    <para>Die in der Vorverarbeitung genutzten Verfahren zur Wortartenerkennung (<indexterm><primary>POS-Tagging</primary></indexterm>
                        POS-Tagging) und Lemmatisierung<indexterm><primary>Lemmatisierung</primary></indexterm> basieren selbst auf Maschinenlernerfahren. Mit
                        dem Auftreten fehlerhaft ausgezeichneter Wortarten oder falsch lemmatisierten Wörtern ist also zu rechnen.</para>
                    <para>Wie der Name andeutet, basiert der TreeTagger auf einem Entscheidungsbaumalgorithmus, konkret ID3, welcher zum Taggen einer Wortart  
                        die Wortarten von bis zu <emphasis>k</emphasis> vorangehenden Wörtern in Betracht zieht (<biblioref linkend="bib_Sch95"/>). Der TreeTagger
                        arbeitet probabilistisch, da die Klassen, die für gewöhnlich in den Blättern von Entscheidungsbäumen stehen, durch Wahrscheinlichkeitsverteilungen 
                        ersetzt werden. Außerdem baut der TreeTagger in der Trainingsphase ein Präfix- und Suffixlexikon auf, um auf dessen Basis auch die Wortarten von 
                        unbekannten Wörtern, also von Wörtern, die dem Tagger in der Trainingsphase nicht präsentiert worden sind, besser schätzen zu können.</para>
                    <para>In <biblioref linkend="bib_Sch95"/> berichtet Schmid von einer Erfolgsquote zwischen 96% und 98%. Unsere kleine POS-Tagger-Evaluation in 
                        <xref linkend="ch_GramProp"/> wies ja eine höhere Fehler- und somit geringere Erfolgsquote auf, so dass das verwendete TreeTagger-Modell vermutlich
                        an seine Trainingsdaten überangepasst ist. Uns interessierte nun die Frage, ob eine nachträgliche Korrektur von POS-Tagging-Fehlern einen 
                        (positiven) Einfluss auf unsere Klassifikatoren haben kann. Zur Klärung dieser Frage wurden über 2700 beobachtete Fehler in Tripel-Form in 
                        einer Textdatei gesammelt: (falsch ausgezeichnetes Wort, falsche Wortart, richtige Wortart)<footnote><para>Diese Korrekturdatei ist online verfügbar unter <link
                            xlink:href="https://github.com/sermo-de-arboribus/Avve/blob/version-1.0/Avve/src/main/resources/opennlp/postag-de-dict.txt"
                                /> (Zugriff am 3.4.2018).</para></footnote>. Um solche Fehler zu finden, wurden verschiedene Strategien angewendet: Erstens ein 
                        Blick in die mit TF-IDF-Werten gerankten Wortvektoren von zufällig ausgewählten E-Book-Datensätzen, zweitens durch Kontrolle auffälliger
                        Wörter im Lucene-Index und drittens durch Lesen einzelner POS-getaggter Sätze in wiederum zufällig ausgewählten E-Book-Datensätzen.</para>
                    <para>In gleicher Weise wurde auch bei der Lemmatisierung vorgegangen, nur dass die Tripel hier lauten: (falsch bzw. nicht lemmatisiertes Wort,
                        POS-Tag, korrekt lemmatisiertes Wort)<footnote><para>Die entsprechende Datei mit über 2500 Korrekturen ist online einsehbar unter <link
                            xlink:href="https://github.com/sermo-de-arboribus/Avve/blob/version-1.0/Avve/src/main/resources/opennlp/lemmatizer-de-dict.txt"/> (Zugriff am 
                            3.4.2018).</para></footnote>.</para>
                    <para>Bei einer Lucene-Indexgröße von fast 3.000.000 Termen fallen 2.000 bis 3.000 Korrekturen vermutlich nicht sonderlich ins Gewicht. Lässt sich
                        aber prinzipiell eine (kleine) Verbesserung beobachten, wäre das ein Hinweis darauf, dass verbessertes POS-Tagging und verbesserte Lemmatisierung
                        auch zu einem besseren Klassifikator führen &#x2012; schließlich basieren einige unserer Attribute direkt auf grammatischen Texteigenschaften,
                        welche wiederum auf Basis der Wortarten berechnet werden.</para>
                    <table frame="all" xml:id="tab_NB_mit_POS_Lem_Korrekturen"><!-- Ergebnisse Naïve Bayes mit Korrekturen von POS-Tags und Lemmata -->
                        <title>Ergebnisse Naïve Bayes mit Korrekturen von POS-Tags und Lemmata</title>
                        <tgroup cols="5">
                            <colspec colname="Nummer" colnum="1" colwidth="1*" align="left"/>
                            <colspec colnum="2" colwidth="2*" align="left" colname="Verfahren"/>
                            <colspec colnum="3" colwidth="1*" align="left"
                                colname="Attributanzahl"/>
                            <colspec colname="Laufzeit" colnum="4" colwidth="2*" align="left"/>
                            <colspec colnum="5" colwidth="1*" align="left"
                                colname="Erfolgsquote"/>
                            <thead>
                                <row>
                                    <entry>Nummer</entry>
                                    <entry>Verfahren</entry>
                                    <entry>Attributanzahl</entry>
                                    <entry>Laufzeit</entry>
                                    <entry>Erfolgsquote</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>28a</entry>
                                    <entry>Naïve Bayes ohne Optimierungen</entry>
                                    <entry>16.270</entry>
                                    <entry>53s (Training), 18m 16s (Evaluation)</entry>
                                    <entry>59,42%</entry>
                                </row>
                                <row>
                                    <entry>36</entry>
                                    <entry>Naïve Bayes mit korrigierten POS-Tags</entry>
                                    <entry>16.358</entry>
                                    <entry>53s (Training), 18m 18s (Evaluation)</entry>
                                    <entry>59,81%</entry>
                                </row>
                                <row>
                                    <entry>37</entry>
                                    <entry>Naïve Bayes mit korrigierten POS-Tags und Lemmata</entry>
                                    <entry>16.326</entry>
                                    <entry>52s (Training), 18m 52s (Evaluation)</entry>
                                    <entry>59,64%</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                    <para>Die Größe des Lucene-Indexes stieg bei Anwendung der POS-Korrektur um 402 auf 2.972.033 Terme und sank bei zusätzlicher Lemma-Korrektur
                        auf 2.971.001, was 630 Terme unterhalb der Ausgangsgröße liegt.</para>
                    <para>Wie in <xref linkend="tab_NB_mit_POS_Lem_Korrekturen"/> ersichtlich, führt die Wortartenkorrektur zu einer Verbesserung der Erfolgsquote
                        um 0,39 Prozentpunkte. Ein Teil dieser Verbesserung geht bei zusätzlicher Korrektur der Lemmatisierung wieder verloren. Ein Testlauf, der
                        ausschließlich die Lemmata korrigiert, nicht aber die POS-Tags, wurde nicht durchgeführt, da die Lemmatisierung auf der Wortarteninformation
                        aufbaut und viele der Lemma-Korrekturregeln nicht greifen würden, wenn nicht zuvor auch die Wortarten korrigiert wurden.</para>
                </sect3>
            </sect2>
            <sect2 xml:id="ch_NB_improve_überblick"><!-- Überblick und Fazit Optimierungsversuche Naïve Bayes -->
                <title>Überblick und Fazit</title>
                <para>Die größte Verbesserung der Warengruppenklassifikation mit Naïve Bayes ergab sich bei Verwendung von <indexterm><primary>AdaBoost</primary>
                </indexterm>AdaBoost.M1 mit Weighting 
                    der Trainingsdaten. Im Bereich der Datenaufbereitung zeigte das Entfernen von URLs den besten Effekt, der jedoch letztlich ein bescheidener blieb.</para>
                <para>Wendet man alle diejenigen Datenoptimierungen gemeinsam an, welche eine kleine Verbesserung der Erfolgsquote gegenüber dem Basislerner erbrachten,
                    also das Ausfiltern von Fremdwörtern, das Auflösen von Ligaturen, die Verwendung von <indexterm><primary>Hyperonym</primary>
                    </indexterm>Hyperonymen und die Wortartenkorrektur, so erbrachte auch dies 
                    nur eine sehr geringfügige Verbesserung der Erfolgsquote von 59,4241% auf 59,6788%. Wurden nicht nur die einzeln positiv zu Buche schlagenden Datenmodifikationen
                    verwendet, sondern alle in <xref linkend="ch_NB_improve_attributes"/> geschilderten gemeinsam, lag die Erfolgsquote auf demselben Niveau: bei 58,6806%.</para>
                <!--<para>[[ TO DO : Evtl. Tabelle mit allen Versuchen und micro- und macro-averaged-Werten anfügen]]</para>-->
            </sect2>
        </sect1>
    </chapter>
    <chapter xml:id="ch_SVM"> <!-- WG-Klassifikation mit SMO -->
        <info>
            <title>WG-Klassifikation mit SMO (Supportvektormaschine)</title>
        </info>
        <sect1> <!-- Darstellung des SMO-Verfahrens -->
            <title xml:id="ch_SVM_intro">Darstellung des Verfahrens</title> <!-- Darstellung des Verfahrens -->
            <para><indexterm class="startofrange" xml:id="idt_008"><primary>Supportvektormaschine</primary></indexterm>Supportvektormaschinen (SVM)<footnote>
                <para>Trotz ihres Namens handelt es sich bei der Supportvektormaschine um eine Bezeichnung für einen <emphasis>Algorithmus</emphasis>, nicht
                    jedoch um eine Maschine im Sinne der Automatentheorie.</para></footnote> gehören zu den am besten für die Textklassifikation<footnote><para>Nach
                    <biblioref linkend="bib_RN16"/>, S. 744, handelt es sich sogar um ein Allroundverfahren, welches fast immer angewendet werden kann, auch wenn man 
                    wenig über die zu untersuchenden Daten weiß.</para></footnote> geeigneten Maschinenlernverfahren (<biblioref linkend="bib_Seb02"/>, S. 39), verorten
                die zu lernenden und zu klassifizierenden Datensätze in einem k-dimensionalen Raum und suchen nach einer <indexterm><primary>Hyperebene</primary>
                </indexterm>Hyperebene in diesem Raum, welche die Klassen voneinander trennt. Zur Ermittlung bzw. Definition dieser Ebene wird nicht die gesamte
                Trainingsmenge benötigt, sondern jeweils nur die der Ebene am nächsten gelegenen Datensätze. Diese nächstgelegenen Datensätze werden
                als Supportvektoren bezeichnet, woher das Verfahren auch seinen Namen hat. Die Hyperebene wird so eingezogen, dass sie den maximalen Abstand
                zu allen Supportvektoren hat (engl. <emphasis>maximum margin hyperplane</emphasis>). Dadurch, dass die Hyperebene nicht durch sämtliche
                Trainingsdatensätze, sondern nur durch wenige, elaborierte „Grenzdatensätze“ definiert wird, ist die Gefahr der <indexterm>
                    <primary>Überanpassung</primary></indexterm>Überanpassung beim Supportvektor-Verfahren gering (<biblioref linkend="bib_Ert16"/>, S. 298; 
                <biblioref linkend="bib_Seb02"/>, S. 30; <biblioref linkend="bib_WFHP17"/>, S. 253-255). Auch dass der Abstand der klassentrennenden Hyperebene
                zu den Supportvektoren maximiert wird trägt zur Robustheit gegenüber Überanpassung bei. Das erlernte Modell einer SVM besteht also nicht
                wie bei Naïve Bayes aus einer Reihe von festgelegten Parametern (Wahrscheinlichkeitswerten), sondern aus einer Menge von Trainingsinstanzen,
                für welche ermittelt wurde, dass sie die besondere Rolle eines Supportvektors einnehmen, sowie der während des Trainingsphase ermittelten 
                Funktionsparameter zur Bestimmung der Hyperebene.</para>
            <para>Eine klassentrennende Hyperebene lässt sich leicht finden für <indexterm><primary>linear separierbar</primary></indexterm>linear separierbare
                Probleme. SVMs funktionieren aber auch für nicht linear separierbare Lernprobleme.
                Der Unterschied zwischen linear separierbaren und nicht separierbaren Problem lässt sich leicht mit den Booleschen Funktionen AND und XOR zeigen.
                Man nehme an, diese beiden Funktionen sollten gelernt werden. Jeder Eingabedatensatz besteht aus zwei Attributen A und B mit dem Wertebereich {0, 1}
                und das Ergebnis der Funktion sei die zu lernende Klasse C, ebenfalls mit dem Wertebereich {0, 1}. Trägt man die AND-Funktion in einem 
                Koordinatensystem ein wie in <xref linkend="abb10"/> (wo Klasse 0 durch - und Klasse 1 durch + symbolisiert wird), so sieht man, dass sich die beiden 
                Klassen durch Einziehen einer Gerade, z.&#160;B. mit der linearen Gleichung <inlineequation><mathphrase>y = -x + 3/2</mathphrase></inlineequation>, trennen
                lassen.</para>
            <?dbfo-need height="8cm" ?>
            <sidebar><!-- Linear separierbares Problem: Die AND-Funktion -->
                <?dbfo sidebar-width="8cm"?>
                <?dbfo float-type="left"?>
                <figure xml:id="abb10" pgwide="0"> <!-- Linear separierbares Problem: Die AND-Funktion -->
                    <title>Linear separierbares Problem: Die AND-Funktion</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="../img/Abb10_linear_separierbares_Problem.svg" width="100%"/>
                        </imageobject>
                    </mediaobject>
                </figure>
            </sidebar>
            <para>Für die XOR-Funktion gibt es hingegen keine Gerade bzw. lineare Funktion, welche die beiden Ergebnisklassen voneinander trennt (siehe <xref
                linkend="abb11"/>). Die beiden Beispiele AND und XOR dienten uns zur Veranschaulichung, weil sich zweidimensionale Probleme grafisch leicht
                darstellen lassen. Man beachte dabei, dass die lineare Trenngerade eindimensional ist, also eine Dimension unterhalb des Problemraums liegt. Diese
                Beziehung ist regelhaft: Ein k-dimensionales Klassifizierungsproblem ist linear separierbar, wenn es eine (k-1)-dimensionale Hyperebene gibt, welche
                die Klassen voneinander trennt.</para>
            <para>Eine lineare Supportvektormaschine geht, wie der Name sagt, von einer (zumindest näherungsweise) linearen Trennbarkeit des Lernproblems aus. 
                Eine Supportvektormaschine kann aber auch so arbeiten, dass sie auf den Attributvektorraum eine nicht-lineare Transformation anwendet, die
                dazu führt, dass das Problem in einer höheren Dimension möglicherweise linear separabel wird, sofern es dies noch nicht war. Es lässt sich beweisen,
                dass es immer möglich ist, ein nicht-widersprüchliches Lernproblem in einen linear separierbaren Vektorraum zu überführen, jedoch kann der
                Zielvektorraum exponentiell mit der Zahl der Dimensionen des ursprünglichen Vektorraums wachsen (<biblioref linkend="bib_Ert16"/>, S. 299;
                <biblioref linkend="bib_WFHP17"/>, S. 252). Andererseits besteht eine obere Dimensionsschranke darin, dass (von einigen Sonderfällen abgesehen)
                N Trainingsinstanzen immer in N-1 Dimensionen linear separiert werden können (<biblioref linkend="bib_RN16"/>, S. 746).</para>
            <sidebar> <!-- Linear nicht separierbares Problem: Die XOR-Funktion -->
                <!--<?dbfo-need height="8cm" ?>-->
                <?dbfo sidebar-width="8cm"?>
                <?dbfo float-type="left"?>
                <figure xml:id="abb11" pgwide="0"> <!-- Linear nicht separierbares Problem: Die XOR-Funktion -->
                    <title>Linear nicht separierbares Problem: Die XOR-Funktion</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="../img/Abb11_nicht_linear_separierbares_Problem.svg" width="100%"/>
                        </imageobject>
                    </mediaobject>
                </figure>
            </sidebar>
            <para>Um zu zeigen, wie die Supportvektormaschine die trennende Hyperebene mit größtmöglichem Abstand zwischen den Klassen ermittelt, bedienen wir 
                uns wieder des einfachen Falls eines linear trennbaren Klassifikationsproblems mit zwei Klassen und zwei Attributen, der sich anschaulich
                zweidimensional darstellen lässt (siehe <xref linkend="abb12"/>). Die SVM ermittelt zunächst die konvexe Hülle um die beiden Klassen, veranschaulicht
                durch die Polygone, welche um die Instanzen jeder Klasse so gezogen sind, dass alle Instanzen auf oder innerhalb der Hülle liegen. Die konvexe Hülle
                hat zudem noch die Eigenschaften, dass (1) die Verbindung zweier beliebiger Punkte, welche zur Menge gehören, ebenfalls vollständig innerhalb der Hülle
                liegt und (2) dass sie im Bezug auf die Ausgangsmenge (also hier die Instanzen einer Klasse) minimal ist: Es gibt kein kleineres Polygon, welches
                Bedinung (1) erfüllt und alle Punkte der Menge enthält.</para>
            <sidebar><!-- Maximum Margin Hyperplane-->
                <?dbfo sidebar-width="8cm"?>
                <?dbfo float-type="left"?>
                <figure xml:id="abb12" pgwide="0"> <!-- Maximum Margin Hyperplane-->
                    <title>Maximum Margin Hyperplane</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="../img/Abb12_maximum_margin_hyperplane.png" width="100%"/>
                        </imageobject>
                    </mediaobject>
                    <caption><para>Abb. aus <biblioref linkend="bib_WFHP17"/>, S. 253</para></caption>
                </figure>
            </sidebar>
            <para>Weil wir voraussetzen, dass die Klassen linear separierbar sind, können die konvexen Hüllen um die Klassen sich offensichtlich nicht überlappen. Die
                zu ermittelnde Maximum Margin Hyperplane soll maximalen Abstand zu den beiden konvexen Hüllen haben. Grafisch-geometrisch lässt sich diese Hyperebene
                ermitteln, indem wir zunächst die kürzeste Linie ermitteln, welche die beiden Hüllen verbindet (in <xref linkend="abb12"/> als gestrichelte Linie
                dargestellt) und anschließend die Mittelsenkrechte dieser Linie einzeichnen. Die Instanzpunkte, welche dieser Mittelsenkrechten am nächsten liegen, 
                sind die Supportvektoren, von denen es pro Klasse mindestens einen, möglicherweise aber auch mehrere gibt (in der Abbildung verdeutlicht durch einen
                zusätzlichen Kreis um die betreffenden Instanzpunkte). Zur Berechnung der trennenden Hyperebene sind lediglich die Supportvektoren nötig, 
                alle anderen Instanzen der Klasse können ignoriert werden.</para>
            <para>Wir wollen hier nicht auf alle mathematischen Details von SVMs eingehen, aber doch noch eine zentrale Funktionsweise nicht-linearer SVMs erläutern,
                nämlich den Kerneltrick.
                Mit diesem Trick gelingt es dem SVM-Algorithmus, aufwändige Skalarprodukt-Berechnunngen im niedrigerdimensionalen Raum durchzuführen, um damit das
                Lernproblem im höherdimensionalen Raum zu lösen, was offensichtlich günstiger ist als die Skalarprodukte im höherdimensionalen Raum selbst zu 
                berechnen. Wenn wir von Konstanten und Lernparametern abstrahieren, bleibt beim Berechnen der Maximum Margin
                Hyperplane oder bei der Klassifikation von Testinstanzen im Wesentlichen eine Summe von Skalarprodukten zu berechnen:</para>
            <equation xml:id="form_maximum_margin_hyperplane"> <!-- Maximum Margin Hyperplane -->
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                        <munder>
                            <mo stretchy="false">∑</mo>
                            <mrow>
                                <mi>i,j</mi>
                                <mtext>&#160;</mtext>
                                <mo stretchy="false">∈</mo>
                                <mtext>&#160;</mtext>
                                <mi mathvariant="italic">SV</mi>
                            </mrow>
                        </munder>
                        <mrow>
                            <mrow>
                                <mover accent="true">
                                    <msub>
                                        <mi>x</mi>
                                        <mi>i</mi>
                                    </msub>
                                    <mo stretchy="true">⃗</mo>
                                </mover>
                                <mo stretchy="false">⋅</mo>
                                <mover accent="true">
                                    <msub>
                                        <mi>x</mi>
                                        <mi>j</mi>
                                    </msub>
                                    <mo stretchy="true">⃗</mo>
                                </mover>
                            </mrow>
                        </mrow>
                    </mrow>
                </math>
            </equation>
            <para>Soll nun auf der Suche nach einer klassentrennenden Hyperebene in eine höhere Dimension gewechselt werden, so sind alle Attributvektoren durch
                eine entsprechende Funktion <inlineequation><mml:math><mml:mi>&#934;</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi>
                    <mml:mo stretchy="true">⃗</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:math></inlineequation> in die gewünschte höhere Dimension zu projezieren.
                Die anschließende Auswertung der Skalarprodukte in <xref linkend="form_maximum_margin_hyperplane"/> würde aufgrund der größeren Vektoren
                dann entsprechend aufwändiger.</para>
            <para>Wählt man die Transformationsfunktion &#934; geschickt, indem man eine sogenannte Kernelfunktion verwendet, so gilt z.&#160;B. für die polynomielle
                Kernelfunktion:</para>
            <equation xml:id="form_Kernel"> <!-- Kernelfunktion -->
                <mml:math>
                    <mml:mrow>
                        <mml:mi>&#934;</mml:mi>
                        <mml:mo>(</mml:mo>
                        <mml:mover accent="true">
                            <mml:msub>
                                <mml:mi>x</mml:mi>
                                <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo stretchy="true">⃗</mml:mo>
                        </mml:mover>
                        <mml:mo>)</mml:mo>
                        <mml:mtext>&#160;</mml:mtext>
                        <mml:mo stretchy="false">⋅</mml:mo>
                        <mml:mtext>&#160;</mml:mtext>
                        <mml:mi>&#934;</mml:mi>
                        <mml:mo>(</mml:mo>
                        <mml:mover accent="true">
                            <mml:msub>
                                <mml:mi>x</mml:mi>
                                <mml:mi>j</mml:mi>
                            </mml:msub>
                            <mml:mo stretchy="true">⃗</mml:mo>
                        </mml:mover>
                        <mml:mo>)</mml:mo>
                    </mml:mrow>
                    <mml:mtext>&#160;</mml:mtext>
                    <mml:mo stretchy="false">=</mml:mo>
                    <mml:mtext>&#160;</mml:mtext>
                    <mml:mrow>
                        <mml:msup>
                            <mml:mrow>
                                <mml:mo>(</mml:mo>
                                <mml:mover accent="true">
                                    <mml:msub>
                                        <mml:mi>x</mml:mi>
                                        <mml:mi>i</mml:mi>
                                    </mml:msub>
                                    <mml:mo stretchy="true">⃗</mml:mo>
                                </mml:mover>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mo stretchy="false">⋅</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mover accent="true">
                                    <mml:msub>
                                        <mml:mi>x</mml:mi>
                                        <mml:mi>j</mml:mi>
                                    </mml:msub>
                                    <mml:mo stretchy="true">⃗</mml:mo>
                                </mml:mover>
                                <mml:mo>)</mml:mo>
                            </mml:mrow>
                            <mml:mi>d</mml:mi>
                        </mml:msup>
                    </mml:mrow>
                </mml:math>
            </equation>
            <para>Durch diesen Kerneltrick kann im höherdimensionalen Raum gelernt werden, während die leichter zu berechnende Kernelfunktion ausgewertet wird
                (<biblioref linkend="bib_RN16"/>, S. 747). Supportvektormaschinen bleiben so auch bis in hohe Dimensionen performant (<biblioref linkend="bib_Seb02"/>,
                S. 30). Ohne diesen Trick wären die Basisoperationen von SVMs „mathematisch teuer“:</para>
            <orderedlist spacing="compact">
                <listitem>
                    <para>die Transformation des nicht linear trennbaren Klassifikationsproblems in ein potentiell linear trennbares Problem würde die sowieso schon
                        hohe Dimensionalität und damit die Aufwändigkeit der Berechnung von Skalarprodukten der beteiligten Attributbektoren noch weiter erhöhen,</para>
                </listitem>
                <listitem>
                    <para>es ist nicht leicht a priori feststellbar, ob ein Problem linear trennbar ist,</para>
                </listitem>
                <listitem>
                    <para>in der Trainingsphase müssten für viele Instanzen die Skalarprodukte der Attributvektoren verschiedener Instanzen berechnet werden,</para>
                </listitem>
                <listitem>
                    <para>in der Test- oder Anwendungsphase müssten zu klassifizierende Instanzen ebenfalls zunächst in die Zieldimension transponiert und 
                        anschließend Skalarprodukte der Instanzen mit allen Supportvektoren ermittelt werden.</para>
                </listitem>
            </orderedlist>
            <para>Zur Berechnung der Supportvektoren und weiterer Parameter wird quadratische Optimierung verwendet, zur Überführung des Problems in einen
                höherdimensionalen Raum eine Kernelfunktion, z.&#160;B. die polynomiale Kernelfunktion
                <inlineequation> <!-- polynomiale Kernelfunktion -->
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <semantics>
                            <mrow class="MJX-TeXAtom-ORD">
                                <mstyle displaystyle="true" scriptlevel="0">
                                    <mi>K</mi>
                                    <mo stretchy="false">(</mo>
                                    <mml:mover accent="true">
                                        <mml:mi>x</mml:mi>
                                        <mml:mo stretchy="true">⃗</mml:mo>
                                    </mml:mover>
                                    <mo>,</mo>
                                    <mml:mover accent="true">
                                        <mml:mi>y</mml:mi>
                                        <mml:mo stretchy="true">⃗</mml:mo>
                                    </mml:mover>
                                    <mo stretchy="false">)</mo>
                                    <mo>=</mo>
                                    <mo fence="false" stretchy="false">(</mo>
                                    <mml:mover accent="true">
                                        <mml:mi>x</mml:mi>
                                        <mml:mo stretchy="true">⃗</mml:mo>
                                    </mml:mover>
                                    <mml:mo stretchy="false">⋅</mml:mo>
                                    <mml:mover accent="true">
                                        <mml:mi>y</mml:mi>
                                        <mml:mo stretchy="true">⃗</mml:mo>
                                    </mml:mover>
                                    <mml:mo>+</mml:mo>
                                    <mml:mn>1</mml:mn>
                                    <msup>
                                        <mo fence="false" stretchy="false">)</mo>
                                        <mrow class="MJX-TeXAtom-ORD">
                                            <mi>d</mi>
                                        </mrow>
                                    </msup>
                                </mstyle>
                            </mrow>
                        </semantics>
                    </math>
                </inlineequation> oder eine radiale Basisfunktion (<biblioref linkend="bib_Bur98"/>, S. 137-142). Bei Wahl von d=1 findet keine Projektion auf eine höhere Dimension statt, so dass sich eine lineare SVM ergibt. Bei Wahl von d > 1
                erfolgt eine Projektion in eine höhere Dimension und somit der Versuch, ein nicht linear trennbares Lernproblem in höherer Dimension zu lösen.</para>
             <para>Anstatt ein nicht linear trennbares Problem in einer höheren Dimension zu lösen, kann man auch eine lineare SVM verwenden und <indexterm>
                 <primary>Schlupfvariable</primary></indexterm>Schlupfvariablen
                 einführen, welche Fehlklassifikationen bis zu einer Obergrenze hin prinzipiell erlauben, solche Fehlklassifikationen aber arithmetisch bestrafen.
                 Dieses Vorgehen funktioniert dann, wenn es bei der Positionierung der Attributvektoren im Raum einzelne Ausreißer gibt, welche eine klare
                 lineare Trennung der Klassen verhindern. Wenn man diese Ausreißer missachten könnte, wäre die Trennung mit Hilfe einer linearen Hyperebene 
                 hingegen problemlos möglich.</para>
            <para>Platt stellte in <biblioref linkend="bib_Pla99"/> eine weitere Optimierung von SVMs vor, welche bei der quadratischen Optimierung ansetzt und welche
                zu SMO (Sequential Minimal Optimization) führte, der Supportvektormaschinenvariante, welche im Weka-Paket standardmäßig enthalten ist und von uns hier
                verwendet wurde. Statt die quadratische Optimierung aufwändig auszurechnen, wählt SMO nach einem bestimmten Verfahren Teilprobleme der quadratischen
                Optimierung und löst diese analytisch. Der Optimierungseffekt von SMO ist vor allem bei dünn besetzten Attributvektoren (engl. <emphasis>sparse
                    data sets</emphasis>) sehr hoch<footnote><para><biblioref linkend="bib_DPHS98"/> berichten von einer Verdreißigfachung der Trainingsgeschwindigkeit bei Verwendung von SMO gegenüber dem
                    Standardvorgehen bei der Berechnung der quadratischen Optimierung. Platt selbst nennt Beschleunigungsfaktoren von 1 bis 1000.</para></footnote>.
                Die Eigenschaft dünn besetzter Attributvektoren ist in der Textklassifikation ja für gewöhnlich erfüllt, da die konkret in einem Dokument vorkommenden
                Worte nur einen Bruchteil des gesamten Korpusvokabulars ausmachen.<indexterm class="endofrange" startref="idt_008"/></para>
            <para>Die bis hierher beschriebene Supportvektormaschine ging immer von einem <indexterm><primary>binäre Klassifikation</primary>
            </indexterm>binären Zweiklassenproblem aus. Um ein Multiklassenproblem zu lösen, 
                erzeugt SMO jeweils ein erlerntes Modell für alle <inlineequation><mml:math><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>|</mml:mo><mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>|</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inlineequation> Klassenpaare mit 
                Wahrscheinlichkeitswerten für jedes Paar. Diese werden dann durch das statistische Verfahren der linearen Diskriminanzanalyse kombiniert, um 
                Testinstanzen zu klassifizieren (<biblioref linkend="bib_HT98"/>). Eine alternative Möglichkeit des Multiklassenlernens mit einer SVM wäre das
                Erzeugen von |C| „Einer-gegen-den-Rest“-Klassifizierern (<biblioref linkend="bib_Bur98"/>, S. 143).</para>
        </sect1>
        <sect1 xml:id="ch_SVM_results"> <!-- Darstellung der Ergebnisse -->
            <title>Darstellung der Ergebnisse</title>
            <para>Von Beginn der Untersuchungen an wurden auf jeder Teilmenge des Korpus sowohl Naïve Bayes als auch SMO ausgeführt. Wir können hier also
                die Ergebnisse für dieselben Trainings- und Testläufe angeben wie in <xref linkend="ch_NB_results"/>. Die dort genannten Einschränkungen
                bezüglich der mangelnden Vergleichbarkeit der Ergebnisse gelten natürlich auch hier, ebenso die Erläuterungen zu den Unterschieden in den
                den einzelnen Trainings- und Evaluationsläufen. Alle hier aufgeführten Läufe fanden mit der linearen Variante von SMO, also mit Parameter d=1
                in der polynomiellen Kernelfunktion, statt.</para>
            <para>Im direkten Vergleich mit Naïve Bayes zeigt sich in allen Läufen eine Überlegenheit von SMO, dessen Erfolgsquote stets zwischen 
                3 und fast 12 Prozentpunkten über Naïve Bayes lag.</para>
            <table frame="all" orient="port" pgwide="0" tocentry="1" xml:id="tab_SMO_bei_wachsendem_Korpus"> <!-- Median und Standardabweichung einiger Attributwerte und Klassen bei SMO-Lernen-->
                <info>
                    <title>Erfolgsquote von SMO bei wachsender Korpusgröße</title>
                </info>
                <tgroup cols="7" align="left" colsep="1" rowsep="1">
                    <colspec colname="nummer" colwidth="1*"/>
                    <colspec colname="train" colwidth="2*"/>
                    <colspec colname="test" colwidth="2*"/>
                    <colspec colname="attribute" colwidth="2*"/>
                    <colspec colname="klassen" colwidth="2*"/>
                    <colspec colname="erfolgsquote" colwidth="2*"/>
                    <colspec colname="bemerkungen" colwidth="4*"/>
                    <thead>
                        <row>
                            <entry>Nummer</entry>
                            <entry>Trainingsinstanzen</entry>
                            <entry>Testinstanzen</entry>
                            <entry>Attribute</entry>
                            <entry>Klassen</entry>
                            <entry>Erfolgsquote</entry>
                            <entry>Bemerkungen</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>2</entry>
                            <entry>1183</entry>
                            <entry>374</entry>
                            <entry>15768</entry>
                            <entry>114</entry>
                            <entry>65,78%</entry>
                            <entry>Erstmals Validierung mit dezidierter Testmenge</entry>
                        </row>
                        <row>
                            <entry>3</entry>
                            <entry>1491</entry>
                            <entry>374</entry>
                            <entry>15890</entry>
                            <entry>99</entry>
                            <entry>66,84%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>4</entry>
                            <entry>1892</entry>
                            <entry>630</entry>
                            <entry>19675</entry>
                            <entry>114</entry>
                            <entry>61,28%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>5</entry>
                            <entry>1987</entry>
                            <entry>675</entry>
                            <entry>19819</entry>
                            <entry>105</entry>
                            <entry>64,95%</entry>
                            <entry>Erstmals mit manuell korrigierten Trainingsdaten</entry>
                        </row>
                        <row>
                            <entry>6</entry>
                            <entry>2058</entry>
                            <entry>675</entry>
                            <entry>21271</entry>
                            <entry>142</entry>
                            <entry>61,04%</entry>
                            <entry>WG 111 und 112 hier erstmals nicht auf WG 110 abgebildet</entry>
                        </row>
                        <row>
                            <!-- zweistellige Klassen -->
                            <entry>7</entry>
                            <entry>2058</entry>
                            <entry>675</entry>
                            <entry>14657</entry>
                            <entry>46</entry>
                            <entry><!--NB: 64,74%-->73,48%</entry>
                            <entry>wie Nr. 6, aber dreistellige WG-Codes auf zwei Stellen gekürzt</entry>
                        </row>
                        <row>
                            <!-- mit controlled vocabulary -->
                            <entry>8</entry>
                            <entry>2259</entry>
                            <entry>821</entry>
                            <entry>13738</entry>
                            <entry>143</entry>
                            <entry><!--NB:45,86%-->59,00%</entry>
                            <entry>mit Verwendung eines kontrollierten Vokabulars</entry>
                        </row>
                        <row>
                            <entry>9</entry>
                            <entry>3353</entry>
                            <entry>1084</entry>
                            <entry>23454</entry>
                            <entry>153</entry>
                            <entry><!-- NB: 55,07%-->63,19%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <!-- wie 9, aber auf zweistellige Klassen gekürzt -->
                            <entry>10</entry>
                            <entry>3353</entry>
                            <entry>1084</entry>
                            <entry>23454</entry>
                            <entry>47</entry>
                            <entry><!--59,69%-->74,45%</entry>
                            <entry>Daten identisch mit Nr. 9, aber WG-Codes auf zweistellige Codes gekürzt</entry>
                        </row>
                        <row>
                            <!-- wie 9, aber auf niedrigfrequente Klassen entfernt -->
                            <entry>11</entry>
                            <entry>2927</entry>
                            <entry>934</entry>
                            <entry>23454</entry>
                            <entry>50</entry>
                            <entry><!-- 59,21% -->69,81%</entry>
                            <entry>Korpus wie Nr. 9, aber niedrigfrequente Klassen entfernt</entry>
                        </row>
                        <row>
                            <entry>12</entry>
                            <entry>4380</entry>
                            <entry>2016</entry>
                            <entry>26713</entry>
                            <entry>157</entry>
                            <entry><!-- NB: 58,48%-->70,04%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>13</entry>
                            <entry>4612</entry>
                            <entry>2292</entry>
                            <entry>23669</entry>
                            <entry>149</entry>
                            <entry><!-- NB: 59,16%-->66,19%</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>28a</entry>
                            <entry>4615</entry>
                            <entry>2292</entry>
                            <entry>16270</entry>
                            <entry>149</entry>
                            <entry><!-- NB: 59,42%-->66,62%</entry>
                            <entry>Referenzlauf für Optimierungsversuche</entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <para>Die Trainingszeit, die für SMO benötigt wurde, lag stets deutlich über der entsprechenden Dauer, die Naïve Bayes für
                dieselben Datensätze benötigte. Sie lag in allen Versuchen zwischen 1 und 10 Minuten. Bei der Evaluation der Testdaten lag SMO hingegen 
                meistens vor Naïve Bayes (zwischen ca. 1 und ca. 25 Minuten), so dass die Gesamtdauer eines Trainings- und Evaluationslaufs in der Regel unter 
                derjenigen von Naïve Bayes lag.</para>
            <para>Die Erfolgsquote im Referenzlauf 28a lag mit 66,623% mehr als sieben Prozentpunkte über derjenigen von Naïve Bayes. Die Genauigkeit
                lag in der Macro-Averaged-Betrachtung bei 0,3826, die Trefferquote bei 0,3178. Bei getrennter Betrachtung von größter Klasse (Warengruppe 112)
                und den übrigen Klassen im der Micro-Averaging-Variante ergeben sich Genauigkeiten von 0,6787 für WG 112 und 0,5497 für den Rest, sowie 
                Trefferquoten von 0,9148 für die Mehrheitsklasse und 0,59 für den Rest.</para>
            <para>Besonders gut gelernt wurden, mit einem F-Maß von 1,0, die Warengruppen 162 (zweisprachige Ausgaben deutsch/französisch mit nur 2 Trainings- 
                und einer Testinstanz), 292 (Kinder- und Jugendbuch Fremdsprachen mit 12 Trainings- und 6 Testinstanzen), 415 (Ratgeber Fotografieren und
                Filmen mit 6 Trainings- und 3 Testinstanzen), 458 (Getränke mit 10 Trainings- und 5 Testinstanzen), 914 (Sprachführer mit 6 Trainings- und
                3 Testinstanzen). Dieses Bild ist demjenigen bei Naïve Bayes sehr ähnlich und lässt auf eine Überanpassung an diese kleinen Trainingsklassen
                schließen.</para>
            <para>Trotz der insgesamt höheren Erfolgsquote von SMO gegenüber Naïve Bayes kommt es bei SMO zu einer noch größeren Menge an Klassen mit einem
                F-Maß von 0: Betroffen sind davon 71 Warengruppen, also fast die Hälfte. Für viele Klassen war die Zahl der Trainingsinstanzen zu gering und
                es scheint aufgrund dieser größeren Zahl an nicht gelernten Klassen so zu sein, dass SMO etwas empfindlicher gegenüber niedrig frequenten
                Trainingsdaten ist als Naïve Bayes. Zusätzlich zu den bei Naïve Bayes genannten sind dies bei SMO noch: Warengruppe 115 (Prosa-Anthologien mit
                12 Trainings- und 6 Testinstanzen), 123 (Horrorromane mit 6 Trainings- und 3 Testinstanzen), 365 (Reiseberichte Asien mit 14 Trainings- und
                7 Testinstanzen), 390 (Reise: Sonstiges mit 2 Trainings- und 1 Testinstanz), 422 (Naturführer mit 2 Trainings- und 1 Testinstanz), 453
                (allgemeine Kochbücher mit 10 Trainings- und 5 Testinstanzen), 948 (Sachbuch Zeitgeschichte mit 10 Trainings- und 5 Testinstanzen), 957
                (Sachbuch Sprache: Allgemeines mit 4 Trainings- und 2 Testinstanzen).</para>
            <para>Auch wenn die geringe Zahl an Trainingsdokumenten die wahrscheinlichste Ursache für die schlechte Klassifikation dieser niedrig frequenten
                Klassen ist, kann nicht ausgeschlossen werden, dass auch thematische Ähnlichkeiten und Überschneidungen die Klassifikation erschweren. Hier
                wären weitere Untersuchungen nötig, idealerweise mit einer Vergrößerung der Trainingsmenge<footnote><para><biblioref linkend="bib_DPHS98"/> berichten von guten
                Ergebnissen ab einer Trainingsmenge von 20 Dokumenten pro Klasse bei SMO; jedoch ging es in deren Studie um die Klassifikation englischsprachiger
                Nachrichtentexte von durchschnittlich 200 Wörtern pro Text, so dass auch hier fraglich ist, inwiefern eine solche Zahlenangabe mit unserem
                Anwendungsfall vergleichbar wäre.</para></footnote> oder alternativ durch spezielle Verfahren, welche das Maschinenlernen von Minderheitsklassen
                unterstützen (vgl. z.&#160;B. <biblioref linkend="bib_ZVJ03"/>).</para>
            <para>Betrachten wir auch hier wieder beispielhaft die Fehlklassifikationen bei den Reiseberichten über Deutschland (WG 361), von denen keine
               Testinstanz korrekt klassifiziert wurde. Stattdessen erhielten jeweils ein Test-E-Book die Warengruppe 111 (Romane, Erzählungen vor 1945), 
               112 (Romane, Erzählungen nach 1945), 185 (Humor), 312 (Reiseführer Deutschland) und zwei E-Books galten dem Modell als Reisebericht Europa (WG 363).
               Zwei Drittel der Fehlklassifikationen waren identisch mit der Naïve-Bayes-Klassifikation für dieselben E-Books. Es gab lediglich zwei Abweichungen:
            </para>
            <itemizedlist spacing="compact">
                <listitem>
                    <para>Der humorvolle Bericht über Almaufenthalte als landwirtschaftliche Aushilfskraft wurde von SMO als Roman (WG 112) angesehen, was 
                         noch etwas weniger vertretbar ist als die Humorklassifikation (WG 185) von Naïve Bayes.</para>
                </listitem>
                <listitem>
                    <para>Das Buch, welches Rundgänge durch Leipzig schildert, wurde von SMO gemeinsam mit dem Harzbuch aus derselben Buchreihe in die 
                         Europa-Reiseberichte gesteckt (WG 362), was ebenfalls weniger vertretbar ist als die WG-312-Klassifikation durch Naïve Bayes.</para>
                </listitem>
            </itemizedlist>
            <para>Auch beim SMO-Klassifikator interessiert natürlich ganz besonders, wie gut die frequentesten Klassen gelernt wurden. Bei der frequentesten 
                Warengruppe 112 liegt die Genauigkeit mit einem Wert von 0,6787 deutlich unterhalb von Naïve Bayes (0,7993), dafür aber die Trefferquote 
                bei 0,91438 (gegenüber 0,5523 bei NB). Das bedeutet also, dass SMO zwar mehr E-Books der Warengruppe 112 als solche erkannt hat, aber auch 
                viele Bücher aus anderen Warengruppen in Gruppe 112 eingeordnet hat. Von den falsch klassifizierten E-Books der WG 112 wurden 14 als Krimis 
                (WG 121) verortet, <indexterm><primary>Jugendbuch</primary></indexterm>12 als Jugendbuch (WG 260). Nur ein einziger Titel wurde außerhalb der belletristischen Literatur klassifiziert (Reisebericht
                Welt, WG 369). Die <emphasis>false positives</emphasis> stammen ebenfalls vorwiegend, aber nicht ausschließlich aus der Belletristik: 24-mal aus dem
                Jugendbuch (WG 260), 19-mal aus den Krimis (WG 121), jeweils 18-mal aus WG 111 (Romane, Erzählungen vor 1945) und WG 116 (biografische Romane),
                13-mal aus den historischen Romanen (WG 113). Bei den nicht-belletristischen <emphasis>false positives</emphasis> ragen die Sachbücher aus Politik 
                und Gesellschaft (WG 973) mit 8 Fehlklassifikationen hervor.</para>
            <para>Details zur Klassifikationsqualität der zehn frequentesten Warengruppen gibt die folgende Tabelle <xref linkend="tab_SMO_frequente_WG"/> wider.</para>
            <table frame="all" orient="port" pgwide="0" tocentry="1" xml:id="tab_SMO_frequente_WG"> <!-- Klassifikationsqualität in den frequentesten Warengruppen -->
                <info>
                    <title>SMO-Klassifikationsqualität in den frequentesten Warengruppen</title>
                </info>
                <tgroup cols="6" align="left" colsep="1" rowsep="1">
                    <colspec colname="warengruppe" colwidth="1*"/>
                    <colspec colname="anzahl" colwidth="1*"/>
                    <colspec colname="tp" colwidth="1*"/>
                    <colspec colname="precision" colwidth="1*"/>
                    <colspec colname="fmeasure" colwidth="1*"/>
                    <colspec colname="bemerkungen" colwidth="4*"/>
                    <thead>
                        <row>
                            <entry>Warengruppe</entry>
                            <entry>Testinstanzen</entry>
                            <entry>TP-Rate</entry>
                            <entry>Genauigkeit (Precision)</entry>
                            <entry>F-Maß</entry>
                            <entry>False negatives</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>112</entry><!-- Warengruppe -->
                            <entry>411</entry><!-- Testinstanzen -->
                            <entry>0,915</entry><!-- TP-Rate -->
                            <entry>0,679</entry><!-- Genauigkeit -->
                            <entry>0,779</entry><!-- F-Maß -->
                            <entry>14-mal WG 121, 12-mal WG 260</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>121</entry>
                            <entry>189</entry>
                            <entry>0,868</entry>
                            <entry>0,863</entry>
                            <entry>0,865</entry>
                            <entry>19-mal WG 112, 4-mal WG 260</entry>
                        </row>
                        <row>
                            <entry>260</entry><!-- Warengruppe -->
                            <entry>185</entry><!-- Testinstanzen -->
                            <entry>0,735</entry><!-- TP-Rate -->
                            <entry>0,760</entry><!-- Genauigkeit -->
                            <entry>0,747</entry><!-- F-Maß -->
                            <entry>24-mal WG 112, 8-mal WG 250, 7-mal WG 132, 5-mal WG 121</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>250</entry><!-- Warengruppe -->
                            <entry>94</entry><!-- Testinstanzen -->
                            <entry>0,660</entry><!-- TP-Rate -->
                            <entry>0,816</entry><!-- Genauigkeit -->
                            <entry>0,729</entry><!-- F-Maß -->
                            <entry>18-mal WG 260, 7-mal WG 112, 3-mal WG 182</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>132</entry><!-- Warengruppe -->
                            <entry>90</entry><!-- Testinstanzen -->
                            <entry>0,844</entry><!-- TP-Rate -->
                            <entry>0,809</entry><!-- Genauigkeit -->
                            <entry>0,826</entry><!-- F-Maß -->
                            <entry>5-mal WG 260, je 3-mal WG 112 und WG 121</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>481</entry><!-- Warengruppe -->
                            <entry>86</entry><!-- Testinstanzen -->
                            <entry>0,814</entry><!-- TP-Rate -->
                            <entry>0,519</entry><!-- Genauigkeit -->
                            <entry>0,633</entry><!-- F-Maß -->
                            <entry>6-mal WG 933, je 2-mal WG 973, WG 112, WG 185</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>973</entry><!-- Warengruppe -->
                            <entry>72</entry><!-- Testinstanzen -->
                            <entry>0,639</entry><!-- TP-Rate -->
                            <entry>0,329</entry><!-- Genauigkeit -->
                            <entry>0,434</entry><!-- F-Maß -->
                            <entry>5-mal 971, 4-mal 972, 3-mal WG 185, 2-mal 526</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>182</entry><!-- Warengruppe -->
                            <entry>59</entry><!-- Testinstanzen -->
                            <entry>1,0</entry><!-- TP-Rate -->
                            <entry>0,922</entry><!-- Genauigkeit -->
                            <entry>0,959</entry><!-- F-Maß -->
                            <entry></entry><!-- FN -->
                        </row>
                        <row>
                            <entry>185</entry><!-- Warengruppe -->
                            <entry>47</entry><!-- Testinstanzen -->
                            <entry>0,702</entry><!-- TP-Rate -->
                            <entry>0,550</entry><!-- Genauigkeit -->
                            <entry>0,617</entry><!-- F-Maß -->
                            <entry>5-mal 112, 4-mal 973, 2-mal 191</entry><!-- FN -->
                        </row>
                        <row>
                            <entry>693</entry><!-- Warengruppe -->
                            <entry>37</entry><!-- Testinstanzen -->
                            <entry>0,946</entry><!-- TP-Rate -->
                            <entry>0,854</entry><!-- Genauigkeit -->
                            <entry>0,897</entry><!-- F-Maß -->
                            <entry>1-mal 692</entry><!-- FN -->
                        </row>
                    </tbody>
                </tgroup>
            </table>
        </sect1>
        <sect1 xml:id="ch_SMO_improve"> <!-- Verbesserungsversuche -->
            <title>Verbesserungsversuche / Boosting</title>
            <sect2 xml:id="ch_SMO_improve_params"> <!-- Wahl der Kernelfunktion und des Komplexitätsparameters -->
                <title>Wahl der Kernelfunktion und des Komplexitätsparameters</title>
                <para>Bei der Verwendung einer Supportvektormaschine ist zunächst zu entscheiden, ob man sie in einer einfachen, linearen Variante einsetzt 
                    oder auf einer Kernelfunktion zur Transformation des Problemraums in einer höhere Dimension aufbaut. Da man einem Problem weder von außen 
                    ansieht, noch einfach berechnen kann, ob sich die Klassen linear trennen lassen, liegt es nahe, zunächst die weniger komplexe lineare 
                    Variante einer SVM auszuprobieren. Der Referenzlauf mit einer solchen linearen SVM wurde oben in <xref linkend="ch_SVM_results"/> 
                    vorgestellt.</para>
                <para>Versuche mit einem polynomiellen Kernel waren insofern nicht erfolgreich, als sich das SMO-Modell auf handelsüblichen Bürocomputern 
                    und -servern mit bis zu 16 GB RAM nicht in annehmbarer Zeit berechnen ließ: Versuche in dieser Richtung wurden jeweils nach ca. einer 
                    Woche Laufzeit abgebrochen. Als Exponenten der Kernelfunktion wurden die Werte 2 und 1,0001 ausprobiert. Um für unsere E-Book-Daten auf
                    einer SVM mit polynomiellem Kernel zu Ergebnissen zu kommen, wären also weitere Versuche mit Maßnahmen aus dem folgenden Maßnahmenkatalog
                    anzustellen:</para>
                <itemizedlist spacing="compact">
                    <listitem>
                        <para>Reduktion der Größe der Attributvektors</para>
                    </listitem>
                    <listitem>
                        <para>Parallelisierung der Berechnung</para>
                    </listitem>
                    <listitem>
                        <para>Ausführung auf einem Großrechner</para>
                    </listitem>
                </itemizedlist>
                <para>Wählt man die Gaußsche radiale Basisfunktion (RBF) als Kernel<footnote><para>Nach <biblioref linkend="bib_SC08"/>, S. 17, eine der wichtigsten
                    Kernelarten in der SVM-Praxis.</para></footnote>, so ist in Weka ein &#947;-Parameter festzulegen<footnote><para>Es existieren alternative Formulierungen von RBF, die sich nicht nur in der Wahl der
                            Variablennamen, sondern auch in einigen Multiplikatoren unterscheiden können, das Grundprinzip ist aber stets gleich. Nach <biblioref
                           linkend="bib_Bur98"/> (S. 157) lautet die Formel für RBF mit
                    &#963; statt &#947; wie folgt:</para>
                        <equation xml:id="form_RBF-Kernel_Burges"> <!-- RBF-Kernelfunktion nach Burges -->
                            <mml:math>
                                <mml:mrow>
                                    <mml:mi>K</mml:mi>
                                    <mml:mo stretchy="false">(</mml:mo>
                                    <mml:mover accent="true">
                                        <mml:msub>
                                            <mml:mi>x</mml:mi>
                                            <mml:mi>i</mml:mi>
                                        </mml:msub>
                                        <mml:mo stretchy="true">⃗</mml:mo>
                                    </mml:mover>
                                    <mml:mtext>&#160;,&#160;</mml:mtext>
                                    <mml:mover accent="true">
                                        <mml:msub>
                                            <mml:mi>x</mml:mi>
                                            <mml:mi>j</mml:mi>
                                        </mml:msub>
                                        <mml:mo stretchy="true">⃗</mml:mo>
                                    </mml:mover>
                                    <mml:mo stretchy="false">)</mml:mo>
                                    <mml:mtext>&#160;</mml:mtext>
                                    <mml:mo stretchy="false">=</mml:mo>
                                    <mml:mtext>&#160;</mml:mtext>
                                    <mml:msup>
                                        <mml:mi>e</mml:mi>
                                        <mml:mrow>
                                            <mml:mo stretchy="false">-</mml:mo>
                                            <mml:msup>
                                                <mml:mrow>
                                                    <mml:mo fence="false" stretchy="false">‖</mml:mo>
                                                    <mml:mover accent="true">
                                                        <mml:msub>
                                                            <mml:mi>x</mml:mi>
                                                            <mml:mi>i</mml:mi>
                                                        </mml:msub>
                                                        <mml:mo stretchy="true">⃗</mml:mo>
                                                    </mml:mover>
                                                    <mml:mo fence="false" stretchy="false">-</mml:mo>
                                                    <mml:mover accent="true">
                                                        <mml:msub>
                                                            <mml:mi>x</mml:mi>
                                                            <mml:mi>j</mml:mi>
                                                        </mml:msub>
                                                        <mml:mo stretchy="true">⃗</mml:mo>
                                                    </mml:mover>
                                                    <mml:mo fence="false" stretchy="false">‖</mml:mo>
                                                </mml:mrow>
                                                <mml:mn>2</mml:mn>
                                            </mml:msup>
                                            <mml:mtext>&#160;</mml:mtext>
                                            <mml:mo fence="true" stretchy="false">/</mml:mo>
                                            <mml:mtext>&#160;</mml:mtext>
                                            <mml:msup>
                                                <mml:mrow>
                                                    <mml:mn>2</mml:mn>
                                                    <mml:mi>&#963;</mml:mi>
                                                </mml:mrow>
                                                <mml:mn>2</mml:mn>
                                            </mml:msup>
                                        </mml:mrow>
                                    </mml:msup>
                                </mml:mrow>
                            </mml:math>
                        </equation>
                    <para>Da der Parameter in der RBF-Implementierung von Weka Gamma heißt, haben wir im Haupttext die von Burges' Formulierung abweichende
                        Weka-Variante gewählt. <biblioref linkend="bib_SC08"/> geben &#947; mit negativem Exponenten an:
                        <equation xml:id="form_RBF-Kernel_SC"> <!-- RBF-Kernelfunktion nach Steinwart / Christmann -->
                            <mml:math>
                                <mml:msub>
                                    <mml:mi>k</mml:mi>
                                    <mml:mi>&#947;</mml:mi>
                                </mml:msub>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:mi>x</mml:mi>
                                <mml:mtext>&#160;,&#160;</mml:mtext>
                                <mml:mi>x&#8242;</mml:mi>
                                <mml:mo stretchy="false">)</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mo stretchy="false">:=</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mi>exp</mml:mi>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mo>-</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:msup>
                                    <mml:mi>&#947;</mml:mi>
                                    <mml:mrow>
                                        <mml:mtext>&#160;</mml:mtext>
                                        <mml:mo>-</mml:mo>
                                        <mml:mn>2</mml:mn>
                                    </mml:mrow>
                                </mml:msup>
                                <mml:msubsup>
                                    <mml:mrow>
                                        <mml:mo fence="false" stretchy="false">‖</mml:mo>
                                        <mml:mi>x</mml:mi>
                                        <mml:mtext>&#160;</mml:mtext>
                                        <mml:mo stretchy="false">-</mml:mo>
                                        <mml:mtext>&#160;</mml:mtext>
                                        <mml:mi>x&#8242;</mml:mi>
                                        <mml:mo fence="false" stretchy="false">‖</mml:mo>
                                    </mml:mrow>
                                    <mml:mn>2</mml:mn>
                                    <mml:mn>2</mml:mn>
                                </mml:msubsup>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mo stretchy="false">)</mml:mo>
                            </mml:math>
                        </equation>
                    </para>
                </footnote> der für ein konkretes Klassifikationsvorhaben experimentell zu bestimmen ist (vgl. <biblioref linkend="bib_Bur98"/>, S. 157):</para>
                <equation xml:id="form_RBF-Kernel_Weka"> <!-- RBF-Kernelfunktion -->
                    <mml:math>
                        <mml:mrow>
                            <mml:mi>K</mml:mi>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:mover accent="true">
                                <mml:msub>
                                    <mml:mi>x</mml:mi>
                                    <mml:mi>i</mml:mi>
                                </mml:msub>
                                <mml:mo stretchy="true">⃗</mml:mo>
                            </mml:mover>
                            <mml:mtext>&#160;,&#160;</mml:mtext>
                            <mml:mover accent="true">
                                <mml:msub>
                                    <mml:mi>x</mml:mi>
                                    <mml:mi>j</mml:mi>
                                </mml:msub>
                                <mml:mo stretchy="true">⃗</mml:mo>
                            </mml:mover>
                            <mml:mo stretchy="false">)</mml:mo>
                            <mml:mtext>&#160;</mml:mtext>
                            <mml:mo stretchy="false">=</mml:mo>
                            <mml:mtext>&#160;</mml:mtext>
                            <mml:msup>
                                <mml:mrow>
                                    <mml:mi>e</mml:mi>
                                    <mml:mtext>&#160;</mml:mtext>
                                </mml:mrow>
                                <mml:mrow>
                                    <mml:mo stretchy="false">-</mml:mo>
                                    <mml:mi>&#947;</mml:mi>
                                    <mml:mtext>&#160;</mml:mtext>
                                    <mml:mo>&#x22c5;</mml:mo>
                                    <mml:mtext>&#160;</mml:mtext>
                                    <mml:msup>
                                        <mml:mrow>
                                            <mml:mo fence="false" stretchy="false">‖</mml:mo>
                                            <mml:mover accent="true">
                                                <mml:msub>
                                                    <mml:mi>x</mml:mi>
                                                    <mml:mi>i</mml:mi>
                                                </mml:msub>
                                                <mml:mo stretchy="true">⃗</mml:mo>
                                            </mml:mover>
                                            <mml:mo fence="false" stretchy="false">-</mml:mo>
                                            <mml:mover accent="true">
                                                <mml:msub>
                                                    <mml:mi>x</mml:mi>
                                                    <mml:mi>j</mml:mi>
                                                </mml:msub>
                                                <mml:mo stretchy="true">⃗</mml:mo>
                                            </mml:mover>
                                            <mml:mo fence="false" stretchy="false">‖</mml:mo>
                                        </mml:mrow>
                                        <mml:mn>2</mml:mn>
                                    </mml:msup>
                                </mml:mrow>
                            </mml:msup>
                        </mml:mrow>
                    </mml:math>
                </equation>
                <para>Außerdem ist für alle SVMs unabhängig von der Linearität und der verwendeten Kernelfunktion ein Kostenparameter C<footnote>
                    <para>In der Dokumentation der Weka-Implementierung von SMO wird dieser Parameter auch C genannt, allerdings in der Dokumentation 
                        <emphasis>complexity parameter</emphasis> genannt, was weniger anschaulich ist, als die Assoziation des Buchstabens C mit
                        <emphasis>cost</emphasis>, wie es im Tutorium von Burges (<biblioref linkend="bib_Bur98"/>) geschieht.</para></footnote>
                    festzulegen. Je größer der Wert von C ist, desto stärker werden Trainingsinstanzen, welche auf der falschen Seite der trennenden
                    Hyperebene liegen, in der Trainingsphase „bestraft“ (<biblioref linkend="bib_Bur98"/>, S. 135). Es handelt sich hierbei also um 
                    einen Parameter im Zusammenhang mit der erwähnten <indexterm><primary>Schlupfvariable</primary></indexterm>Schlupfvariable.</para>
                <para>Für SMOs mit RBF-Kernel wurden verschiedene Kombinationen von C und &#947; ausprobiert, aus Ressourcengründen aber auch hier
                    nicht für ein vollständiges Parametergitter (ein vollständiger Durchlauf von Training und Evaluation mit RBF-Kernel dauerte auf 
                    vergleichbarer Hardware je nach Parametern zwischen ca. 4,5 und ca. 13 Stunden). Die erzielten Erfolgsquoten zeigt <xref linkend="tab_SMO_RBFparams"/>.</para>
                <table frame="all" orient="port" pgwide="1" tocentry="1" xml:id="tab_SMO_RBFparams"> <!-- Erfolgsquote von Trainings- und Testläufen mit SMO und RBF-Kernel für verschiedene Werte von C und &#947; -->
                    <info>
                        <title>Erfolgsquote von Trainings- und Testläufen mit SMO und RBF-Kernel für verschiedene Werte von C und &#947;</title>
                    </info>
                    <tgroup cols="10" align="left" colsep="1" rowsep="1">
                        <colspec colname="c-wert" colwidth="1*"/>
                        <colspec colname="gamma1" colwidth="1*"/>
                        <colspec colname="gamma2" colwidth="1*"/>
                        <colspec colname="gamma3" colwidth="1*"/>
                        <colspec colname="gamma4" colwidth="1*"/>
                        <colspec colname="gamma5" colwidth="1*"/>
                        <colspec colname="gamma6" colwidth="1*"/>
                        <colspec colname="gamma7" colwidth="1*"/>
                        <colspec colname="gamma8" colwidth="1*"/>
                        <colspec colname="gamma9" colwidth="1*"/>
                        <thead>
                            <row>
                                <entry></entry>
                                <entry>&#947; = 0,000001</entry>
                                <entry>&#947; = 0,00001</entry>
                                <entry>&#947; = 0,0001</entry>
                                <entry>&#947; = 0,001</entry>
                                <entry>&#947; = 0,01</entry>
                                <entry>&#947; = 0,1</entry>
                                <entry>&#947; = 1</entry>
                                <entry>&#947; = 10</entry>
                                <entry>&#947; = 100</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>C = 1</entry>
                                <entry>17,932%</entry>
                                <entry></entry>
                                <entry>30,323%</entry>
                                <entry>55,367%</entry>
                                <entry>21,684%</entry>
                                <entry>20,942%</entry>
                                <entry>20,506%</entry>
                                <entry></entry>
                                <entry></entry>
                            </row>
                            <row>
                                <entry>C = 10</entry>
                                <entry/>
                                <entry>30,803%</entry>
                                <entry/>
                                <entry/>
                                <entry>22,426%</entry>
                                <entry>20,942%</entry>
                                <entry/>
                                <entry/>
                                <entry/>
                            </row>
                            <row>
                                <entry>C = 100</entry>
                                <entry/>
                                <entry/>
                                <entry>66,623%</entry>
                                <entry/>
                                <entry>22,382%</entry>
                                <entry/>
                                <entry>20,550%</entry>
                                <entry>20,375%</entry>
                                <entry>20,201%</entry>
                            </row>
                            <row>
                                <entry>C = 1000</entry>
                                <entry/>
                                <entry/>
                                <entry>66,623%</entry>
                                <entry>64,323%</entry>
                                <entry>22,613%</entry>
                                <entry/>
                                <entry/>
                                <entry>20,399%</entry>
                                <entry/>
                            </row>
                            <row>
                                <entry>C = 10000</entry>
                                <entry/>
                                <entry>66,580%</entry>
                                <entry/>
                                <entry/>
                                <entry/>
                                <entry>20,942%</entry>
                                <entry/>
                                <entry/>
                                <entry>20,226%</entry>
                            </row>
                            <row>
                                <entry>C = 100000</entry>
                                <entry>66,580%</entry>
                                <entry/>
                                <entry/>
                                <entry/>
                                <entry/>
                                <entry/>
                                <entry>20,573%</entry>
                                <entry/>
                                <entry/>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <para>Es ist somit ersichtlich, dass die besten RBF-Kernel-basierten Klassifikatoren mit hohen C-Werten und sehr kleinen &#947;-Werten
                    zu erzielen sind. Es zeigt sich außerdem, dass die Erfolgsquote von keinem dieser Klassifikatoren den Referenzwert des linearen
                    SMO-Klassifikators übertraf, sondern lediglich erreichte (bei C = 10000 und &#947; = 0,0001). Die Trainingsdauer lag wie bereits erwähnt
                    bei allen Versuchen mit RBF-Kernel im Bereich mehrerer Stunden, bei linearen SMO hingegen bei ca. 11 Minuten, so dass hier eine 
                    klare Präferenz für das einfachere, lineare Verfahren abgeleitet werden kann. Unser Lernproblem scheint aus annähernd linear trennbaren
                    Teilproblemen zu bestehen. Es wurde keine nicht-lineare SVM gefunden, die eine bessere Erfolgsquote erzielt als eine lineare SVM.</para>
            </sect2>
            <sect2 xml:id="ch_SMO_improve_ensemble"> <!-- SMO: Ensemble Learning -->
                <title>Ensemble Learning</title>
                <para>Die <indexterm><primary>Ensemble Learning</primary></indexterm>Ensemble-Learning-Verfahren <indexterm><primary>Bagging</primary>
                </indexterm>Bagging und AdaBoost wurden in <xref linkend="ch_NB_improve_ensemble"/> bereits beschrieben. Sie lassen
                    sich mit SMO genauso einsetzen wie mit Naïve Bayes. Die Effekte sind jedoch vergleichbar: <indexterm><primary>AdaBoost</primary>
                    </indexterm>AdaBoost bricht im Standardverfahren (mit Neugewichtung
                    der Trainingsinstanzen in Folgeiterationen) vor Erreichen der zehnten Iteration ab und erreicht dieselbe Erfolgsquote wie der Basislerner
                    ohne <indexterm><primary>Boosting</primary></indexterm>Boosting. Die Erfolgsquote nach Bagging sank um ca. drei Prozentpunkte gegenüber dem Basislerner. Beide Ensemble-Learning-Verfahren 
                    benötigten im Gegenzug natürlich sehr viel mehr Zeit als der Basislerner: AdaBoost lief auf derselben Hardware wie der Basislerner ca.
                    12 Stunden länger (Training und Evaluation), die Dauer beim Bagging mit 10 Iterationen lag etwa beim 12-fachen des Basislerners.</para>
                <para>Dass unsere beiden untersuchten Lernverfahren Naïve Bayes und SMO beide wenig bis gar nicht vom Ensemble Learning profitieren, bestärkt
                    die Vermutung, dass die Basislerner bereits „zu stark“ sind und somit zu wenig Varianz entsteht, von der das Ensemble Learning profitieren
                    könnte. Andererseits ist uns aus der Evaluation der beiden Basislerner bekannt, dass schlecht gelernte Klassen oft über viel zu wenige
                    Trainingsinstanzen verfügen. Wenn AdaBoost in Folgeiterationen genau auf diese Warengruppenklassen fokussiert, wird dieser Mangel besonders 
                    ins Gewicht fallen. Weitere Versuche insbesondere mit AdaBoost könnten interessant sein, wenn für kleine Klassen mehr Trainings-E-Books 
                    beschafft werden können.</para>
            </sect2>
            <sect2 xml:id="ch_SMO_improve_attributes"> <!-- SMO: Optimierungspotential in der Datenaufbereitung -->
                <title>Optimierungspotential in der Datenaufbereitung</title>
                <para>Die Optimierungsversuche in der Datenaufbereitung, welche in <xref linkend="ch_NB_improve_attributes"/> bereits beschrieben wurden,
                    fanden auch in unseren Untersuchungen zu SMO Anwendung. Die Beschreibung der Datenänderungen werden hier nicht wiederholt, sondern nur
                    kurz in einem Satz erneut angerissen, bevor auf die Ergebnisse dieser Versuche und deren Interpretation im SMO-Kontext eingegangen wird.</para>
                <sect3 xml:id="ch_SMO_improve_cv"> <!-- Kontrolliertes Vokabular -->
                    <title>Hinzufügen eines kontrollierten Vokabulars zum Wortvektor</title>
                    <para>Das kontrollierte Vokabular für die VLB-Warengruppen sorgte nur für einen sehr geringen Zuwachs bei der Erfolgsquote (von 66,62% auf 
                        66,71%), aber immerhin nicht für Absinken, wie bei Naïve Bayes, denn SMO ist weniger empfindlich gegenüber Abhängigkeitsbeziehungen
                        im Attributvektor. Besonders beeindruckend ist dieser Anstieg andererseits jedoch nicht. Man könnte auf den Einsatz dieser Maßnahme
                        gut verzichten, sie schadet allerdings auch nicht, da die Berechnungszeit des kontrollierten Vokabulars insgesamt nicht ins Gewicht fällt.
                        Interessanterweise profitiert SMO allerdings vom Entfernen von Duplikatwörtern, welche einerseits über den TF-IDF-basierten Wortvektor und andererseits
                        über das kontrollierte Vokabular in den Attributvektur gelangt sind. Nach Entfernen dieser ca. 70 Einheiten stieg die Erfolgsquote des Klassifikators
                        noch einmal um 0,22 Prozentpunkte auf 66,93%. Eine Übersicht über die verschiedenen Varianten gibt <xref linkend="tab_SMO_mit_CV"/></para>
                    <table frame="all" xml:id="tab_SMO_mit_CV"><!-- Ergebnisse SMO mit kontrolliertem Vokabular -->
                        <title>Ergebnisse SMO mit kontrolliertem Vokabular</title>
                        <tgroup cols="5">
                            <colspec colname="Nummer" colnum="1" colwidth="1*" align="left"/>
                            <colspec colnum="2" colwidth="2*" align="left" colname="Verfahren"/>
                            <colspec colnum="3" colwidth="1*" align="left" colname="Attributanzahl"/>
                            <colspec colname="Laufzeit" colnum="4" colwidth="2*" align="left"/>
                            <colspec colnum="5" colwidth="1*" align="left" colname="Erfolgsquote"/>
                            <thead>
                                <row>
                                    <entry>Nummer</entry>
                                    <entry>Verfahren</entry>
                                    <entry>Attributanzahl</entry>
                                    <entry>Laufzeit</entry>
                                    <entry>Erfolgsquote</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>28a</entry>
                                    <entry>SMO ohne Optimierungen</entry>
                                    <entry>16270</entry>
                                    <entry>3m 50s (Training), 7m (Evaluation)</entry>
                                    <entry>66,62%</entry>
                                </row>
                                <row>
                                    <entry>29a</entry>
                                    <entry>SMO mit kontrolliertem Vokabular</entry>
                                    <entry>16867</entry>
                                    <entry>3m 54s (Training), 14m 47s (Evaluation)</entry>
                                    <entry>66,71%</entry>
                                </row>
                                <row>
                                    <entry>29b</entry>
                                    <entry>SMO mit kontrolliertem Vokabular und entfernten Attribut-Duplikaten</entry>
                                    <entry>16798</entry>
                                    <entry>4m 3s (Training), 14m 56s (Evaluation)</entry>
                                    <entry>66,93%</entry>
                                </row>
                                <row>
                                    <entry>30</entry>
                                    <entry>SMO mit kontrolliertem Vokabular, aber ohne Wortvektor</entry>
                                    <entry>553</entry>
                                    <entry>37s (Training), 13s (Evaluation)</entry>
                                    <entry>46,70%</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </sect3>
                <sect3 xml:id="ch_SMO_improve_fw"><!-- Ausschließen von Wörtern aus Fremdsprachen -->
                    <title>Ausschließen von Wörtern aus Fremdsprachen</title>
                    <para><indexterm><primary>Fremdsprachen</primary><secondary>Indexierung von Wörtern in</secondary>
                        </indexterm>Unsere lineare SMO-Supportvektormaschine reagierte auf das Auslassen von Wörtern aus Fremdsprachen genau umgekehrt wie Naïve Bayes: Die Laufzeit
                        der Trainings- und Evaluationsphase stieg auf etwa das 1,5-fache an und die Erfolgsquote sank um 0,17 Prozentpunkte von 66,62% auf 66,45%. Der 
                        Informationsverlust durch die geringere Anzahl von Fremdwörtern wirkt sich somit leicht negativ auf SMO aus.</para>
                </sect3>
                <sect3 xml:id="ch_SMO_improve_nolig"><!-- Auflösen von Ligaturen -->
                    <title>Auflösen von Ligaturen</title>
                    <para>Das Bereinigen von rein typografisch, nicht orthografisch bedingten Ligaturen, hatte keinerlei Effekt auf die Qualität des gelernten
                        SMO-Klassifikators. Die Evaluationsphase dauerte etwas länger als bei den Referenzdatensätzen ohne Ligaturenauflösung, vermutlich wegen des
                        geringfügig größeren Attributvektors, aber die Erfolgsquote betrug unveränderte 66,62%.</para>
                    <para>Die unveränderte Erfolgsquote bedeutet jedoch nicht, dass die beiden Klassifikatoren identisch wären. Einzelne Warengruppen wurden
                        besser erkannt, z.&#160;B. die historischen Romane (WG 113), bei denen die TP-Rate 0,438 auf 0,469 und das F-Maß von 0,509 auf 0,536 stieg. Andere 
                        Warengruppen sanken dann entsprechend in der Klassifikationsgenauigkeit, z.&#160;B. die Science Fiction (WG 131), deren Genauigkeit
                        (Precision) zwar in beiden Fällen beim Höchstwert 1,0 lag, deren Trefferquote (Recall) jedoch aufgrund einer größeren Zahl von 
                        <emphasis>False Negatives</emphasis> von 0,571 auf 0,429 sank. Möchte man den Klassifikator also auf die korrekte Behandlung bestimmter 
                        Warengruppen hin optimieren, kann sich ein genauerer Blick auf die Unterschiede lohnen.</para>
                </sect3>
                <sect3 xml:id="ch_SMO_improve_urlnorm"><!-- URL-Normalisierung -->
                    <title>Entfernen von URLs</title>
                    <para>Beim Entfernen von URLs aus den Korpustexten scheint Information verloren zu gehen, welche SMO zum Trennen von Klassengrenzen dienen kann.
                        Bei gleichbleibender Trainings- und Evaluationsdauer sinkt die Erfolgsquote leicht um 0,04 Prozentpunkt auf 66,58%. Die durchschnittliche 
                        Genauigkeit und Trefferquote der Minderheitenklassen (micro-averaged) sinken jeweils leicht, die Mehrheitsklasse (Warengruppe 112) kann 
                        hingegen zumindest bei der Trefferquote (Recall) etwas zulegen (von 0,915 auf 0,918), während die Genauigkeit von 0,679 auf 0,676 sinkt. Das Entfernen
                        der URLs führte also dazu, dass der SMO-Klassifikator stärker dazu neigte, ein E-Book als der Mehrheitsklasse zugehörig zu klassifizieren.</para>
                </sect3>
                <sect3 xml:id="ch_SMO_improve_hyeronym"> <!-- Verwendung von Oberbegriffen -->
                    <title>Verwendung von Oberbegriffen (Hyperonymen)</title>
                    <para>Während die Verbesserung der Erfolgsquote durch Hinzuziehung von <indexterm><primary>Hyperonym</primary></indexterm>Hyperonymen bei Naïve Bayes sehr moderat ausfiel, kann SMO deutlich besser
                        von dieser Maßnahme profitieren. Die Erfolgsquote liegt mit 67,80% mehr als einen Prozentpunkt über dem Referenztraining ohne Hyperonyme.
                        Interessant ist zudem, dass bei Entfernung des TF-IDF-basierten Wortvektors und alleinigem Training mit den Standardattributen und den 
                        Oberbegriffen mit SMO eine Qualität erreicht wird, die derjenigen von Naïve Bayes mit vollem Wortvektor entspricht. Die Berechnung der 
                        Hyperonyme war zwar in der Datenaufbereitungsphase sehr aufwändig, die Effekte auf das Training einer Supportvektormaschine sind jedoch 
                        deutlich positiv, so dass diese Maßnahme bei ausreichenden Zeitressourcen empfehlenswert erscheint. Den Vergleich zwischen Naïve Bayes und
                        SMO in Bezug auf die Verwendung von Hyperonymen stellt die folgende <xref linkend="tab_SMO_mit_Hypernonymen"/> im Überblick dar.</para>
                    <table frame="all" xml:id="tab_SMO_mit_Hypernonymen"><!-- Ergebnisse SMO mit Hyperonymen -->
                        <title>Vergleich von Naïve Bayes und SMO in Bezug auf Hyperonymverwendung</title>                            
                        <tgroup cols="5">
                            <colspec colname="Nummer" colnum="1" colwidth="1*" align="left"/>
                            <colspec colnum="2" colwidth="2*" align="left" colname="Verfahren"/>
                            <colspec colnum="3" colwidth="1*" align="left" colname="Attributanzahl"/>
                            <colspec colname="Laufzeit" colnum="4" colwidth="2*" align="left"/>
                            <colspec colnum="5" colwidth="1*" align="left" colname="Erfolgsquote"/>
                            <thead>
                                <row>
                                    <entry>Nummer</entry>
                                    <entry>Verfahren</entry>
                                    <entry>Attributanzahl</entry>
                                    <entry>Laufzeit</entry>
                                    <entry>Erfolgsquote</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>35b</entry>
                                    <entry>SMO mit Hyperonymen, aber ohne TF-IDF-Wortvektor</entry>
                                    <entry>2165</entry>
                                    <entry>2m 48s (Training), 3m 49s (Evaluation)</entry>
                                    <entry>59,29%</entry>
                                </row>
                                <row>
                                    <entry>28a</entry>
                                    <entry>Naïve Bayes ohne Optimierungen</entry>
                                    <entry>16270</entry>
                                    <entry>53s (Training), 18m 16s (Evaluation)</entry>
                                    <entry>59,42%</entry>
                                </row>
                                <row>
                                    <entry>28a</entry>
                                    <entry>SMO ohne Optimierungen</entry>
                                    <entry>16270</entry>
                                    <entry>3m 50s (Training), 7m (Evaluation)</entry>
                                    <entry>66,62%</entry>
                                </row>
                                <row>
                                    <entry>35a</entry>
                                    <entry>SMO mit TF-IDF-Wortvektor und Hyperonymen</entry>
                                    <entry>25912</entry>
                                    <entry>6m 8s (Training), 10m 52s (Evaluation)</entry>
                                    <entry>67,80%</entry>
                                </row>
                            </tbody>
                        </tgroup>
                        <caption><para>Versuche werden aufsteigend nach Erfolgsquote sortiert angezeigt</para></caption>
                    </table>
                </sect3>
                <sect3 xml:id="ch_SMO_improve_pclem_corrections"><!-- POS-Tagging- und Lemmakorrektor -->
                    <title>Korrektur der Wortartenerkennung und der Lemmatisierung</title>
                    <para>Die Effekte der Korrekturen von POS-Tags und Lemmata, wie sie der TreeTagger und Apache OpenNLP ausgeben, sind bei SMO kaum merklich, 
                        also noch geringer als bei Naïve Bayes. Es lässt sich aber auch hier beobachten, dass die reine Wortartenkorrektur einen besseren Effekt erzielt als
                        die kombinierte Korrektur von Wortarten und Lemmata. Der Umfang der manuellen Korrekturlisten ist wohl im Vergleich zu der Gesamtgröße des
                        Lucene-Indexes, dem sie vorgeschaltet sind, zu gering, um merkliche Effekte zu erzielen. Es lässt sich also hieraus nichts ableiten oder schließen,
                        insbesondere nicht, dass bessere Wortarten-Tags oder saubere Lemmata nicht zu einer Verbesserung der Klassifikation führen können.</para>
                    <table frame="all" xml:id="tab_SMO_mit_POS_Lem_Korrekturen"><!-- Ergebnisse SMO mit Korrekturen von POS-Tags und Lemmata -->
                        <?dbfo keep-together="always"?>
                        <title>Ergebnisse SMO mit Korrekturen von POS-Tags und Lemmata</title>
                        <tgroup cols="5">
                            <colspec colname="Nummer" colnum="1" colwidth="1*" align="left"/>
                            <colspec colnum="2" colwidth="2*" align="left" colname="Verfahren"/>
                            <colspec colnum="3" colwidth="1*" align="left"
                                colname="Attributanzahl"/>
                            <colspec colname="Laufzeit" colnum="4" colwidth="2*" align="left"/>
                            <colspec colnum="5" colwidth="1*" align="left"
                                colname="Erfolgsquote"/>
                            <thead>
                                <row>
                                    <entry>Nummer</entry>
                                    <entry>Verfahren</entry>
                                    <entry>Attributanzahl</entry>
                                    <entry>Laufzeit</entry>
                                    <entry>Erfolgsquote</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>28a</entry>
                                    <entry>SMO ohne Optimierungen</entry>
                                    <entry>16.270</entry>
                                    <entry>3m 50s (Training), 7m (Evaluation)</entry>
                                    <entry>66,62</entry>
                                </row>
                                <row>
                                    <entry>36</entry>
                                    <entry>SMO mit korrigierten POS-Tags</entry>
                                    <entry>16.358</entry>
                                    <entry>3m 53s (Training), 13m 58s (Evaluation)</entry>
                                    <entry>66,71%</entry>
                                </row>
                                <row>
                                    <entry>37</entry>
                                    <entry>SMO mit korrigierten POS-Tags und Lemmata</entry>
                                    <entry>16.326</entry>
                                    <entry>3m 54s (Training), 14m 26s (Evaluation)</entry>
                                    <entry>66,67%</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </sect3>
            </sect2>
            <sect2 xml:id="ch_SMO_improve_überblick"><!-- Überblick und Fazit Optimierungsversuche SMO -->
                <title>Überblick und Fazit</title>
                <para>Die Einzelmaßnahme, welche für SMO die größte Verbesserung zeitigte, war die Zugabe von Oberbegriffen zum Attributvektor, gefolgt von 
                   der Verwendung eines bereinigten kontrollierten Vokabulars ohne Wortvektor-Duplikate. Interessant war schließlich noch, ob sich einige dieser
                   Effekte bei gemeinsamer Anwendung addieren. Eine SVM, deren Trainingsdaten mit allen in <xref linkend="ch_SMO_improve_attributes"/> genannten
                   Optimierungsversuchen vorverarbeitet wurden, erreichte eine Erfolgsquote von 67,45%, was etwas schlechter ist als der nur mit Oberbegriffen 
                   angereicherte SMO-Klassifikator, aber besser als alle anderen Einzeloptimierungen.</para>
                <para>Weder mit Ensemble-Learnern noch mit Kernelfunktionen, welche die Lerndaten in eine höhere Dimension transformieren, konnte die lineare
                   Supportvektormaschine übertroffen werden. Allerdings wurden alle Optimierungsversuche auf Ebene der Eingabedaten ausschließlich mit 
                   der linearen Variante getestet, so dass eventuell in der Kombination „Kernelfunktion + Eingabeoptimierung“ schlummerndes Potential nicht
                   entdeckt werden konnte.</para>
                <!--<para>[TO DO: evtl. abschließend noch Tabelle mit allen SMO-Läufen, inkl. einiger Micro- oder Macro-Averaged Daten]</para>--> 
            </sect2>
        </sect1>
    </chapter>
    <chapter xml:id="ch_ThemaMeka"><!-- THEMA-Klassifikation (Multiklassenklassifikation mit MEKA) -->
        <info>
            <title>Thema-Klassifikation (Multiklassenklassifikation mit MEKA)</title>
        </info>
        <para><indexterm class="startofrange" xml:id="idt_018"><primary>Thema-Klassifikation</primary></indexterm>Das in <xref linkend="ch_Intro_Thema"/> vorgestellte Klassifikationsschema ist in mehrerlei Hinsicht interessanter als es die VLB-Warengruppen
            sind. Die strikte Entscheidung für genau eine Warengruppe stellt hier kein Problem dar, da beliebig viele passende Klassifikationscodes pro Buch vergeben
            werden können. Ein Krimi für Jugendliche erhält dann sowohl einen Krimi-Code als auch einen Jugendbuchcode.</para>
        <para>Andererseits stellte uns der Wunsch, auch ein Maschinenlernmodell für die Thema-Klassifikation zu erarbeiten, vor zwei neue praktische Probleme: Zunächst
            war es in der Zusammenstellung eines <indexterm><primary>Korpus</primary></indexterm>Korpus nicht mehr so leicht möglich, repräsentative Trainings- und Testmengen zu erstellen, in
            denen die einzelnen Thema-Codes statistisch gleichverteilt sind, da die Beziehung „ein E-Book, eine Klasse“ nicht mehr galt, und da es nur in seltenen 
            Ausnahmefällen vorkam, dass mehrere E-Books dieselben Klassifikationen hatten. Die meisten E-Books waren „individuell klassifiziert“, d.h. die 
            spezifische Kombination von Thema-Codes kam nur einmal im gesamten Korpus vor.</para>
        <para>Zum zweiten stellt die von uns für die VLB-Warengruppenklassifikation verwendete Weka-Bibliothek keine Verfahren für die <glossterm
            linkend="gt_multilabel_klassifikation">Multilabel-Klassifikation</glossterm> bereit. Es gibt jedoch eine Weka-Erweiterung namens MEKA<footnote>
                <para><link xlink:href="http://meka.sourceforge.net/"/> (Zugriff 27.3.2018).</para></footnote>, welche die zusätzlich benötigten 
            Algorithmen anbietet <biblioref linkend="bib_RRPH16"/>.</para>
        <para>Kein neues Problem, aber eines, welches auch das Thema-Training weiter befrachtet, ist eine inkonsistente Klassifikation der Trainings- 
            und Testdaten im Korpus. Neben Abgrenzungsproblemen zwischen einzelnen Klassen und individuellen Präferenzen für die Vergabe eher allgemeinerer
            Oberklassen oder spezifischerer Klassen aus den Blättern des hierarchischen Klassenbaums, kommt es aber auch hier zu einer Weiterung der
            Problematik, welche beim VLB-Warengruppentraining nicht bestand: Die Anzahl der vergebenen Thema-Codes pro E-Book schwankt stark zwischen 1 und 21.
            Teilweise ist diese Schwankung sicherlich inhaltlich begründet, zu einem nicht leicht zu ermittelnden anderen Teil aber wohl auch dem 
            Personalstil des jeweiligen Indexers geschuldet. In vielen Fällen verstoßen die vorklassifizierten Daten in unserem Korpus auch ganz deutlich
            gegen die in <biblioref linkend="bib_Thema13"/> festgelegten Klassifikationsprinzipien, z.&#160;B. dass keine zusätzlichen Elterncodes verwendet
            werden sollen, wenn spezielle Codes vergeben wurden, also z.&#160;B. nicht Code <emphasis>1H</emphasis> (Afrika) für ein Buch über Äthiopien mit 
            Code <emphasis>1HFGA</emphasis> beizufügen<footnote><para>Wobei es meines Erachtens durchaus auch Fälle geben kann, wo es gerechtfertigt wäre,
                einen Eltern- und einen Kindknoten gemeinsam für ein Buch zu verwenden. Um bei dem genannten Beispiel zu bleiben: Ein Buch über die Rolle 
                Äthiopiens im Pan-Afrikanischen Kongress würde zu Recht sowohl mit <emphasis>1H</emphasis> als auch <emphasis>1HFGA</emphasis> versehen werden können.
            </para></footnote>.</para> 
        <para>Aus dieser Tatsache heraus lässt sich für die Evaluation erwarten, dass die üblichen Qualitätsmetriken
            keine allzu hohen Werte aufweisen werden, denn viele sparsam vorklassifizierte Testtitel werden vom gelernten Klassifikator voraussichtlich mit 
            mehr Klassen versehen, welche durchaus korrekt sein können, aber vom Erwartungswert abweichen. Umgekehrt gilt für umfangreich vorklassifizierte
            Testtitel, dass der Klassifikator voraussichtlich weniger Klassen zuweist und die Abweichung ebenfalls als Fehler gewertet wird<footnote><para>Die
                Erwartung, dass die Multilabel-Klassifikation schlechtere statistische Resultate ergibt als die Singlelabel-Klassifikation deckt sich auch mit den
                Erfahrungen von <biblioref linkend="bib_LH03"/>, S. 148.</para></footnote>.</para>
        <para>Es standen im Rahmen der vorliegenden Arbeit keine ausreichenden Ressourcen zur Verfügung, um diese genannten Probleme systematisch anzugehen, zu
            untersuchen und Lösungsstrategien zu entwickeln. Anders als die vorangehenden Kapitel zur VLB-Warengruppenklassifikation hat dieses Kapitel
            deshalb einen unsystematischeren Charakter und gibt eher einen skizzenhaften Ausblick als eine umfassende Untersuchung der automatischen
            Thema-Klassifikation.</para>
        <sect1 xml:id="ch_ThemaMeka_process"> <!-- Thema / MEKA: Darstellung des Verfahrens -->
            <title>Darstellung des Verfahrens</title>
            <para><indexterm class="startofrange" xml:id="idt_009"><primary>Multilabel-Lernproblem</primary></indexterm>Ein Multilabel-Lernproblem kann einfach
                in eine Folge von mehreren <indexterm><primary>binäre Klassifikation</primary></indexterm>binären Klassifikationsproblemen überführt werden.
                Das <emphasis>Binary Relevance</emphasis>-Verfahren <indexterm class="startofrange" xml:id="idt_013">
                    <primary>Binary Relevance</primary></indexterm> trainiert so viele Maschinenmodelle, wie es Klassen gibt, für jede Klasse ein Modell. Jedes
                individuelle Modell ist dann dafür zuständig zu entscheiden, ob ein zu klassifizierendes Dokument zu dieser Klasse gehört oder nicht. Diese 
                Entscheidung erfolgt probabilistisch.</para>
            <para>Da wir bisher für unsere E-Book-Klassifikation die besten Erfahrungen mit SMO gemacht haben und da auch <biblioref linkend="bib_LH03"/>
                angeben, gute Erfahrungen mit binären Supportvektormaschinen in der Multiklassen-Textklassifikation gemacht zu haben, wurde zunächst genau dieses
                Setup auch für die Thema-Klassifikation verwendet. Ein Vergleich zu Naïve Bayes bot sich schließlich aber auch hier an.</para>
            <para>Bei der Zusammenstellung der Dokumente für Trainings- und Testkorpus ergab sich, dass sehr viele Thema-Klassen eine sehr geringe Frequenz aufwiesen.
                Die folgende <xref linkend="tab_thema-statistik"/> zeigt einige statistische Daten, welche sich in einem zufällig ausgewählten Subset des Korpus ergab
                und vor allem zeigt, wie viele einmalige Klassen im Korpus vorkommen, d. h. Klassen, für die jeweils nur ein einzelnes Dokument vorliegt.</para>
            <para>Um nicht zu viel manuellen Aufwand in das Vorbereiten zu lernender Thema-Klassen zu stecken, wurde mit Hilfe der Meka- und Weka-Bibliotheken 
                ein Java-Programm <code>ThemaTrainer</code> in Avve integriert, welches das Filtern von zu lernenden Thema-Klassen algorithmisch durchführt. Dieses
                Programm führt dazu folgende Schritte aus:</para>
            <orderedlist spacing="compact">
                <listitem>
                    <para>Zwei <indexterm><primary>ARFF</primary></indexterm>ARFF-Dateien werden erwartet und geladen: Eine enthält die Attributvektoren der Trainingsmenge, die andere diejenige der Testmenge</para>
                </listitem>
                <listitem>
                    <para>Alle Thema-Klassenattribute, welche entweder nur in der Trainingsmenge oder nur in der Testmenge vorkommen, werden entfernt. Nach diesem
                        Schritt enthalten also Trainings- und Testmenge dasselbe Set an Klassenattributen</para>
                </listitem>
                <listitem>
                    <para>Alle Thema-Klassenattribute unterhalb eines Schwellenparameters werden entfernt. Genaugenommen arbeitet <code>ThemaTrainer</code> mit
                        zwei Schwellenparametern, jeweils einem für Trainings- und Testmenge. Wird einer der beiden Schwellen unterschritten, so wird die Thema-Klasse
                        aus dem Klassenvektor entfernt.</para>
                </listitem>
                <listitem>
                    <para><code>ThemaTrainer</code> übernimmt dann die Aufgaben, welche für die VLB-Warengruppen-Vorverarbeitung mit Hilfe von Wekas Knowledge-Flow-Modul
                        erledigt wurde (siehe <xref linkend="ch_Avvelauf"/>): Die Trainings- und Testdaten werden zusammengeführt, ein gemeinsamer Wortvektor wird erstellt,
                        dann werden die Datensätze wieder getrennt. Dies dient dazu sicherzustellen, dass sowohl die Trainings- als auch die Testdaten identische 
                        Wortvektoren aufweisen.</para>
                </listitem>
                <listitem>
                    <para>Die so vorbereiteten Daten werden zu Referenzzwecken im ARFF-Format abgespeichert.</para>
                </listitem>
                <listitem>
                    <para>Mit Hilfe der <code>BR</code>-Klasse von Meka, welche das <emphasis>Binary-Relevance</emphasis>-Verfahren <indexterm>
                        <primary>Binary Relevance</primary></indexterm>implementiert, wird ein Klassifikator trainiert und auf Basis der Testmenge
                        evaluiert.</para>
                </listitem>
            </orderedlist>
            <table frame="all" xml:id="tab_thema-statistik"> <!-- Multilabel-Statistik im verwendeten Teilkorpus -->
                <title>Multilabel-Statistik im verwendeten Teilkorpus</title>
                <tgroup cols="3">
                    <colspec colname="Eigenschaft" colnum="1" colwidth="3*"/>
                    <colspec colname="Train" colnum="2" colwidth="1*"/>
                    <colspec colname="Test" colnum="3" colwidth="1*"/>
                    <thead>
                        <row>
                            <entry>Eigenschaft</entry>
                            <entry>Trainingsmenge</entry>
                            <entry>Testmenge</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>Anzahl E-Books</entry>
                            <entry>1050</entry>
                            <entry>205</entry>
                        </row>
                        <row>
                            <entry>Anzahl Thema-Klassen</entry>
                            <entry>808</entry>
                            <entry>405</entry>
                        </row>
                        <!-- Ermittelt mit Xquery:
                           for $i in 1 to 799 (tatsächliche Anzahl der Klassen einsetzen)
                           let $sum := sum(//instances/instance/value[$i])
                           where $sum eq 1
                           return concat("Einzelklasse: ", $i)
                        -->
                        <row>
                            <entry>Anzahl der Thema-Klassen mit nur einem Dokument im Korpus</entry>
                            <entry>355</entry>
                            <entry>253</entry>
                        </row>
                        <!-- Ermittelt mit Xquery:
                           let $overall := sum(
                              for $i in 1 to count(//instances/instance)
                              let $sum := sum(//instances/instance[$i]/value[position() lt 800])
                              return $sum
                           )
                           return $overall div count(//instances/instance)
                        -->
                        <row>
                            <entry>Ø Thema-Klassen pro E-Book</entry>
                            <entry>4,28</entry>
                            <entry>5,54</entry>
                        </row>
                        <row>
                            <entry>Anzahl der Thema-Klassen, welche sowohl in Trainings- als auch in Testmenge vorkommen</entry>
                            <entry colsep="0">296</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>Anzahl der Thema-Klassen, welche in der Trainingsmenge mindestens 10x und in der Testmenge mindestens 2x vorkommen</entry>
                            <entry colsep="0">78</entry>
                            <entry></entry>
                        </row>
                        <row>
                            <entry>Anzahl der Thema-Klassen, welche in der Trainingsmenge mindestens 50x und in der Testmenge mindestens 10x vorkommen</entry>
                            <entry colsep="0">4</entry>
                            <entry></entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
        
            <para>Aus Zeitgründen wurde auf die Berechnung von <indexterm><primary>Hyperonym</primary></indexterm>Hyperonymen zu den Trainingsdaten verzichtet,
                <code>ThemaTrainer</code> würde diese aber
                entsprechend der Hyperonyme bei der VLB-Warengruppenklassifikation verarbeiten, sofern sie in den Korpusdaten enthalten sind. Auch wurden 
                keine umfangreichen Experimente zur Verwendung verschiedener Algorithmen, Parameter oder Datenvorverarbeitungsstrategien angestellt.<indexterm class="endofrange" startref="idt_009"/></para>
            <para>Das Lernen der Thema-Klassifikation erfolgte einmal mit den <code>ThemaTrainer</code>-Schwellenwerten für Klassen, welche mindestens 10-mal
                in der Trainings- und 2-mal in der Testmenge vorkommen (Lauf <emphasis>Thema_10</emphasis>) und ein zweites Mal mit Klassenschwellwerten von 50
                Trainingsdatensätzen und 10 Testdatensätzen (Lauf <emphasis>Thema_50</emphasis>). Es wurde das <emphasis>Binary-Relevance</emphasis>-Verfahren
                jeweils mit einem Naïve-Bayes-Klassifikator und einem SMO-Klassifikator eingesetzt. Die statistische
                Verteilung der Thema-Codes im Korpus <emphasis>Thema_10</emphasis> zeigt <xref linkend="app_WG_dist"/>.</para>
        </sect1>
        <sect1> <!-- Darstellung der Ergebnisse -->
            <title>Darstellung der Ergebnisse</title>
            <para>Wir werden auch bei der Auswertung der Thema-Ergebnisse bei der Klassifikation mit MEKA wieder die bekannten Qualitätsmaße aus <xref linkend="ch_Evaluation_Kennzahlen"/>
                verwenden. Zusätzlich gibt es Qualitätsmaße, welche besonders im Bereich des Multilabel-Lernens häufig eingesetzt werden. Als Maß für <emphasis>Binary Relevance</emphasis>
                eignet sich <indexterm><primary>Hamming Loss</primary></indexterm><emphasis>Hamming Loss</emphasis><footnote><para>Vgl. Vortragsfolien von Jesse Read,
                    einem der MEKA-Entwickler: <biblioref linkend="bib_Rea15"/>.</para></footnote>. Dabei werden die erwarteten 
                und die vom Klassifikator ausgegebenen Klassenvektoren, welche beide jeweils nur die Bits 1 (Testinstanz gehört zur Klasse) und 0 (gehört nicht 
                zur Klasse) enthalten, bitweise miteinander verglichen und die Zahl der Abweichungen addiert. Der so ermittelte Wert gibt die <indexterm>
                    <primary>Hammingdistanz</primary></indexterm>Hammingdistanz der beiden Vektoren an. Die Hammingdistanz wird durch Division durch die Anzahl der
                verglichenen Positionen normalisiert und ergibt so das <emphasis>Hamming-Loss</emphasis>-Maß.</para>
            <para>Betrachten wir als Beispiel einen binären Klassenvektor mit vier Positionen für die Thema-Kategorien (unter Verwendung einer vereinfachten
                Arrayschreibweise): [1DFG, FBA, FRD, FYT]. Unser vorklassifizierter Testdatensatz sei mit y = [0, 1, 0, 0] klassifiziert, d. h. er gehört zur Klasse FBA und 
                nicht zu 1DFG, FRD und FYT. Unser Klassifikator gibt als Schätzwert für die Klassenzugehörigkeit aus: ŷ = [1, 0, 0, 0]. Die Hammingdistanz zwischen y und ŷ 
                beträgt 2, da die Vektoren an zwei Stellen voneinander abweichen. Hamming Loss beträgt dann 2 / 4 = 0,5. Für eine gesamte Evaluation wird diese Berechnung
                für alle Testdatensätze gemeinsam durchgeführt. Es sei hd(y, ŷ) die Hammingdistanz zwischen zwei Klassenvektoren, t die Anzahl der Testdatensätze und n 
                die Anzahl der Klassen, dann errechnet sich Hamming Loss für die gesamte Testevaluation durch</para>
            <equation xml:id="formula_hamming_loss"> <!-- Hamming Loss -->
                <mml:math><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>tn</mml:mi>
                </mml:mfrac></mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn>
                </mml:mrow><mml:mi>t</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mi>hd</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub>
                    <mml:mtext>&#160;,&#160;</mml:mtext><mml:msub><mml:mi>ŷ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math>
            </equation>
            <para>Wie der Name Hamming Loss andeutet, sind kleinere Werte besser, denn sie bedeuten ja durchschnittlich weniger Abweichungen zwischen erwarteten
                und gelieferten Klassen.</para>
            <para>Evaluationskriterien machen natürlich nur Sinn, wenn man Vergleichswerte hat. Wir wollen Hamming Loss und die bereits aus den vorhergehenden Kapiteln 
                bekannten Maße verwenden, um vier Testläufe der Thema-Klassifikation gegenüberzustellen, und zwar <emphasis>Thema_10</emphasis> und <emphasis>Thema_50</emphasis>
                jeweils mit Naïve Bayes und SMO als Basislerner.</para>
            <para>Anders als bei den VLB-Warengruppen lag die Erfolgsquote mit Naïve Bayes deutlich über derjenigen von SMO, jedoch beide auf sehr geringem Niveau: Bei
                <emphasis>Thema_10</emphasis> erreicht Binary Relevance mit SMO als Basislerner lediglich eine Erfolgsquote von 0,027, Hamming Loss liegt bei 0,056. Werte 
                für weitere Maße für die Gesamtqualität gibt <xref linkend="tab_overall_result_thema"/>. Beim Vergleich der Durchläufe mit Schwellwert 50, bei dem wie
                erwähnt alle Klassen außer den vier frequentesten entfernt wurden, ist zu bedenken, dass die höheren Qualitätswerte allein dem Effekt der veränderten
                Durchschnittsbildung geschuldet sind. Binary Relevance trainiert ja pro Klasse einen eigenen Klassifikator, welcher von den Klassifikatoren für die 
                anderen Klassen unabhängig ist und somit dasselbe Ergebnis ausgibt, unabhängig davon, wieviele andere weitere Klassen im selben Lauf trainiert werden.</para>
            <para>Dass die Hamming-Loss-Werte bei Vorhandensein einer größeren Zahl von Klassen (78 Klassen bei <emphasis>Thema_10</emphasis> gegenüber 4 Klassen bei 
                <emphasis>Thema_50</emphasis>) besser sind, liegt daran, dass die Hamming-Loss-Berechnung grundsätzlich spärlichere Klassenbesetzungen begünstigt.
                Steigt die Zahl der Klassen, steigt auch die Zahl der Stellen in den Klassenvektoren und damit wächst auch der Nenner der Hamming-Loss-Formel.</para>
            <table pgwide="0" xml:id="tab_overall_result_thema"> <!-- Gesamt-Qualitätswerte Thema-Klassifikation mit Binary Relevance -->
                <title>Gesamtqualität Thema-Klassifikation mit Binary Relevance</title>
                <tgroup cols="5">
                    <colspec colname="measure" colnum="1" colwidth="3*"/>
                    <colspec colname="thema_10_nb" colnum="2" colwidth="1*"/>
                    <colspec colname="thema_10_smo" colnum="3" colwidth="1*"/>
                    <colspec colname="thema_50_nb" colnum="4" colwidth="1*"/>
                    <colspec colname="thema_50_smo" colnum="5" colwidth="1*"/>
                    <thead>
                        <row>
                            <entry>Maß</entry>
                            <entry><emphasis>Thema_10</emphasis> NB</entry>
                            <entry><emphasis>Thema_10</emphasis> SMO</entry>
                            <entry><emphasis>Thema_50</emphasis> NB</entry>
                            <entry><emphasis>Thema_50</emphasis> SMO</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>Erfolgsquote</entry>
                            <entry>0,081</entry>
                            <entry>0,027</entry>
                            <entry>0,363</entry>
                            <entry>0,407</entry>
                        </row>
                        <row>
                            <entry>Hamming Loss</entry>
                            <entry>0,086</entry>
                            <entry>0,056</entry>
                            <entry>0,293</entry>
                            <entry>0,238</entry>
                        </row>
                        <row>
                            <entry>Genauigkeit (micro-averaged)</entry>
                            <entry>0,05</entry>
                            <entry>0,029</entry>
                            <entry>0,147</entry>
                            <entry>0,093</entry>
                        </row>
                        <row>
                            <entry>Genauigkeit (macro-averaged)</entry>
                            <entry>0,071</entry>
                            <entry>0,013</entry>
                            <entry>0,173</entry>
                            <entry>0,075</entry>
                        </row>
                        <row>
                            <entry>Trefferquote (micro-averaged)</entry>
                            <entry>0,098</entry>
                            <entry>0,024</entry>
                            <entry>0,164</entry>
                            <entry>0,052</entry>
                        </row>
                        <row>
                            <entry>Trefferquote (macro-averaged)</entry>
                            <entry>0,09</entry>
                            <entry>0,028</entry>
                            <entry>0,275</entry>
                            <entry>0,088</entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <para>Die folgende <xref linkend="tab_top-class_result_thema"/> zeigt die Qualitätsmaße Genauigkeit und Trefferquote für die zehn größten Thema-Klassen. 
                Es ist eklatant, dass insbesondere in der Kombination Binary Relevance<indexterm class="endofrange" startref="idt_013"/> und SMO selbst einige dieser relativ großen Klassen überhaupt nicht gelernt wurden (Trefferquote und 
                Genauigkeit bei 0, also keine einzigen erzielten TPs in der Testmenge). Die Klasse FFP („Krimis: Polizeiarbeit“) konnte sogar in keinem der beiden 
                Verfahren einen korrekten Treffer aufweisen, wofür wir keine plausible Erklärung finden konnten. Bei der größten Klasse FYT („Belletristik in Übersetzung“)
                müsste sich die Klassifikation eigentlich auf ein einziges Kriterium stützen: Wird auf dem Titelblatt oder im Impressum des Buches eine Übersetzerin 
                angegeben? Dass diese Klasse trotz ihrer hohen Frequenz nicht besonders gut gelernt wurde, ist verständlich: Die Titelblätter der E-Books sind nicht
                immer als HTML-Seiten im EPUB enthalten, sondern werden oftmals als Grafik eingebunden, so dass eine Übersetzerangabe im Volltext u. U. gar nicht 
                gefunden und indiziert wurde. Außerdem kann natürlich auch ein Roman der deutschen Literatur irgendwo in seinem Volltext das Lemma „übersetzen“, vielleicht
                sogar im Sinne von „einen Fluss überqueren“, enthalten, obwohl das Buch gar keine Übersetzung ist. Es ist auf Basis dieser Ausgangssituation sehr 
                wahrscheinlich, dass sich im gelernten Modell falsche Hypothesen bezüglich der FYT-Klasse wiederfinden.</para>
            <para>Der Thema-Code FBA stellt eine sehr allgemeine Klasse für belletristische Literatur dar, welche inhaltlich mit der VLB-Warengruppe 112 verwandt ist,
                aber aufgrund der Multilabel-Eigenschaften der Thema-Klassifikation z.&#160;B. auch bei einem „literarischen“ Krimi oder einem auch für junge Erwachsene
                geeigneten Jugendbuch<indexterm><primary>Jugendbuch</primary></indexterm> beigefügt werden kann. Andererseits werden sich einige Verlage an die Thema-Richtlinien halten und keine allgemeineren Codes
                vergeben, wenn speziellere Codes den allgemeineren Code bereits implizieren. Dies führt natürlicherweise zu stärkeren Abgrenzungsschwierigkeiten zwischen
                FBA und anderen Belletristikklassen, als zwischen der VLB-Warengruppe 112 und dort verwandten anderen Warengruppen.</para>
            <para>Interessant erscheint uns das Ergebnis für den geografischen Thema-Qualifizierer für Deutschland. Naïve Bayes konnte hier eine beeindruckend hohe
                Trefferquote von 0,818 erzielen, d. h. es wurden fast alle mit 1DFG ausgezeichneten E-Books der Testmenge auch vom Klassifikator so erkannt. Die gleichzeitig
                niedrige Genauigkeit von 0,099 weist auf eine hohe Zahl von <emphasis>false negatives</emphasis> hin. Hier sollte ein genauerer Blick ansetzen und 
                diese Datensätze untersuchen, um folgende Frage zu klären: Hat der Klassifikator 1DFG oft vergeben für Testtitel, welche mit einer Unterkategorie
                (z.&#160;B. 1DFG-DE-TB für Bayern) versehen sind? Dann wäre die 1DFG-Klassifikation innerhalb des hierarchischen Thema-Kategorienbaums ja gar nicht falsch.</para>
            <table pgwide="0" xml:id="tab_top-class_result_thema"> <!-- Qualitätswerte für die größten Klassen bei automatischer Thema-Klassifikation mit Binary Relevance -->
                <title>Qualitätswerte für die zehn größten Klassen bei automatischer Thema-Klassifikation mit Binary Relevance</title>
                <tgroup cols="7">
                    <colspec colname="themacode" colnum="1" colwidth="3*"/>
                    <colspec colname="training" colnum="2" colwidth="1*"/>
                    <colspec colname="test" colnum="3" colwidth="1*"/>
                    <colspec colname="precision_nb" colnum="4" colwidth="1*"/>
                    <colspec colname="recall_nb" colnum="5" colwidth="1*"/>
                    <colspec colname="precision_smo" colnum="6" colwidth="1*"/>
                    <colspec colname="recall_smo" colnum="7" colwidth="1*"/>
                    <thead>
                        <row>
                            <entry>Thema-Code</entry>
                            <entry>Anzahl Trainingsinstanzen</entry>
                            <entry>Anzahl Testinstanzen</entry>
                            <entry>Genauigkeit (Precision) NB</entry>
                            <entry>Trefferquote (Recall) NB</entry>
                            <entry>Genauigkeit (Precision) SMO</entry>
                            <entry>Trefferquote (Recall) SMO</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row> <!-- Index 0-basiert: 65 -->
                            <entry>FYT (Belletristik in Übersetzung)</entry>
                            <entry>257</entry>
                            <entry>57</entry>
                            <entry>0,292</entry> <!-- Precision NB -->
                            <entry>0,123</entry> <!-- Recall NB -->
                            <entry>0,118</entry> <!-- Precision SMO -->
                            <entry>0,35</entry> <!-- Recall -->
                        </row>
                        <row> <!-- Index 0-basiert: 37 -->
                            <entry>FBA (Moderne und zeitgenössische Belletristik)</entry>
                            <entry>201</entry>
                            <entry>47</entry>
                            <entry>0,2</entry>
                            <entry>0,106</entry>
                            <entry>0,1</entry>
                            <entry>0,043</entry>
                        </row>
                        <row><!-- Index 0-basiert: 4 -->
                            <entry>1DFG (Deutschland)</entry>
                            <entry>110</entry>
                            <entry>11</entry>
                            <entry>0,099</entry>
                            <entry>0,818</entry>
                            <entry>0,081</entry>
                            <entry>0,273</entry>
                        </row>
                        <row> <!-- Index 0-basiert: 53 -->
                            <entry>FRD (zeitgenössische Liebesromane)</entry>
                            <entry>76</entry>
                            <entry>19</entry>
                            <entry>0,1</entry>
                            <entry>0,053</entry>
                            <entry>0</entry>
                            <entry>0</entry>
                        </row>
                        <row> <!-- Index 0-basiert: 59 -->
                            <entry>FU (Belletristik: Humor)</entry>
                            <entry>77</entry>
                            <entry>9</entry>
                            <entry>0,115</entry>
                            <entry>0,333</entry>
                            <entry>0</entry>
                            <entry>0</entry>
                        </row>
                        <row> <!-- Index 0-basiert: 14 -->
                            <entry>3MP (20. Jahrhundert)</entry>
                            <entry>75</entry>
                            <entry>6</entry>
                            <entry>0,025</entry>
                            <entry>0,333</entry>
                            <entry>0,033</entry>
                            <entry>0,167</entry>
                        </row>
                        <row> <!-- Index 0-basiert: 9-->
                            <entry>1KBB (USA)</entry>
                            <entry>60</entry>
                            <entry>5</entry>
                            <entry>0,077</entry>
                            <entry>0,2</entry>
                            <entry>0</entry>
                            <entry>0</entry>
                        </row>
                        <row><!-- Index 0-basiert: 77-->
                            <entry>YFM (Kinder/Jugend: Liebesromane</entry>
                            <entry>41</entry>
                            <entry>12</entry>
                            <entry>1,0</entry>
                            <entry>0,105</entry>
                            <entry>0</entry>
                            <entry>0</entry>
                        </row>
                        <row> <!-- Index 0-basiert: 42-->
                            <entry>FFP (Krimis: Polizeiarbeit)</entry>
                            <entry>46</entry>
                            <entry>11</entry>
                            <entry>0</entry>
                            <entry>0</entry>
                            <entry>0</entry>
                            <entry>0</entry>
                        </row>
                        <row> <!-- Index 0-basiert: 26 -->
                            <entry>5JA (Zielgruppe: Frauen/Mädchen)</entry>
                            <entry>45</entry>
                            <entry>11</entry>
                            <entry>0,167</entry>
                            <entry>0,091</entry>
                            <entry>0</entry>
                            <entry>0</entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <para>Es zeigt sich insgesamt, dass die Untersuchung der Möglichkeiten einer Multilabel-Klassifikation für Thema-Kategorien deutlich mehr 
                Trainingsdaten benötigen würde. Es stand hier nur etwa ein Sechstel der Daten zur Verfügung, welche für die VLB-Warengruppen verwendet
                werden konnten. Damit waren die Trainingsdaten pro Thema-Klasse viel zu spärlich.</para>
            <para>Es lässt sich aber auch bereits an diesem vorläufigen Ergebnis abschätzen, dass es Klassen gibt, welche prinzipiell einfacher und solche, 
                welche prinzipiell schwerer zu erlernen sind. Die geografischen Qualifizierer können, wie das Beispiel 1DFG zeigte, gut erlernt werden, aber
                belletristische Genres sind schwieriger zu unterscheiden als in der „einfachen“ Multiklassen-Klassifikation. Für die zeitlichen 
                Thema-Qualifizierer wäre es sicher besser, wenn wir in der Datenaufbereitungsphase nicht alle Zahlen durch das #-Zeichen ersetzt hätten,
                so dass auch Jahreszahlen, die in den Texten vorkommen können, nicht mehr unterschieden werden können. Dies sollte bei zukünftigen Versuchen
                mit einem Maschinenlernen der Thema-Kategorien unbedingt berücksichtigt werden.<indexterm class="endofrange" startref="idt_018"/></para>
        </sect1>
    </chapter>
    <chapter xml:id="ch_Zusammenfassung"><!-- Zusammenfassung und Ausblick -->
        <info>
            <title>Zusammenfassung und Ausblicke</title>
        </info>
        <para>Bei der automatischen Klassifikation von E-Book-Volltexten bezüglich der VLB-Warengruppen konnten mit dem Naïve-Bayes-Verfahren knapp
            60% der E-Books korrekt ausgezeichnet werden, mit dem SMO-Verfahren über 67%. Korrektheit bedeutete in diesem Kontext, dass das trainierte 
            Maschinenmodell die Test-E-Books genauso klassifizierte, wie es die Mitarbeiterinnen der Verlage getan zuvor haben. Dieses berechnete
            Qualitätsmaß missachtet zwei Aspekte, die im praktischen Einsatz eines E-Book-Klassifikationsprogramms aber eine Rolle spielen würden: Erstens
            die Tatsache, dass auch von Menschen vergebene VLB-Warengruppen keine Korrektheit von 100% erreichen und zweitens, dass für eine ebenfalls
            unbekannte Menge von E-Books zwei oder sogar drei verschiedenen Warengruppen in Frage kommen können, von denen keine als wirklich falsch zu 
            betrachten ist. Als Ausblick geben wir in <xref linkend="ch_sum_neue_Evaluation"/> einen Vorschlag und Beispiele für eine genauere, manuelle
            Auswertung unserer Testergebnisse. Vorläufig gilt es hier festzuhalten, dass eine Fehlerquote von 33% noch deutlich oberhalb der von 
            uns geschätzten Fehlerquote<footnote><para>Vgl. <xref linkend="ch_Evaluation_EBookKlassifikation"/>.</para></footnote> von Verlagsmitarbeitern
            von <modifiedInstancesRatio/> liegt. Für eine vollautomatische Klassifikation reicht die Qualität unserer Modelle somit nicht aus, für ein 
            <indexterm><primary>Vorschlagssystem</primary></indexterm>Vorschlagssystem hingegen durchaus.</para>
        <para>Der Versuch, auch die Thema-Klassifikation zu erlernen und zu automatisieren muss beim aktuellen Stand als Fehlschlag bewertet werden. 
            Allerdings wurde von uns auch bedeutend weniger Aufwand in Experimente mit Thema-Daten gesteckt und die Trainings- und Testdatenmengen waren
            in Anbetracht der hohen Anzahl von Klassen viel zu niedrig.</para>
        <para>Die durchgeführten Optimierungsversuche, welche die drei Bereiche (a) Parameteroptimierung, (b) <indexterm><primary>Ensemble Learning</primary>
        </indexterm>Ensemble Learning und (c) verbesserte
            Datenaufbereitung betrafen, konnten bestenfalls kosmetische Verbesserungen bringen. Bei Verwendung von SMO lohnte sich die Zugabe von 
            Oberbegriffen zu den Attributvektoren am meisten, bei Naïve Bayes das Boosten mit <indexterm><primary>AdaBoost</primary>
            </indexterm>AdaBoost.M1. Beide Verfahren führten zu einem deutlich
            höheren Bedarf an Berechnungszeit. Eine praktische Anwendung müsste also abwägen, ob der Qualitätsgewinn von etwa 1,5 Prozentpunkten bei 
            der Erfolgsquote diese zusätzliche Zeit wert ist.</para>
        <para>Nicht erforscht haben wir bisher Möglichkeiten der Dimensionalitätsreduktion der Eingabedaten, welche im Idealfall sowohl zu Qualitäts-
            als auch zu Laufzeitverbesserungen führen kann. Einige Ansätze für Untersuchungen in dieser Richtung geben wir als zweiten Ausblick in 
            <xref linkend="ch_sum_weitere_Untersuchungsvorschläge"/>. Ebenfalls unerforscht blieb die Frage, ob es bessere Arten der Textextraktion aus den 
            E-Books gibt. Wir haben den Volltext unter Missachtung von HTML-Tags verwendet, welche aber durchaus interessante Hinweise geben könnten. 
            Einen alternativen Vorschlag in dieser Richtung reißen wir in <xref linkend="ch_sum_strukturierte_Textverarbeitung"/> an.</para>
        <sect1 xml:id="ch_sum_neue_Evaluation"> <!-- Alternatives Qualitätsmaß für die Evaluation -->
            <title>Alternatives Qualitätsmaß für die Evaluation</title>
            <para>Es ist eine typische Eigenschaft von Textklassifikationsaufgaben, dass oft keine eindeutige Entscheidung zwischen richtig („Datensatz gehört zur
                Klasse“) und falsch („Datensatz gehört nicht zur Klasse“) möglich ist. Dieses Phänomen betrifft nicht nur das Maschinenlernen, sondern, wie bereits erwähnt<footnote>
                    <para>Siehe den Eintrag „Indexierungsinkonsistenz“ <indexterm><primary>Indexierungsinkonsistenz</primary></indexterm>im Register dieser
                        Arbeit.</para></footnote>, auch die Klassifikation durch menschliche Indexer. Die
                Evaluation der Klassifikation dann auf einer einfachen binären richtig/falsch-Logik mit TP/TN einerseits und FP/FN andererseits aufzubauen, wird dem 
                Problem nicht gerecht, auch wenn es den Vorteil bietet, leicht berechenbar zu sein. </para>
            <para>Eigentlich bedürfte es einer mehrstufigen Gewichtung der Testklassifikationen, bei der z.&#160;B. <emphasis>true positives</emphasis> weiterhin der Wert 
                1 zugeordnet wird und nicht vertretbare Fehlklassifikationen mit dem Faktor 0 in die Berechnung eingehen, bei der es aber noch zwei Zwischenstufen gäbe:
                eine nicht ganz passende, aber doch vertretbare Klassifikation mit Gewichtungsfaktor zwischen 0,25 oder 0,5 und eine auch passende, aber von der
                vorgegebenen Klasse abweichende Klassifikation mit Gewichtungsfaktor zwischen 0,5 oder 0,75. Eine solche Evaluation könnte durch die Aufsplittung der
                <emphasis>false positives</emphasis> besser feststellen, ob das automatische Klassifikationsergebnis auch für menschliche Anwenderinnen und Anwender
                akzeptabel ist oder nicht. Besonders wichtig wäre eine weiche Evaluation bei der Thema-Klassifikation, wo aufgrund der feineren Gliederung und Abstufung
                der Thema-Klassen auch die 
                akzeptablen Überschneidungen zwischen den Klassen zunehmen. Während etwa die VLB-Warengruppen nur zwischen Kinderbüchern (bis 11 Jahre) und Jugendbüchern
                (ab 12 Jahren) trennen, gibt es in Thema für jedes empfohlene Alter von 3 bis 14 Jahren eine eigene Klasse (Codes 5AC bis 5AQ). An welchen Textmerkmalen 
                lässt es sich festmachen, ob ein <indexterm><primary>Jugendbuch</primary></indexterm>Jugendbuch für 11- oder 12-jährige Kinder zu empfehlen ist? Gibt es solche Merkmale überhaupt? Nach welchen Kriterien
                entscheiden die Jugendbuchverlage? Um hier zu einer angemessenen Einschätzung automatischer Klassifikation zu kommen, bedarf es auch des Austauschs
                mit Domänenexpertinnen.</para>
            <para>Je größer die Datenmenge ist, die beim Maschinenlernen von E-Book-Klassifikationen verwendet wird, desto aufwändiger wird natürlich auch eine solche
                Evaluation, denn die Zuschreibung des jeweiligen Status zu den Fehlklassifikationen müsste durch mehrere menschliche Klassifikatoren erfolgen, um eine
                intersubjektive Aussagekraft zu erlangen. Eine solche Bewertung von <emphasis>false positives</emphasis> würde dann z.&#160;B. so aussehen wie in der folgenden
                Tabelle dargestellt.</para>
            <table pgwide="1"> <!-- Beispiele für eine differenzierte Bewertung von false positives -->
                <title>Beispiele für eine differenzierte Bewertung von <emphasis>false positives</emphasis></title>
                <tgroup cols="6">
                    <colspec colname="ID" colnum="1" colwidth="1.5*"/>
                    <colspec colname="erwartet" colnum="2" colwidth="1*"/>
                    <colspec colname="verfahren" colnum="3" colwidth="1.5*"/>
                    <colspec colname="vergeben" colnum="4" colwidth="1*"/>
                    <colspec colname="gewicht" colnum="5" colwidth="1*"/>
                    <colspec colname="bemerkung" colnum="6" colwidth="4*"/>
                    <thead>
                        <row>
                            <entry>ID</entry>
                            <entry>Erwartete WG</entry>
                            <entry>Verfahren</entry>
                            <entry>Gelieferte WG</entry>
                            <entry>Gewicht</entry>
                            <entry>Begründung</entry>
                        </row>
                    </thead>
                    <tbody>
                        <!--<row><entry>000004860</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry></entry><entry>bedingt: Genre stimmt (Roman), Roman im Original 1935 erschienen (Grenze WG: 111/112 liegt bei 1945), dt. Übersetzung stammt von 2008</entry></row>-->
                        <row><entry>000004860</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,25</entry><entry>Genre stimmt (Roman), Roman im Original 1935 erschienen (Grenze WG: 111/112 liegt bei 1945),
                                dt. Übersetzung stammt von 2008</entry></row>
                        <row><entry>000004877</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>121</entry><entry>0</entry><entry>das Buch ist kein Krimi, sondern ein humorvoller Roman</entry></row>
                        <!--<row><entry>000004877</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>121</entry><entry></entry><entry>nein, das Buch ist kein Krimi, sondern ein humorvoller Roman</entry></row>-->
                        <row><entry>000005012</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>121</entry><entry>0,25</entry><entry>kein klassischer Krimi, aber laut einer Rezension enthält das Buch Szenen von „erheblicher
                                Grausamkeit“</entry></row>
                        <!--<row><entry>000005012</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>121</entry><entry></entry><entry>bedingt: kein klassischer Krimi, aber laut Rezension in SZ vom 18.07.2011 enthält das Buch Szenen von „erheblicher Grausamkeit“ (via Perlentaucher)</entry></row>-->
                        <!--<row><entry>000005086</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>250</entry><entry>0</entry><entry>nein, eher würde noch 116 (biografischer Roman) passen; was auf Jugendbuch hindeuten könnte: die Übersetzung stammt von Mirjam Pressler</entry></row>-->
                        <!--<row><entry>000005086</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>250</entry><entry></entry><entry>nein, eher würde noch 116 (biografischer Roman) passen; was auf Jugendbuch hindeuten könnte: die Übersetzung stammt von Mirjam Pressler</entry></row>-->
                        <row><entry>000005168</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0</entry><entry>im Buch geht es zwar um Freundinnen (typisches Mädchenbuchthema?), aber die Protagonistinnen
                                sind Mitte 30</entry></row>
                        <!--<row><entry>000005168</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry></entry><entry>nein; im Buch geht es zwar um Freundinnen (typisches Mädchenbuchthema?), aber die Protagonistinnen sind Mitte 30</entry></row>-->
                        <row><entry>000005562</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0</entry><entry>ein Familienroman über die Zeit nach dem Zweiten Weltkrieg, nicht spezifisch für jugendliches
                                Publikum</entry></row>
                        <!--<row><entry>000005562</entry><entry>112</entry><entry>SMO</entry><entry>260</entry><entry></entry><entry>nein; ein Familienroman über die Zeit nach dem Zweiten Weltkrieg, nicht spezifisch für jugendliches Publikum</entry></row>-->
                        <row><entry>000005720</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0,25</entry><entry>humorvoller Liebesroman mit erwachsener Protagonistin</entry></row>
                        <row><entry>000005821</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0,5</entry><entry>die Protagonistin ist ein sechzehnjähriger Flüchtling aus Afrika, das könnte auch als
                                Jugendbuch durchgehen</entry></row>
                        <row><entry>000005846</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>185</entry><entry>0,5</entry><entry>das Buch passt in beide Klassen: Es enthält humorvolle Erzählungen und wird von <indexterm>
                            <primary>Lippe, Jürgen von der</primary></indexterm>Jürgen von der Lippe empfohlen</entry></row>
                        <row><entry>000005850</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0,25</entry><entry>es geht um die Modelkarriere einer jungen Frau, müsste man genauer prüfen</entry></row>
                        <row><entry>000005854</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>113</entry><entry>0</entry><entry>der Roman evoziert zwar Geschehen in der Vergangenheit, aber um 1939; ein historischer Roman
                                müsste vor 1900 spielen</entry></row>
                        <row><entry>000006493</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>121</entry><entry>0</entry><entry>der Roman ist laut Waschzettel kein Krimi, sondern eine Geschichte „alltäglicher
                                Tragödien“</entry></row>
                        <!--<row><entry>000006503</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry></entry><entry>bedingt; humorvoller Liebesroman mit erwachsener Protagonistin</entry></row>-->
                        <row><entry>000006503</entry><entry>112</entry><entry>SMO</entry><entry>260</entry><entry>0,25</entry><entry>humorvoller Liebesroman mit erwachsener Protagonistin</entry></row>
                        <row><entry>000006536</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>185</entry><entry>0,5</entry><entry>das Buch passt in beide Kategorien: Es enthält humorvolle Erzählungen</entry></row>
                        <row><entry>000007292</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>971</entry><entry>0</entry><entry>das Buch ist fiktional – thematisch ist die falsche Klassifkation aber durchaus passend: Im
                                Roman erleidet eine reale Politikerin auf einer Auslandreise nach einem Unfall einen Gedächtnisverlust</entry></row>
                        <row><entry>000007292</entry><entry>112</entry><entry>SMO</entry><entry>121</entry><entry>0,25</entry><entry>da es um Politik und Russland geht, könnte Geheimdienstarbeit / Spionage eine Rolle spielen;
                                Buch ist aber in erster Linie satirisch</entry></row>
                        <row><entry>000007442</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>116</entry><entry>0,5</entry><entry>der Roman behandelt das Leben historischer Personen, die im Widerstand gegen die Nazis tätig
                                waren</entry></row>
                        <!--<row><entry>000008342</entry><entry>361</entry><entry>Naïve Bayes</entry><entry>111</entry><entry>0,5</entry><entry>ja, das Buch gilt als „Reiseroman“ und erschien 1904; die abweichende Klassifikation ist absolut vertretbar</entry></row>-->
                        <row><entry>000008342</entry><entry>361</entry><entry>SMO</entry><entry>111</entry><entry>0,5</entry><entry>das Buch gilt als „Reiseroman“ und erschien 1904; die abweichende Klassifikation ist absolut
                                vertretbar</entry></row>
                        <row><entry>000013493</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,25</entry><entry>Genre stimmt (Roman); das Buch ist 1928 erstmals erschienen</entry></row>
                        <row><entry>000014021</entry><entry>112</entry><entry>SMO</entry><entry>121</entry><entry>0</entry><entry>das Buch gibt ein politisch-zeithistorisches Panorama, ist nicht auf Krimi / Thriller
                                getrimmt</entry></row>
                        <row><entry>000014670</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>117</entry><entry>0</entry><entry>es könnten evtl. Briefe in den Roman eingebettet sein, aber es bleibt ein Roman</entry></row>
                        <row><entry>000015876</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>111</entry><entry>0</entry><entry>der Autor ist 1937 geboren, sein Werk gehört klar in die Zeit nach 1945</entry></row>
                        <row><entry>000015909</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0</entry><entry>Familienroman mit Scheidungskindern, evtl. etwas zu gruselig für Jugendliche?</entry></row>
                        <row><entry>000016057</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>118</entry><entry>0</entry><entry>der GAU in Tschernobyl wird hier als Erzählung, nicht als Essay verarbeitet</entry></row>
                        <row><entry>000016091</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>151</entry><entry>0</entry><entry>das ist keine Lyrik (auch wenn der Autor Lyriker ist), sonder eine Prosa-Erzählung</entry></row>
                        <row><entry>000016117</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>926</entry><entry>0</entry><entry>das Buch ist der Roman einer Kindheit, kein Sachbuch über Christentum</entry></row>
                        <row><entry>000016122</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>362</entry><entry>0</entry><entry>das Buch ist kein Reisebuch, sondern essayistische Erzählung über Jugend und
                                Erwachsenwerden</entry></row>
                        <!--<row><entry>000016127</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>111</entry><entry/><entry>nein, Handkes Bücher wurden sämtlich nach 1945 publiziert, das passt nicht in WG 111</entry></row>-->
                        <row><entry>000016375</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0,25</entry><entry>Liebesroman mit einer achtzehnjährigen Protagonistin, das könnte auch Jugendliche interessieren</entry></row>
                        <row><entry>000016396</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>362</entry><entry>0,5</entry><entry>das Buch ist auch eine Reiseerzählung mit Italien als Thema</entry></row>
                        <row><entry>000016447</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0</entry><entry>diese Geschichte einer Scheinehe zur Steuerersparnis ist humorvoll, aber thematisch im
                                Erwachsenenbereich als in jugendlicher Lebenswelt
                                angesiedelt</entry></row>
                        <!--<row><entry>000016523</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>113</entry><entry>0,5</entry><entry>ja, das Buch enthält erotische Geschichte, die im England des 19. Jahrhunderts spielen, man kann es also als historische Erzählungen einordnen</entry></row>-->
                        <row><entry>000016523</entry><entry>112</entry><entry>SMO</entry><entry>113</entry><entry>0,5</entry><entry>das Buch enthält erotische Geschichten, die im England des 19. Jahrhunderts spielen, man kann
                                es also als historische Erzählungen einordnen</entry></row>
                        <row><entry>000016536</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>260</entry><entry>0</entry><entry>erotischer Roman mit sehr freizügigem Cover: ganz klar für Erwachsene</entry></row>
                        <!--<row><entry>000016540</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>132</entry><entry>0</entry><entry>das Buch ist „reiner“ erotischer Roman, aber nicht Fantasy</entry></row>-->
                        <!--<row><entry>000016543</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>132</entry><entry/><entry>nein, das Buch würde in 113 als historischer Roman passen, aber nicht Fantasy</entry></row>-->
                        <!--<row><entry>000016543</entry><entry>112</entry><entry>SMO</entry><entry>132</entry><entry>0</entry><entry>nein, das Buch würde in 113 als historischer Roman passen, aber nicht Fantasy</entry></row>-->
                        <!--<row><entry>000016561</entry><entry>112</entry><entry>Naïve Bayes</entry><entry>132</entry><entry>0,5</entry><entry>das ist erotischer Roman, Zeitreisegeschichte und Kampf mit Dämonen: passt wohl auch in Fantasygenre</entry></row>-->
                        <!--<row><entry>000016564</entry><entry>112</entry><entry>SMO</entry><entry>113</entry><entry/><entry>ja, dieser erotische Roman spielt im antiken Ägytpen, ist also auch ein historischer Roman</entry></row>-->
                        <!--<row><entry>000016564</entry><entry>112</entry><entry>SMO</entry><entry>132</entry><entry>0</entry><entry>nein, das Buch würde in 113 als historischer Roman passen, aber nicht Fantasy</entry></row>-->
                        <row><entry>000162644</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,25</entry><entry>Genre stimmt (Roman); das Buch ist 1928 im Original erschienen, die deutsche Übersetzung
                                stammt von 1982</entry></row>
                        <!--<row><entry>000431793</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>926</entry><entry/><entry>nein: Übersetzung ist zwar modern (Wolfgang Kasack), aber Werk ist klar aus dem 19. Jahrhundert und kein Sachbuch über christliche Religion</entry></row>-->
                        <row><entry>000431793</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>926</entry><entry>0</entry><entry>Übersetzung ist zwar modern, aber das Werk ist klar aus dem 19. Jahrhundert und kein Sachbuch
                                über christliche Religion</entry></row>
                        <!--<row><entry>000431793</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0</entry><entry>nein, das Buch stammt klar aus dem 19. Jahrhundert</entry></row>-->
                        <!--<row><entry>000593168</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry>0,25</entry><entry>bedingt: Genre stimmt (Roman), Roman im Original 1932 erschienen (Grenze WG: 111/112 liegt bei 1945), erste dt. Übersetzung stammt von 1950, wurde vermutlich später noch einmal bearbeitet</entry></row>-->
                        <!--<row><entry>000593168</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>bedingt: Genre stimmt (Roman), Roman im Original 1932 erschienen (Grenze WG: 111/112 liegt bei 1945), erste dt. Übersetzung stammt von 1950, wurde vermutlich später noch einmal bearbeitet</entry></row>-->
                        <row><entry>000593168</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,25</entry><entry>Genre stimmt (Roman), Roman im Original 1932 erschienen (Grenze WG: 111/112 liegt bei 1945),
                                erste dt. Übersetzung stammt von 1950, wurde vermutlich später noch
                                einmal bearbeitet</entry></row>
                        <!--<row><entry>001139777</entry><entry>361</entry><entry>Naïve Bayes</entry><entry>185</entry><entry>0,5</entry><entry>ja, das Buch enthält humorvolle Schilderungen der Erfahrungen zweier Autoren auf Lesereise durch Deutschland</entry></row>-->
                        <row><entry>001139777</entry><entry>361</entry><entry>SMO</entry><entry>185</entry><entry>0,5</entry><entry>das Buch enthält humorvolle Schilderungen der Erfahrungen auf einer Autorenlesereise durch
                                Deutschland</entry></row>
                        <!--<row><entry>001177461</entry><entry>361</entry><entry>Naïve Bayes</entry><entry>185</entry><entry/><entry>ja, das Buch enthält humorvolle Schilderungen des zeitweisen Lebens auf einer Alm</entry></row>-->
                        <!--<row><entry>001177461</entry><entry>361</entry><entry>SMO</entry><entry>112</entry><entry>0,25</entry><entry>das Buch enthält humorvolle Schilderungen des zeitweisen Lebens auf einer Alm</entry></row>-->
                        <row><entry>001177841</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,5</entry><entry>des Autors Hauptwerk liegt eher vor 1945, aber dieser Roman ist erst 1957 oder später
                                erschienen</entry></row>
                        <!--<row><entry>001177843</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry>0,5</entry><entry>ja: Roman im Original erst 1947 vollständig erschienen, dt. Übersetzung von 1949</entry></row>-->
                        <!--<row><entry>001177843</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>ja: Roman im Original erst 1947 vollständig erschienen, dt. Übersetzung von 1949</entry></row>-->
                        <row><entry>001177843</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,5</entry><entry>Roman im Original erst 1947 vollständig erschienen, dt. Übersetzung von 1949</entry></row>
                        <row><entry>001426514</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>151</entry><entry>0</entry><entry>das Buch ist in Prosa verfasst, keine Lyrik</entry></row>
                        <!--<row><entry>001426514</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry/><entry>nein, das Buch ist klar ein Werk des 19. Jahrhunderts</entry></row>-->
                        <!--<row><entry>001579953</entry><entry>361</entry><entry>Naïve Bayes</entry><entry>312</entry><entry>0</entry><entry>nein, das Buch wird beschrieben als ein „poetischer Genuss“, die Erzählung steht also im Vordergrund vor der praktischen Reiseinformation</entry></row>-->
                        <row><entry>001579953</entry><entry>361</entry><entry>SMO</entry><entry>312</entry><entry>0</entry><entry>das Buch wird beschrieben als ein „poetischer Genuss“, die Erzählung steht also im
                                Vordergrund vor der praktischen Reiseinformation</entry></row>
                        <!--<row><entry>001856356</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>bedingt: Genre stimmt (Roman), Roman im Original 1925 erschienen (Grenze WG: 111/112 liegt bei 1945), dt. Übersetzung von Peter Handke modern (1982)</entry></row>-->
                        <!--<row><entry>001856356</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>bedingt: Genre stimmt (Roman), Roman im Original 1925 erschienen (Grenze WG: 111/112 liegt bei 1945), dt. Übersetzung von Peter Handke modern (1982)</entry></row>-->
                        <!--<row><entry>001856356</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry/><entry>bedingt: Genre stimmt (Roman), Roman im Original 1925 erschienen (Grenze WG: 111/112 liegt bei 1945), dt. Übersetzung von Peter Handke modern (1982)</entry></row>-->
                        <row><entry>001959809</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry>0,25</entry><entry>Genre stimmt (Roman), Roman im Original 1913 erschienen, aber die Übersetzung ist wohl aus
                                den 2000er Jahren</entry></row>
                        <!--<row><entry>001959809</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>bedingt: Genre stimmt (Roman), Roman im Original 1913 erschienen, aber die Übersetzung ist wohl aus den 2000er Jahren</entry></row>-->
                        <!--<row><entry>001959809</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry/><entry>bedingt: Genre stimmt (Roman), Roman im Original 1913 erschienen, aber die Übersetzung ist wohl aus den 2000er Jahren</entry></row>-->
                        <row><entry>002601127</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>152</entry><entry>0</entry><entry>falsches Genre (richtig Roman, klassifiziert als Drama) – kommt wohl daher, dass Autor auch
                                Dramen geschrieben hat und eines seiner Dramen in der Trainingsmenge
                                war?</entry></row>
                        <!--<row><entry>002601127</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>152</entry><entry/><entry>nein: falsches Genre (richtig Roman, klassifiziert als Drama) – kommt wohl daher, dass Autor auch Dramen geschrieben hat und eines seiner Dramen in der Trainingsmenge war?</entry></row>-->
                        <!--<row><entry>002601127</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,25</entry><entry>bedingt: Genre stimmt; Roman 1937 Jahren erschienen</entry></row>-->
                        <row><entry>002722686</entry><entry>361</entry><entry>Naïve Bayes</entry><entry>362</entry><entry>0</entry><entry>der Harz gehört klar zu Deutschland; vermutlich Überanpassung wegen des Reihentitels, der
                                häufiger für Titel über Europa / Rest der Welt vorkommt</entry></row>
                        <!--<row><entry>002722686</entry><entry>361</entry><entry>SMO</entry><entry>362</entry><entry/><entry>nein, der Harz gehört klar zu Deutschland; vermutlich Überanpassung wegen des Reihentitels, der häufiger für Titel über Europa / Rest der Welt vorkommt</entry></row>-->
                        <row><entry>002722782</entry><entry>361</entry><entry>Naïve Bayes</entry><entry>312</entry><entry>0,25</entry><entry>die Buchreihe ist in erzählerischem Duktus geschrieben, bietet aber auch praktische
                                Reiseinformation</entry></row>
                        <!--<row><entry>002722782</entry><entry>361</entry><entry>SMO</entry><entry>362</entry><entry>0</entry><entry>Leipzig gehört klar zur Deutschland; vermutlich Überanpassung wegen des Reihentitels, der auch für europäische Orte verwendet wird</entry></row>-->
                        <row><entry>002725325</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,5</entry><entry>ja, das Buch ist 1943 erschienen, liegt also ziemlich genau auf der Zeitgrenze; ob das Hauptwerk des Autors vor oder nach 1945 liegt, ist diskutabel</entry></row>
                        <!--<row><entry>004768014</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>362</entry><entry/><entry>ja, Buch passt in beide Klassen</entry></row>-->
                        <!--<row><entry>004768014</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>362</entry><entry/><entry>ja, Buch passt in beide Klassen</entry></row>-->
                        <row><entry>005127210</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>116</entry><entry>0</entry><entry>das Buch stammt im Original von 1895 und trägt keine Züge eines biografischen Romans</entry></row>
                        <!--<row><entry>005127210</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>116</entry><entry/><entry>nein, das Buch stammt im Original von 1895 und trägt keine Züge eines biografischen Romans</entry></row>-->
                        <!--<row><entry>005127210</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry/><entry>bedingt; Genre stimmt (Roman); es erschien im Original erstmals 1895, war aber thematisch zu seiner Zeit recht modern</entry></row>-->
                        <!--<row><entry>005509811</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry/><entry>nein, das Buch ist ein Werk des 19. Jahrhunderts</entry></row>-->
                        <row><entry>006102793</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,5</entry><entry>das Buch erschien im Original 1962, wenn auch das Hauptwerk des Autors vor 1945 liegt</entry></row>
                        <!--<row><entry>006710350</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>ja, der Roman ist zwar 1931 erschienen, aber die Autorin veröffentlichte bis 1962 Bücher, so dass dass fraglich ist, ob ihr Hauptwerk vor oder nach 1945 situiert ist</entry></row>-->
                        <!--<row><entry>006710350</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>ja, der Roman ist zwar 1931 erschienen, aber die Autorin veröffentlichte bis 1962 Bücher, so dass dass fraglich ist, ob ihr Hauptwerk vor oder nach 1945 situiert ist</entry></row>-->
                        <row><entry>006710350</entry><entry>111</entry><entry>SMO</entry><entry>112</entry><entry>0,5</entry><entry>der Roman ist zwar 1931 erschienen, aber die Autorin veröffentlichte bis 1962 Bücher, so dass
                                dass fraglich ist, ob ihr Hauptwerk vor oder nach 1945 situiert
                                ist</entry></row>
                        <!--<row><entry>006710352</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>ja, der Roman ist zwar 1937 erschienen, aber die Autorin veröffentlichte bis 1962 Bücher, so dass dass fraglich ist, ob ihr Hauptwerk vor oder nach 1945 situiert ist</entry></row>-->
                        <!--<row><entry>006710352</entry><entry>111</entry><entry>Naïve Bayes</entry><entry>112</entry><entry/><entry>ja, der Roman ist zwar 1937 erschienen, aber die Autorin veröffentlichte bis 1962 Bücher, so dass dass fraglich ist, ob ihr Hauptwerk vor oder nach 1945 situiert ist</entry></row>-->
                    </tbody>
                </tgroup>
            </table>
            <para>Wenn man vor dem Aufwand einer solchen manuellen Auswertung zurückschreckt bzw. nicht die entsprechenden Ressourcen dazu hat, besteht eine
                weitere denkbare Alternative darin, Affinitäten zwischen Warengruppen zu definieren. Vergibt dann ein Klassifikator eine falsche, aber affine
                Klasse für ein Test-E-Book, dann würde dieser Fehler je weniger schwer gewichtet, desto größer die definierte Affinität zwischen der falschen und 
                der richtigen Klasse ist. Diese Möglichkeit wäre zwar mit weniger Aufwand zu erstellen als eine manuelle Begutachtung, der Gewinn einer solchen 
                Vorgehensweise wäre aber ebenfalls von fraglicher Güte: Welcher Experte könnte Affinitäten zwischen Warengruppen mathematisch und objektiv
                quantifizieren? In der Praxis würde dadurch wohl lediglich eine weitere Quelle von Subjektivität eröffnet.</para>
        </sect1>
        <sect1 xml:id="ch_sum_weitere_Untersuchungsvorschläge"> 
            <title>Weitere Untersuchungsmöglichkeiten</title>
            <sect2 xml:id="ch_sum_Dimensionsreduktion"><!-- Dimensionsreduktion -->
                <title>Dimensionsreduktion</title>
                <para>Die meisten Maschinenlernalgorithmen benötigen lange Rechenzeiten. Das liegt einerseits an der möglicherweise großen Komplexität der
                    Algorithmen selbst, aber auch daran, dass man in der Trainingsphase möglichst große Datenmengen verwenden möchte, da die Qualität der 
                    erlernten Modelle mit der Menge der Trainingsdaten steigt. Eine Möglichkeit, die Berechenbarkeit von Lernproblemen zu verbessern, ist die 
                    Reduktion der Größe des Attributvektors. Bei der Festlegung von Grenzwerten auf Basis der TF-IDF-Scores bei der Erstellung unserer Wortlisten
                    haben wir ja eine eigene Dimensionalitätsreduktion bereits verwendet (vgl. <xref linkend="ch_LexSemProp"/>). Dennoch blieben in unseren 
                    Untersuchungen Attributvektoren mit mehr als 16.000 Attributen übrig.</para>
                <para>Einen Überblick über weitere verbreitete Verfahren zur Dimensionsreduktion geben <biblioref linkend="bib_WFHP17"/> in
                    Kapitel 8 ihres Buches. Ein sehr einfaches Verfahren ist die Reduktion der Attribute auf Basis ihrer Entropie, wie wir es in <xref linkend="ch_AttribEval_Entropie"/>
                    beschrieben haben. Kurze Versuche haben mit unseren E-Book-Daten zwar eine Verbesserung der Laufzeit ergeben, aber auch eine Verschlechterung der
                    Erfolgsquote. Ausführlichere Experimente in dieser Richtung könnten sich jedoch lohnen, ebenso wie Versuche mit elaborierteren statistischen 
                    Verfahren wie der Diskriminanzanalyse.</para>
                <para>Einen interessanten Ansatz zur Reduktion des Attributvektors speziell in der Textklassifikation liefern <biblioref linkend="bib_MCCD13"/>,
                    welche Korpora mit einem Vokabular von mehreren Millionen Wörtern auf eine kontinuierliche Repräsentation abbilden, wobei ähnliche Wörter in
                    räumlicher Nähe zueinander abgebildet werden. Diese Reduktion erfolgt mit Hilfe von neuronalen Netzen in effizienter Weise.</para>
                <para>Im Laufe der vorliegenden Arbeit wurde schließlich auch wiederholt darauf hingewiesen, dass Attribute, welche nicht statistisch unabhängig voneinander
                    sind, schädlich sein können für Naïve Bayes, zumindest, wenn ihre Zahl zu groß wird. Auch auf lineare Supportvektormaschinen können zu viele
                    abhängige Attribute eine schädliche Wirkung haben. Deshalb ist der Vorschlag zur heuristischen Abschätzung der Attributabhängigkeit von 
                    <biblioref linkend="bib_JB03"/> ebenfalls bedenkenswert. Mit Hilfe dieser Abschätzung könnte dann eine gezielte Verkleinerung der Attributmenge
                    erfolgen.</para>
                <para>Nach der Vorverarbeitung der Daten durch solche Reduktionsmaßnahmen können aufgrund des verkleinerten Attributvektors auch Maschinenlernalgorithmen
                    interessant werden, welche aufgrund von Schwierigkeiten bei der Berechnung (zu lange Dauer und / oder zu hoher Speicherbedarf) aus unserer 
                    Untersuchung früh ausgeschlossen worden sind.</para>                
            </sect2>
            <sect2 xml:id="ch_sum_weitere_Texteigenschaften"><!-- Ermittlung weiterer Texteigenschaften -->
                <title>Ermittlung weiterer Texteigenschaften</title>
                <para>In <xref linkend="ch_TextProp"/> haben wir uns auf Texteigenschaften gestützt, die mit frei verfügbaren Mitteln leicht errechnet werden 
                    können. Die Entropieberechnung für Attribute hat ja gezeigt, dass die meisten grammatikalisch-statistischen Texteigenschaften einen hohen
                    Informationsgehalt bezüglich der Klassifikationsaufgabe aufweisen. Werkzeuge zur genaueren grammatikalischen Textanalyse vorausgesetzt,
                    könnten noch weitere stilistische Textmerkmale ermittelt werden, welche bei der Klassifikation hilfreich sein können. So unterliegt z.&#160;B.
                    auch die Verwendungshäufigkeit von verschiedenen Verbformen (z.&#160;B. Präteritum, Plusquamperfekt, Futur I) Schwankungen, welche unter
                    Umständen genutzt werden können (<biblioref linkend="bib_All89"/>, S. 551). Auch die von Interpunktion und die Verwendung bestimmter
                    syntaktischer Kategorien kann für die Textklassifikation fruchtbar gemacht werden (<biblioref linkend="bib_KNS97"/>).</para>
                <para>In unserem Fazit zur Thema-Klassifikation haben wir bereits ausgeführt, dass einige Zahlen von großer Bedeutung sein können für die 
                    Abgrenzung von Klassen, z.&#160;B. Jahreszahlen oder Altersangaben. Unsere frühe, durch die Fachliteratur (z.&#160;B. <biblioref
                        linkend="bib_BR99"/> und <biblioref linkend="bib_GS04"/>) getriebene Vorgehensweise, im Text vorkommende Ziffern zu ignorieren, 
                    würden wir so nach Abschluss der Untersuchungen nicht mehr treffen. Ein Mittelweg zwischen dem Ignorieren von Zahlen und der allzu großen
                    Varianz, die sie in den Wortvektor einfügen können, wäre vielleicht eine Textvorverarbeitung mit Hilfe von regulären Ausdrücken, welcher
                    (vierstellige) Jahreszahlen und kleinere Zahlen bis hundert im Text belässt, aber längere Zahlenkolonnen (z.&#160;B. Telefonnummern oder 
                    ISBNs) weiterhin ausschließt.</para>
            </sect2>
        </sect1>
        <sect1 xml:id="ch_sum_strukturierte_Textverarbeitung">
            <title>Arbeit mit semistrukturiert ausgezeichneten Texten</title>
            <para>Bei unserer Extraktion der E-Book-Volltexte wurden sowohl die im EPUB vorhandenen Metadaten (welche in einem XML-Format in der sogenannten 
                OPF-Datei enthalten sind) als auch die strukturierenden HTML-Elemente ignoriert. Dabei wäre die Annahme, dass etwa Wörter, welche in 
                Überschriften vorkommen, wichtiger sein könnten, als solche, die nur im Fließtext erscheinen, recht plausibel. Ein interessanter weiterer
                Verbesserungsversuch wäre dann, solche Informationen auszuwerten und zu boosten &#x2012; boosten nicht im Sinne des Ensemble Learnings à la AdaBoost, sondern
                in der Bedeutung, die das Wort im <indexterm><primary>Information Retrieval</primary>
                </indexterm>Information Retrieval hat und mit Hilfe des Lucene-Suchindexes leicht implementiert werden könnte.</para>
        </sect1>
    </chapter>
    <appendix xml:id="app_WG_dist"><!-- Anhang A: Statistische Verteilung der Warengruppen im Korpus -->
        <info>
            <title>Statistische Verteilung der Klassen im Korpus</title>
        </info>
        <para><indexterm><primary>Korpus</primary></indexterm>Dieser Anhang gibt die statistische Verteilug der VLB-Warengruppen und der Thema-Codes im jeweils 
            verwendeten Korpus wider.</para>
        <sect1>
            <title>VLB-Warengruppen-Verteilung</title>
            <para>Die folgende Tabelle listet die statistische Verteilung der VLB-Warengruppen im Korpus (Trainingsdaten und 
                Testdaten gemeinsam) auf. Die Spalte <emphasis>Anteil KNV-Katalog</emphasis> gibt an, welchen Anteil diese
                Warengruppe im E-Book-Katalog des <glossterm linkend="gt_barsortiment">Barsortiments</glossterm> Koch, Neff
                &amp; Volckmar hat, welcher als repräsentativ für den deutschen E-Book-Gesamtmarkt angenommen wird. 
            </para>
            <warengruppenDistribution/>
        </sect1>
        <sect1>
            <title>Thema-Code-Verteilung</title>
            <para>Die folgende Tabelle listet die statistische Verteilung der Thema-Codes im Korpus der Thema-Trainings- und
                Testdaten auf. Sie zeigt die Verteilung der Thema-Codes, nachdem die Bereinigung mit den Schwellenwerten von 
                mind. 10 Instanzen pro Thema-Code in der Trainingsmenge und mind. 2 Instanzen pro Thema-Code in der Testmenge
                erfolgt sind (vgl. <xref linkend="ch_ThemaMeka_process"/>). Man beachte, dass sich die Prozentwerte hier im Gegensatz
                zu den VLB-Warengruppen auf über 100% summieren, da ja jedes Buch mehrere Codes tragen kann.
            </para>
            <themaDistribution/>
        </sect1>
    </appendix>
    <appendix xml:id="app_Avve"><!-- Anhang B: Das Programm Avve -->
        <info>
            <title>Das Programm Avve</title>
        </info>
        <para><indexterm class="startofrange" xml:id="idt_010"><primary>Avve</primary></indexterm>Das Java-Programm Avve ist darauf ausgelegt, zu einem Maschinenlernprogramm für die in der vorliegenden Arbeit beschriebenen
            E-Book-Klassifikationsaufgaben zu werden. Zum Zeitpunkt des Abschlusses dieser Untersuchung besteht es aber lediglich aus einigen kleineren Werkzeugen
            zur Datenvorverarbeitung. Der Quellcode des Programms steht online unter der Adresse <link xlink:href="https://github.com/sermo-de-arboribus/Avve/tree/version-1.0"/> zur
            Verfügung.</para>
        <para>Ich gebe hier einen kurzen Überblick über die Installation, die Kommandozeilenparameter, mit denen die vorhandenen Teilprogramme aufgerufen werden können und 
            motiviere anschließend einige Entwurfsentscheidungen und verwendete Bibliotheken.</para>
        <sect1> <!-- Installation und Starten -->
            <title>Installation und Starten</title>
            <para>Der Quellcodes des Programms kann von Github heruntergeladen werden: <link xlink:href="https://github.com/sermo-de-arboribus/Avve"/> (Zugriff
                1.4.2018). Um eine lauffähige Version aus den Quellen zu bauen, benötigt man ein aktuelles JDK (ab Version 8): <link
                    xlink:href="http://www.oracle.com/technetwork/java/javase/downloads/index.html"/> (Zugriff 1.4.2018) und Maven: <link
                    xlink:href="http://maven.apache.org/install.html"/> (Zugriff 1.4.2018).</para>
            <para>Das im Avve-Code enthaltene Maven-Buildskript erstellt eine einzelne Jar-Datei, welche alle nötigen Abhängigkeiten inklusive der Konfigurationsdateien
                enthält. Möchte man Konfigurationsparameter ändern, sollte man dies in den von Github heruntergeladenen Sourcen tun, bevor man das Projekt
                mit Maven erstellt. Die Konfigurationsdateien, z.&#160;B. für Lemma- und Wortartenkorrektur oder für den Zugriff auf die OpenThesaurus-Datenbank,
                befinden sich in Unterordnern von <code>src/main/resources</code>.</para>
            <para>Möchte man auch die Einspeisung von Oberbegriffen (<indexterm><primary>Hyperonym</primary></indexterm>Hyperonymen) verwenden, so benötigt man den OpenThesaurus als MySQL-Dump, der hier
                erhältlich ist: <link xlink:href="https://www.openthesaurus.de/about/download"/> (Zugriff 1.4.2018). Man importiert diesen Dump in eine
                beliebige MySQL-Datenbank und gibt die Zugriffsdaten (Datenbankhost, Username, Passwort) in der Ressourcendatei <code>hyperonym/hyperonym.properties</code>
                an.</para>
            <para>Zum Erstellen des Programms navigiert man dann in einer Kommando-Shell in den Wurzelordner
                des heruntergeladenen Sourcecodes (Avve) und gibt dort den Befehl <code>mvn package</code> ein. Im Unterordner <code>target</code>
                landen dann die beiden neu gebauten Dateien <code>avve-1.0.jar</code> und <code>avve-1.0-jar-with-dependencies.jar</code>. 
                Für das einfache Starten des Programms sollte möglichst letztere verwendet werden.</para>
            <para>Auf dem Rechner, auf dem Avve ausgeführt werden soll, muss für die Lemmatisierung auch der TreeTagger installiert sein. Das Programm und
                eine Installationsanleitung finden sich hier: <link xlink:href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"/> (Zugriff 1.4.2018).
                Man beachte, dass die TreeTagger-Lizenz nur für Forschungszwecke eine kostenlose Verwendung erlaubt. Kommerzielle Nutzung bedarf gesonderter
                Lizenzierung.</para>
            <para>Ein Aufruf des Programms erfolgt dann mit <code>java [Systemproperties] -jar avve-1.0-jar-with-dependencies.jar [Parameter]</code>. Bei
                den [Systemproperties] muss mindestens der Pfad zum Wurzelordner der TreeTagger-Installation angegeben werden: <code>-Dtreetagger.home=[/Pfad/zum/Treetagger-Verzeichnis]</code>.
                Es empfiehlt sich außerdem auf Windowssystemen die Standarddatei-Codierung auf UTF-8 zu setzen: <code>-Dfile.encoding=utf-8</code>. Einen Überblick
                über die möglichen Parameter gibt der folgende Abschnitt.</para>
        </sect1>
        <sect1> <!-- Teilprogramm EpubExtractor -->
            <title>Das Teilprogramm <code>EpubExtractor</code></title>
            <para>Kernstück von Avve ist die Klasse <code>EpubExtractor</code>, welche eine statische <code>main</code>-Methode zum Kommandozeilenaufruf
                bereitstellt. Die Aufgabe dieses Programms besteht darin, alle EPUB-Dateien in einer bestimmten Ordnerstruktur auszulesen, zu indizieren und
                als Datensätze im <indexterm><primary>XRFF</primary></indexterm>XRFF- oder <indexterm><primary>ARFF</primary>
                </indexterm>ARFF-Format zur Weiterverarbeitung mit Hilfe von Weka-Algorithmen zu speichern. Verwendet man wie oben beschrieben
                eine jar-Datei, welche über die Maven-Definition erstellt wurde, wird die <code>main</code>-Methode von <code>EpubExtractor</code> standardmäßig
                verwendet beim Aufruf von Java mit dem <code>-jar</code>-Parameter. Das Programm erstellt in dem Ordner, in dem es abgelegt ist, einen 
                Unterordner <code>output</code>, in welchem wiederum Unterordner für bestimmte Informationen abgelegt werden. Der <code>index</code>-Ordner
                nimmt den Lucene-Index der E-Book-Volltexte auf. Der Ordner <code>temp</code> speichert den Zwischenstand der aufbereiteten E-Book-Inhalte
                nach dem ersten Durchlauf, <code>stats</code> enthält für jede E-Book-Datei einzeln die statistisch aufbereiteten Daten, sowie den
                lemmatisierten Volltext. Diese Dateien sind vor allem für Debugging-Zwecke interessant. Im <code>output</code>-Hauptordner wird am Ende
                die erzeugte ARFF- oder XRFF-Datei mit einem Zeitstempel im Dateinamen abgelegt. Die Unterordner dieses Verzeichnisses werden nach dem 
                Lauf nicht automatisch gelöscht, man wird dies in der Regel aber zumindest für <code>temp</code> und <code>stats</code> vor dem nächsten 
                Programmlauf tun wollen. Ob man auch <code>index</code> löscht, hängt davon ab, was man tun möchte. Es kann sinnvoll sein, einen gemeinsamen 
                Lucene-Index über mehrere Läufe hinweg zu verwenden, z.&#160;B. wenn man Trainings- und Testdaten in getrennten Avve-Läufen erstellt und eine
                einheitliche TF-IDF-Berechnung erhalten möchte.</para>
            <para>Das Programm erwartet per <code>-folder</code>-Argument die absolute Pfadangabe zu einem Ordner im Dateisystem, in welchem die EPUB-Dateien
                abgelegt sind. Dabei wird davon ausgegangen, dass sich die EPUB-Dateien in Unterordnern des angegebenen Ordners befinden und die Namen der Unterordner
                den Namen der zu lernenden Klassen entsprechen (vgl. <xref linkend="abb5"/>). Der allgemeine Ablauf der E-Book-Aufbereitung wurde bereits
                in <xref linkend="ch_Avvelauf"/> geschildert und wird hier nicht wiederholt. Die Kommandozeilenparameter für den Aufruf von
                <code>EpubExtractor</code> lauten:</para>
            <itemizedlist spacing="compact"> <!-- Liste der Kommandozeilenparameter -->
                <listitem>
                    <para><code>-folder [Pfad]</code>: Der vollständige Pfad zum Eingabeordner, in dem sich die aufzubereitenden EPUB-Dateien befinden. Weder Dateiendungen
                        noch Inhalt der EPUB-Dateien werden vorab überprüft. Sollten sich im Eingabeordner Dateien befinden, welche keine EPUB-Dateien sind, erfolgt
                        erst beim erfolglosen Versuch, diese Dateien zu öffnen und zu verarbeiten eine Fehlermeldung, welche abgefangen und geloggt wird. Ein solcher
                        Fehler führt nicht zum Programmabbruch, sondern zum Übergang zur nächsten Datei.</para>
                </listitem>
                <listitem>
                    <para><code>-ml</code>: Die Dateien sollen für das <glossterm linkend="gt_multilabel_klassifikation">Multilabel-Lernen</glossterm> aufbereitet werden,
                        wie es in der vorliegenden Untersuchung für die Thema-Klassifikation ausprobiert wurde. Die Namen der Unterordner mit den EPUB-Dateien
                        müssen in diesem Fall aus einer kommagetrennten Liste aller auf eine Datei zutreffenden Klassen bestehen (z.&#160;B. „FBA,1KBB,5AL“). Die Ausgabe
                        erfolgt bei gesetztem <code>ml</code>-Flag als ARFF-Datei, da Meka keine XRFF-Dateien verarbeiten kann. Bei nicht gesetztem <code>ml</code>-Flag
                        wird eine XRFF-Datei ausgegeben, um ggf. Manipulations- oder Abfragemöglichkeiten mit Hilfe von XML-Technologien zu bieten.</para>
                </listitem>
                <listitem>
                    <para><code>-wvs [Zahl]</code>: Gibt die Größe der (vorläufigen) Wortliste an, welche nach den in <xref linkend="ch_LexSemProp_Wortvektor"/> 
                        beschriebenen Prinzipien ausgegeben wird. Diese Wortliste stellt einen Zwischenschritt dar zwischen dem Lucene-Index, der alle Lemmata aller
                        eingelesenen E-Books enthält und dem noch zu erstellenden Wortvektor. Die Wortliste ist in der erzeugten ARFF- bzw. XRFF-Datei deshalb zunächst
                        eine einzelnes String-Attribut, welches höchstens die Anzahl an Wörtern enthält, die mit dem <code>wvs</code>-Parameter angegeben wurde. Wird
                        kein <code>wvs</code>-Parameter angegeben, wird ein Defaultwert von 200 angenommen.</para>
                </listitem>
                <listitem>
                    <para><code>-cv [Pfad]</code>: Weist Avve an, ein kontrolliertes Vokabular zu verwenden und in den ARFF- bzw. XRFF-Dateien mit auszugeben. Zusammen
                        mit dem Parameter wird ein Dateipfad übergeben zu einer Textdatei, welche pro Zeile ein Lemma des kontrollierten Vokabulars enthält. Die 
                        generierte XRFF-Datei enthält dann für jeden E-Book-Datensatz und jedes Lemma aus der <code>cv</code>-Datei ein Attribut, welches die 
                        Vorkommenshäufigkeit dieses Lemmas im jeweiligen E-Book angibt.</para>
                </listitem>
                <listitem>
                    <para><code>-dnifw</code>: <indexterm><primary>Fremdsprachen</primary><secondary>Indexierung von Wörtern in</secondary>
                    </indexterm>Die Buchstabenkombination dieses Flags steht für „do not index foreign words“. Wörter, welche der Wortarten-Tagger als 
                        <emphasis>FM</emphasis> („fremdsprachliches Material“) gekennzeichnet hat, werden nicht in den Lucene-Index geschrieben (vgl. <xref
                            linkend="ch_NB_improve_fw"/>).</para>
                </listitem>
                <listitem>
                    <para><code>-lc</code>: Wenn dieses Flag („lemma correction“) gesetzt ist, wird im Standard-Ressourcen-Ordner eine Textdatei mit dem Namen
                        „lemmatizer-de-dict.txt“ dazu verwendet, Lemmata, welche der TreeTagger-Lemmatisierer ausgibt, zu kontrollieren und ggf. zu korrigieren.
                        Die Textdatei muss dazu leerzeichen- oder tabulatorgetrennte Tripel enthalten: falsches_Lemma Wortartenkürzel richtiges_Lemma. Die in den
                        Avve-Ressourcen enthaltene Lemmakorrekturdatei enthält über 2600 Einträge, welche teilweise auch der Vereinheitlichung von Schreibweisen, z.&#160;B. 
                        bei Interjektionen, dient.</para>
                </listitem>
                <listitem>
                    <para><code>-pc</code>: Wenn dieses Flag („part-of-speech correction“) gesetzt ist, erfolgt eine Korrektur von Wortartenkürzeln. Das Verfahren 
                        funktioniert analog zu <code>-lc</code>, die verwendete Textdatei muss sich im Ressourcenordner unter dem Namen „posttag-de-dict.txt“ befinden.</para>
                </listitem>
                <listitem>
                    <para><code>-nolig</code>: Das Flag „no ligatures“ normalisiert typografische Ligaturen (z.&#160;B. zusammengezogenes fi, Unicode-Punkt U+FB01) zu den
                        entsprechenden Einzelbuchstaben, und zwar bevor Wortartentagger und Lemmatisierer auf den Text angewandt werden (vgl. <xref
                            linkend="ch_NB_improve_nolig"/>).</para>
                </listitem>
                <listitem>
                    <para><code>-urlnorm</code>: Dieses Flag sorgt dafür, dass in den E-Book-Texten vorkommende Webseitenlinks verkürzt werden auf die reine
                        Angabe des Protokolls (z.&#160;B. https://www.hochschule-trier.de/index.php?id=fernstudium wird verkürzt zu https). So ist im Text immer
                        noch die Information vorhanden, dass an dieser Stelle ein WWW-Link vorhanden ist, aber störende Nichtworte wie z.&#160;B. „php“ oder
                        kryptische Buchstabenkombinationen wie z.&#160;B. „xFH73-3IjlY“ im Youtube-Link https://www.youtube.com/watch?v=xFH73-3IjlY werden somit
                        nicht indiziert (vgl. <xref linkend="ch_NB_improve_urlnorm"/>).</para>
                </listitem>
                <listitem>
                    <para><code>-usethesaurus</code>: <indexterm><primary>Hyperonym</primary></indexterm>Ist dieses Flag gesetzt, so wird versucht, nach dem in <xref linkend="ch_LexSemProp_Hyperonym"/> geschilderten
                        Vorgehen Oberbegriffe zu finden. Dabei wird der unter <link xlink:href="https://www.openthesaurus.de/about/download"/> herunterladbare Thesaurus
                        verwendet. Dieser Thesaurus muss in einer MySQL-Datenbank verfügbar gemacht werden. Die Zugriffsdaten auf diese MySQL-Datenbank sind im
                        Standardressourcen-Ordner, Unterordner „hyperonym“, Dateiname „hyperonym.properties“ zu hinterlegen. Die Verwendung dieses Flags kann 
                        die Aufbereitungsdauer der XRFF-Daten erheblich verlängern.</para> 
                </listitem>
            </itemizedlist>
            <para>Das Programm gibt die Ergebnisse auf der Konsole aus und schreibt diese zusätzlich in die Datei <code>avveRun.log</code>.</para>
        </sect1>
        <sect1> <!-- Das Programm ThemaTrainer -->
            <title>Das Programm <code>ThemaTrainer</code></title>
            <para>Aufgabe und Funktionsweise des Programms <code>avve.meka.ThemaTrainer</code> wurden in <xref linkend="ch_ThemaMeka_process"/> beschrieben. Man benötigt 
                für dieses Programm zwei <glossterm linkend="gt_arff">ARFF</glossterm>-Dateien mit Multilabel-Datensätzen. Zwei optionale Parameter sind die 
                Schwellenwerte für die Mindestzahl an Trainingsdatensätzen und Testdatensätzen pro Klasse. Die Angabe der ersten beiden Dateipfade ist
                verpflichtend, die Angabe der Schwellenwerte ist optional.</para>
            <para>Ein typischer Programmaufruf von <code>ThemaTrainer</code> sieht dann z.&#160;B. so aus:</para>
            <para><code>java -cp avve-1.0-jar-with-dependencies.jar avve.meka.ThemaTrainer /home/user/Trainingsdatei.arff /home/user/Testdatei.arff 20 10</code></para>
            <para>Das Programm gibt die Ergebnisse auf der Konsole aus und schreibt diese zusätzlich in die Datei <code>avveRun.log</code>.</para>
        </sect1>
        <sect1> <!-- Das Programm DumpIndex -->
            <title>Das Programm <code>DumpIndex</code></title>
             <para>Um Umfang und Inhalt des Lucene-Indexes überprüfen zu können, enthält Avve ein kleines Werkzeug um einen Dump dieses Indexes auszugeben.
                 Der Inhalt des Indexes wird in einem XML-Format auf der Kommandozeile ausgegeben und kann bei Bedarf leicht in eine Textdatei umgelenkt werden.
                 Das Programms erwartet einen Lucene-Index im Ordner <code>./output/index</code>. Ein typischer Aufruf sieht wie folgt aus:</para>
            <para><code>java -cp avve-1.0-jar-with-dependencies.jar avve.services.lucene.DumpIndex >luceneDump.xml</code></para>
            <para>In der parameterlosen Verwendung werden alle im Index enthaltenen Tokens inklusive ihrer Häufigkeit (Document Frequency) ausgegeben.
                Alternativ dazu ist der Aufruf mit dem Flag <code>docs</code> möglich. In diesem Fall erhält man als Ausgabe eine Aufliste aller im Index
                gespeicherten Dokumente.</para>
        </sect1>
        <sect1> <!-- Einige Entwurfsentscheidungen -->
            <title>Einige Entwurfsentscheidungen</title>
            <para>Da es sich wie erwähnt bei Avve derzeit nicht um ein vollwertiges Anwendungsprogramm, sondern lediglich um einen kleinen Werkzeugkasten
                zur Hilfe bei der Untersuchung der Fragestellungen rund um die E-Book-Klassifikation handelt, lohnt sich keine umfassende Dokumentation
                des derzeit vorliegenden Programmcodes. An dieser Stelle sollen lediglich einige Entwurfsentscheidungen motiviert, einige verwendete
                Bibliotheken begründet und einige Desiderata aufgezeigt werden.</para>
            <sect2>
                <title>Testbarkeit</title>
                <para>Sollte sich Avve hin zu einem Anwendungsprogramm entwickeln, wird es nötig werden, den Programmcode automatisiert zu testen. Da 
                    das Projekt mit Hilfe von Maven angelegt und strukturiert wurde, ist für Tests bereits ein Verzeichnis (<code>src/test/java</code>)
                    vorgesehen. Damit bei Unit-Tests von Dateizugriffen abstrahiert werden kann, wurden alle Dateizugriffe aus dem Programm heraus
                    nicht direkt ausgeführt, sondern werden an ein <code>FileService</code>-Interface delegiert. Im normalen Programmablauf erfolgen die
                    tatsächlichen Dateizugriffe über <code>FileServiceImpl</code>. Im Testumfeld könnte stattdessen einen alternative Implementierung 
                    zum Einsatz kommen, welche die Dateisystemzugriffe abfängt und ggf. gemockte Ergebnisse zurückliefert. Indem die konkret zu 
                    verwendende Implementierung per Dependency Injection an die verwendende Klasse übergeben wird, gelingt diese Abstraktion leicht.</para>
            </sect2>
            <sect2>
                <title>Konfigurierbarkeit</title>
                <para>In erster Linie wird Avve derzeit über die oben aufgeführten Kommandozeilenparameter konfiguriert. Weitere Konfigurationsdateien
                    befinden sich im Resourcen-Ordner <code>/src/main/resources</code> und dessen Unterordnern. Da dieser Ordner beim Erstellen mit dem
                    Maven-Skript in die .jar-Datei übernommen wird, ist die Anpassung dieser Resourcen-Konfigurationen nach dem Erstellen dieser 
                    Datei nur noch schwer möglich. Hier sollte ein Umbau erfolgen, so dass diejenigen Konfigurationen, welche für Anwenderinnen interessant
                    sind, außerhalb der .jar-Datei vorgenommen werden können. Interessant aus Anwendersicht sind insbesondere die folgenden 
                    Konfiguratinen:</para>
                <itemizedlist>
                    <listitem>
                        <para>Die Datenbankverbindungsparameter für die OpenThesaurus-Datenbank</para>
                    </listitem>
                    <listitem>
                        <para>Die Ersetzungslisten für Wortarten- und Lemmakorrekturen</para>
                    </listitem>
                    <listitem>
                        <para>Die Konfiguration des Loggingverhaltens, insbesondere des Loglevels<footnote><para>Zum Logging wird die Bibliothek log4j2 verendet: <link
                            xlink:href="https://logging.apache.org/log4j/2.x/"/> (Zugriff 1.4.2018).</para></footnote></para>
                    </listitem>
                </itemizedlist>
                <para>Daneben finden sich in den Ressourcen Internationalisierungsdateien für vom Programm ausgegebene Texte (siehe unten) und 
                    die Konfiguration für OASIS Catalogs (siehe <link xlink:href="https://www.oasis-open.org/committees/entity/spec-2001-08-06.html"/>, 
                    Zugriff 1.4.2018), welche für die Verarbeitung der EPUB-Dateien mit XML-Technologien notwendig sind. Diese Kataloge sind von rein
                    technischem Interesse und müssen von Anwendern nicht modifiziert werden.</para>
                <para>Mängel bei der Konfigurierbarkeit bestehen vor allem bei der zu verwendenden Sprache. Derzeit geht Avve über fest im Code gesetzte
                    Konstanten davon aus, dass die zu verarbeitenden E-Books in deutscher Sprache verfasst sind und sein müssen. Ein Einsatz von Avve für andere
                    Sprachen ließe sich jedoch leicht realisieren. Auch ist bisher nur ein Teil der Textvorverarbeitungsschritte konfigurierbar (z.&#160;B. 
                    URL-Normalisierung), andere sind dies noch nicht (z.&#160;B. Behandlung von Ziffern im Text). Hier sollte Avve noch flexibler werden.</para>
            </sect2>
            <sect2>
                <title>Internationalisierung</title>
                <para>Avve enthält vorläufig alle auf der Kommandozeile bzw. in der Logdatei ausgegebenen Texte sowohl in deutscher als auch in englischer 
                    Sprache. Es wird stets die Sprache verwendet, welche in der Java Virtual Machine (JVM) gesetzt ist. Im Programmcode befinden sich 
                    somit an keiner Stelle direkte Ausgaben von Strings. Stattdessen werden die auszugebenden Texte über <code>ResourceBundle</code>s (aus
                    dem Java-Paket <code>java.util</code>) geladen.</para>
                <para><indexterm class="endofrange" startref="idt_010"/></para>
            </sect2>
        </sect1>
    </appendix>
    <appendix xml:id="app_Verfahrensauswahl"><!-- Anhang C: Weka Knowledge-Flows und Ergebnisse der Versuche zur Auswahl geeigneter ML-Verfahren -->
        <info>
            <title>Weka Knowledge-Flows und Ergebnisse der Versuche zur Auswahl geeigneter ML-Verfahren</title>
        </info>
        <para>Ein Weka-Knowledge-Flow für die Datenvorverarbeitung wurde bereits in <xref linkend="abb7"/> gezeigt. Training und Evaluation wurden 
            zumindest für die Experimente mit VLB-Warengruppen<indexterm><primary>Warengruppe</primary></indexterm> ebenfalls als Weka-Knowledge-Flow
            konfiguriert und anschließend ausgeführt, allerdings in der Regel ohne grafische Benutzeroberfläche, sondern über die Kommandozeilen-Schnittstelle
            von Weka.</para>
        <figure xml:id="abb13" pgwide="1"> <!-- Trainings- und Evaluationsphase in Weka Knowledge Flow -->
            <title>Trainings- und Evaluationsphase in Weka Knowledge Flow</title>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="../img/Abb13_KF_Training.png" width="18.5cm"/>
                </imageobject>
            </mediaobject>
        </figure>
        <para>Im Knowledgeflow werden die Trainings- und Testdaten aus der Vorverarbeitungsphase zunächst geladen und entsprechend den Bedürfnissen von 
            Weka markiert, bevor sie dem eigentlichen Trainingsmodul übergeben werden, welches jeweils einen Maschinenlernalgorithmus implementiert.
            Nach Abschluss der Trainings werden drei verschiedene Dateien gespeichert: Die klassifzierten Testdaten, so dass sich dort Fehlklassifikationen
            genau und in Einzelfällen nachvollziehen lassen, das Evaluationsergebnis als Textdatei, sowie das trainierte Modell, welches später wieder
            geladen und zur Klassifikation verwendet werden kann.</para>
        <para>Die folgende <xref linkend="tab_Sondierungsphase"/> gibt nun eine Übersicht über die jeweils letzten Evaluationsergebnisse für diejenigen
            Maschinenlernverfahren, welche nach der Sondierungsphase nicht weiter verwendet wurden, entweder weil sie zu schlechte Erfolgsquoten aufwiesen
            oder weil sie sich als allzu ressourcenintensiv herausstellten. Die angegebene Dauer wurde jeweils auf Hardware mit 
            8 GigaByte RAM, 64-Bit-Betriebssystemen (Windows 10, Ubuntu Linux 16.04 LTS) und üblichen Bürocomputer-Prozessoren (z.&#160;B. Intel Core i5-4300U
            mit 1,90 GHz oder Intel Core i5-2520M mit 2.50 GHz) gemessen. Einige Versuche wurden auch auf Server-Rechnern mit bis zu 32 GB RAM ausgeführt, 
            insbesondere im Bereich des <indexterm><primary>Ensemble Learning</primary></indexterm>Ensemble Learnings. Wir geben hier aber vor allem die Ergebnisse von Basislernern.</para>
        <table xml:id="tab_Sondierungsphase" pgwide="1"> <!-- Ergebnisse der Auswahlphase -->
            <title>Ergebnisse der Auswahlphase</title>
            <tgroup cols="7">
                <colspec colname="Verfahren" colnum="1" colwidth="2*" align="left"/>
                <colspec colname="Instanzen" colnum="2" colwidth="1*" align="right"/>
                <colspec colname="Attributzahl" colnum="3" colwidth="1*" align="right"/>
                <colspec colname="Klassenzahl" colnum="4" colwidth="1*" align="right"/>
                <colspec colname="Dauer" colnum="5" colwidth="2*" align="left"/>
                <colspec colname="Erfolgsquote" colnum="6" colwidth="1*" align="right"/>
                <colspec colname="Bemerkung" colnum="7" colwidth="2*" align="left"/>
                <thead>
                    <row>
                        <entry>Verfahren</entry>
                        <entry>Instanzen (Train + Test)</entry>
                        <entry>Attributzahl</entry>
                        <entry>Klassenzahl</entry>
                        <entry>Dauer (Train + Eval)</entry>
                        <entry>Erfolgsquote</entry>
                        <entry>Bemerkung</entry>
                    </row>
                </thead>
                <tbody>
                    <row>
                        <entry>A1DE</entry> <!-- Verfahren -->
                        <entry>4437</entry> <!-- Instanzen (Train + Test) -->
                        <entry>551</entry> <!-- Attributzahl -->
                        <entry>153</entry> <!-- Klassenzahl -->
                        <entry>2m</entry> <!-- Dauer -->
                        <entry>40,77%</entry> <!-- Erfolgsquote -->
                        <entry>Averaged 1-Dependence Estimator; wurde wegen Speicherproblemen ohne TF-IDF-basierten Wortvektor, sondern nur
                            mit dem kontrollierten Vokabular verwendet</entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Bayes Net</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>38m</entry> <!-- Dauer -->
                        <entry>49,45%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>CHIRP</entry> <!-- Verfahren -->
                        <entry>3081</entry> <!-- Instanzen (Train + Test) -->
                        <entry>13738</entry> <!-- Attributzahl -->
                        <entry>143</entry> <!-- Klassenzahl -->
                        <entry>7h 45m</entry> <!-- Dauer -->
                        <entry>43,92%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Decision Stump</entry> <!-- Verfahren -->
                        <entry>2733</entry> <!-- Instanzen (Train + Test) -->
                        <entry>21271</entry> <!-- Attributzahl -->
                        <entry>142</entry> <!-- Klassenzahl -->
                        <entry>1m</entry> <!-- Dauer -->
                        <entry>19,11%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Decision Table</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>11h 42m</entry> <!-- Dauer -->
                        <entry>38,05%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Forest PA</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>8h 27m</entry> <!-- Dauer -->
                        <entry>54,76%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Hoeffding Tree</entry> <!-- Verfahren -->
                        <entry>4437</entry> <!-- Instanzen (Train + Test) -->
                        <entry>23454</entry> <!-- Attributzahl -->
                        <entry>153</entry> <!-- Klassenzahl -->
                        <entry>1h 25m</entry> <!-- Dauer -->
                        <entry>31,83%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>IBk</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>4m</entry> <!-- Dauer -->
                        <entry>53,87%</entry> <!-- Erfolgsquote -->
                        <entry>IBk ist eine Implementierung von k-nearest-neighbour</entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>J48 (C4.5)</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>6h 8m</entry> <!-- Dauer -->
                        <entry>37,05%</entry> <!-- Erfolgsquote -->
                        <entry>Mit Bagging wurde nach 44h 19m Erfolgsquote 47,22% erreicht, mit Random Subspace nach 22h 33m 50,20%</entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>JRip</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>20h 10m</entry> <!-- Dauer -->
                        <entry>54,86%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>KStar</entry> <!-- Verfahren -->
                        <entry>2733</entry> <!-- Instanzen (Train + Test) -->
                        <entry>21271</entry> <!-- Attributzahl -->
                        <entry>142</entry> <!-- Klassenzahl -->
                        <entry>8h 24m</entry> <!-- Dauer -->
                        <entry>0,30%</entry> <!-- Erfolgsquote -->
                        <entry>Bei Versuchen mit KStar blieben viele Testinstanzen unklassifiziert</entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Logit Boost mit Decision Stump</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>46h 17m</entry> <!-- Dauer -->
                        <entry>54,86%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>OneR</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>1m</entry> <!-- Dauer -->
                        <entry>23,15%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>PART</entry> <!-- Verfahren -->
                        <entry>2662</entry> <!-- Instanzen (Train + Test) -->
                        <entry>19819</entry> <!-- Attributzahl -->
                        <entry>105</entry> <!-- Klassenzahl -->
                        <entry>20h</entry> <!-- Dauer -->
                        <entry>32,64%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Random Forest</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>7m</entry> <!-- Dauer -->
                        <entry>41,96%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Random Tree</entry> <!-- Verfahren -->
                        <entry>6396</entry> <!-- Instanzen (Train + Test) -->
                        <entry>26713</entry> <!-- Attributzahl -->
                        <entry>157</entry> <!-- Klassenzahl -->
                        <entry>1m</entry> <!-- Dauer -->
                        <entry>19,10%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>REP Tree mit Bagging</entry> <!-- Verfahren -->
                        <entry>1h 18m</entry> <!-- Instanzen (Train + Test) -->
                        <entry>6396</entry> <!-- Attributzahl -->
                        <entry>26713</entry> <!-- Klassenzahl -->
                        <entry>157</entry> <!-- Dauer -->
                        <entry>50,40%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                    <row>
                        <entry>Simple Logistic</entry> <!-- Verfahren -->
                        <entry>4437</entry> <!-- Instanzen (Train + Test) -->
                        <entry>23454</entry> <!-- Attributzahl -->
                        <entry>153</entry> <!-- Klassenzahl -->
                        <entry>38h 56m</entry> <!-- Dauer -->
                        <entry>33,30%</entry> <!-- Erfolgsquote -->
                        <entry></entry> <!-- Bemerkung -->
                    </row>
                </tbody>
            </tgroup>
        </table>
    </appendix>
    <bibliography><!-- Bibliografie -->
        <title>Bibliografie</title>
        <!-- TODO: Koestler in Bibliografie aufnehmen? -->
        <biblioentry role="monograph" xml:id="bib_ABC14">
            <abbrev>ABC14</abbrev>
            <editor><personname><firstname>Thomas</firstname><surname>Bez</surname></personname></editor>
            <title xml:lang="de">ABC des Zwischenbuchhandels</title>
            <edition>7. Auflage</edition>
            <publisher>
                <publishername>Börsenverein des Deutschen Buchhandels</publishername>
                <address><city>Frankfurt am Main</city></address>
            </publisher>
            <pubdate>2014</pubdate>
            <pagenums>50 S.</pagenums>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_All89">
            <abbrev>All89</abbrev>
            <biblioset relation="part">
                <author><personname><firstname>Robert</firstname><othername>F.</othername><surname>Allen</surname></personname></author>
                <title xml:lang="en">Computer-Aided Stylistic Analysis. A Case Study of French Texts</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>István</firstname><othername>S.</othername><surname>Bátori</surname></personname></editor>
                    <editor><personname><firstname>Winfried</firstname><surname>Lenders</surname></personname></editor>
                    <editor><personname><firstname>Wolfgang</firstname><surname>Putschke</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Computational Linguistics / Computerlinguistik</title>
                <publisher>
                    <publishername>Walter de Gruyter</publishername>
                    <address><city>Berlin</city></address>
                </publisher>
                <pubdate>1989</pubdate>
                <pagenums>S. 544-552</pagenums>
                <biblioid>3-11-009792-3</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="messagesystem" xml:id="bib_BBL07">
            <abbrev>BBL07</abbrev>
            <title xml:lang="de">Warengruppe wird Pflichtfeld</title>
            <bibliomisc role="medium">online</bibliomisc>
            <publisher>
                <publishername>MVB Marketing- und Verlagsservice des Buchhandels</publishername>
                <address><city>Frankfurt am Main</city></address>
            </publisher>
            <pubdate>Dezember 2011</pubdate>
            <date role="cit">18.11.2017</date>
            <biblioid class="uri">https://www.boersenblatt.net/artikel-vlb.175619.html</biblioid>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_BGSV09">
            <abbrev>BGSV09</abbrev>
            <authorgroup>
                <author><personname><firstname>Pavel</firstname><surname>Brazdil</surname></personname></author>
                <author><personname><firstname>Christophe</firstname><surname>Giraud Carrier</surname></personname></author>
                <author><personname><firstname>Carlos</firstname><surname>Soares</surname></personname></author>
                <author><personname><firstname>Ricardo</firstname><surname>Vilalta</surname></personname></author>
            </authorgroup>
            <title xml:lang="en">Metalearning</title>
            <subtitle xml:lang="en">Applications to Data Mining</subtitle>
            <publisher>
                <publishername>Springer</publishername>
                <address><city>Berlin, Heidelberg</city></address>
            </publisher>
            <pubdate>2009</pubdate>
            <edition>1. Aufl.</edition>
            <pagenums>XI + 176 S.</pagenums>
            <biblioid class="isbn">978-3-540-73262-4</biblioid>
            <biblioid class="doi">10.1007/978-3-540-73263-1</biblioid>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_BH06">
            <abbrev>BH06</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Stephan</firstname><surname>Bloehdorn</surname></personname></author>
                    <author><personname><firstname>Andreas</firstname><surname>Hotho</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Boosting for Text Classification with Semantic Features.</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Bamshad</firstname><surname>Mobasher</surname></personname></editor>
                    <editor><personname><firstname>Olfa</firstname><surname>Nasraoui</surname></personname></editor>
                    <editor><personname><firstname>Bing</firstname><surname>Liu</surname></personname></editor>
                    <editor><personname><firstname>Brij</firstname><surname>Masand</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Advances in Web Mining and Web Usage Analysis</title>
                <seriesvolnums>Lecture notes in computer science; Vol. 3932</seriesvolnums>
                <confgroup>
                    <conftitle>6th International Workshop on Knowledge Discovery on the Web, WebKDD</conftitle>
                    <address><city>Seattle</city><state>Washington</state><country>USA</country></address>
                    <confdates>August 22-25, 2004</confdates>
                </confgroup>
                <publisher>
                    <publishername>Springer</publishername>
                    <address><city>Berlin, Heidelberg</city></address>
                </publisher>
                <pubdate>2006</pubdate>
                <pagenums>S. 149-166</pagenums>
                <biblioid class="isbn">978-3-540-47127-1</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="article" xml:id="bib_BLS14">
            <abbrev>BLS14</abbrev>
            <biblioset relation="article">
                <authorgroup>
                    <author><personname><firstname>Vladimir</firstname><othername>V.</othername><surname>Bochkarev</surname></personname></author>
                    <author><personname><firstname>Eduard</firstname><othername>Yu.</othername><surname>Lerner</surname></personname></author>
                    <author><personname><firstname>Anna</firstname><othername>V.</othername><surname>Shevlyakova</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Deviations in the Zipf and Heaps laws in natural languages</title>
            </biblioset>
            <biblioset relation="journal">
                <title xml:lang="en">Journal of Physics: Conference Series</title>
                <pubdate>2014</pubdate>
                <pagenums>4 S.</pagenums>
                <volumenum>Vol. 490</volumenum>
                <biblioid class="doi">doi:10.1088/1742-6596/490/1/012009</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_BR99">
            <abbrev>BR99</abbrev>
            <author><personname><firstname>Ricardo</firstname><surname>Baeza-Yates</surname></personname></author>
            <author><personname><firstname>Berthier</firstname><surname>Ribeiro-Neto</surname></personname></author>
            <title xml:lang="en">Modern Information Retrieval</title>
            <publisher>
                <publishername>ACM Press</publishername>
                <address><city>New York</city></address>
            </publisher>
            <pubdate>1999</pubdate>
            <pagenums>XX, 513 S.</pagenums>
            <biblioid class="isbn">0-201-39829-X</biblioid>
        </biblioentry>
        <biblioentry role="messagesystem" xml:id="bib_Bro16">
            <abbrev>Bro16</abbrev>
            <author><personname><firstname>Jason</firstname><surname>Brownlee</surname></personname></author>
            <title xml:lang="en">Do Not Use Random Guessing As Your Baseline Classifier</title>
            <pubdate>March 4, 2016</pubdate>
            <date role="cit">20.2.2018</date>
            <biblioid class="uri">https://machinelearningmastery.com/dont-use-random-guessing-as-your-baseline-classifier/</biblioid>
        </biblioentry>
        <biblioentry role="article" xml:id="bib_Bur98">
            <abbrev>Bur98</abbrev>
            <biblioset relation="article">
                <author><personname><firstname>Christopher</firstname><othername>J. C.</othername><surname>Burges</surname></personname></author>
                <title xml:lang="en">A Tutorial on Support Vector Machines for Pattern Recognition</title>
            </biblioset>
            <biblioset relation="journal">
                <title xml:lang="en">Data Mining and Knowledge Discovery</title>
                <pubdate>1998</pubdate>
                <pagenums>S. 121-167</pagenums>
                <volumenum>Vol. 2</volumenum>
            </biblioset>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_Buß02">
            <abbrev>Buß02</abbrev>
            <editor><personname><firstname>Hadumod</firstname><surname>Bußmann</surname></personname></editor>
            <title xml:lang="de">Lexikon der Sprachwissenschaft</title>
            <publisher>
                <publishername>Alfred Kröner</publishername>
                <address><city>Stuttgart</city></address>
            </publisher>
            <pubdate>2002</pubdate>
            <edition>Dritte, aktualisierte und erweiterte Auflage</edition>
            <pagenums>783</pagenums>
            <biblioid class="isbn">3-520-45203-0</biblioid>
        </biblioentry>
        <biblioentry role="messagesystem" xml:id="bib_DNB03">
            <abbrev>DNB03</abbrev>
            <title xml:lang="de">Sachgruppen der Deutschen Nationalbibliografie bis 2003</title>
            <publisher>
                <publishername>Deutsche Nationalbibliothek</publishername>
                <address><city>Frankfurt am Main</city></address>
            </publisher>
            <pubdate>2014</pubdate>
            <date role="cit">21.01.2018</date>
            <biblioid class="uri">http://www.dnb.de/SharedDocs/Downloads/DE/DNB/service/sachgruppenDnbBis2003.html</biblioid>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_DPHS98">
            <abbrev>DPHS98</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Susan</firstname><surname>Dumais</surname></personname></author>
                    <author><personname><firstname>John</firstname><surname>Platt</surname></personname></author>
                    <author><personname><firstname>David</firstname><surname>Heckerman</surname></personname></author>
                    <author><personname><firstname>Mehran</firstname><surname>Sahami</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Inductive Learning Algorithms and Representations for Text Categorization</title>
            </biblioset>
            <biblioset relation="book">
                <editor><personname><firstname>Niki</firstname><surname>Pissinou</surname></personname></editor>
                <title xml:lang="en">Proceedings of the Seventh International Conference on Information and Knowledge Management</title>
                <confgroup>
                    <conftitle>7th European Conference on Principles and Practice of Knowledge Discovery in Databases</conftitle>
                    <address><city>Bethesda</city><state>Maryland</state><country>USA</country></address>
                    <confdates>November 02 - 07, 1998</confdates>
                </confgroup>
                <publisher>
                    <publishername>ACM</publishername>
                    <address><city>New York</city></address>
                </publisher>
                <pubdate>1998</pubdate>
                <biblioid class="isbn">1-58113-061-9</biblioid>
                <biblioid class="doi">10.1145/288627.288651</biblioid>
                <pagenums>S. 148-155</pagenums>
                <annotation><para>Online verfügbar ohne Originalpaginierung: <link xlink:href="http://susandumais.com/cikm98.pdf"/>. Diese Version wurde
                verwendet</para></annotation>
            </biblioset>
        </biblioentry>
        <biblioentry role="messagesystem" xml:id="bib_EDI17">
            <abbrev>EDI17</abbrev>
            <title xml:lang="en">Thema - the Subject Category Scheme for a Global Book Trade</title>
            <publisher>
                <publishername>EDItEUR</publishername>
                <address><city>London</city></address>
            </publisher>
            <pubdate>2017</pubdate>
            <date role="cit">21.01.2018</date>
            <biblioid class="uri">http://www.editeur.org/151/Thema/</biblioid>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_ER03">
            <abbrev>ER03</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Tapio</firstname><surname>Elomaa</surname></personname></author>
                    <author><personname><firstname>Juho</firstname><surname>Rousu</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">On Decision Boundaries of Naïve Bayes in Continuous Domains</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Nada</firstname><surname>Lavrač</surname></personname></editor>
                    <editor><personname><firstname>Dragan</firstname><surname>Gamberger</surname></personname></editor>
                    <editor><personname><firstname>Hendrik</firstname><surname>Blockeel</surname></personname></editor>
                    <editor><personname><firstname>Ljupco</firstname><surname>Todorovski</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Knowledge Discovery in Databases: PKDD 2003</title>
                <confgroup>
                    <conftitle>7th European Conference on Principles and Practice of Knowledge Discovery in Databases</conftitle>
                    <address><city>Cavtat-Dubrovnik</city><country>Croatia</country></address>
                    <confdates>September 22-26, 2003</confdates>
                </confgroup>
                <seriesvolnums xml:lang="en">Lecture Notes in Computer Science 2838</seriesvolnums>
                <publisher>
                    <publishername>Springer</publishername>
                    <address><city>Berlin, Heidelberg, New York</city></address>
                </publisher>
                <pubdate>2003</pubdate>
                <pagenums>S. 144-155</pagenums>
                <biblioid>3-540-20085-1</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_Ert16">
            <abbrev>Ert16</abbrev>
            <author><personname><firstname>Wolfgang</firstname><surname>Ertel</surname></personname></author>
            <title xml:lang="de">Grundkurs Künstliche Intelligenz. Eine praxisorientierte Einführung</title>
            <edition>4. Auflage</edition>
            <publisher>
                <publishername>Springer Vieweg</publishername>
                <address><city>Wiesbaden</city></address>
            </publisher>
            <pubdate>2016</pubdate>
            <pagenums>XVIII + 385 S.</pagenums>
            <biblioid class="isbn">978-3-658-13548-5</biblioid>
        </biblioentry>
        <biblioentry role="article" xml:id="bib_GS04">
            <abbrev>GS04</abbrev>
            <biblioset relation="article">
                <authorgroup>
                    <author><personname><firstname>Thomas</firstname><othername>L.</othername><surname>Griffiths</surname></personname></author>
                    <author><personname><firstname>Mark</firstname><surname>Steyvers</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Finding Scientific Topics</title>
            </biblioset>
            <biblioset relation="journal">
                <title xml:lang="en">Proceedings of the National Academy of Sciences (PNAS)</title>
                <pubdate>April 6, 2004</pubdate>
                <pagenums>S. 5228-5235</pagenums>
                <volumenum>Vol. 101, Suppl. 1</volumenum>
            </biblioset>
        </biblioentry>
        <biblioentry role="article" xml:id="bib_HT98">
            <abbrev>HT98</abbrev>
            <biblioset relation="article">
                <authorgroup>
                    <author><personname><firstname>Trevor</firstname><surname>Hastie</surname></personname></author>
                    <author><personname><firstname>Robert</firstname><surname>Tibshirani</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Classification by Pairwise Coupling</title>
            </biblioset>
            <biblioset relation="journal">
                <title xml:lang="en">The Annals of Statistics</title>
                <pubdate>1998</pubdate>
                <pagenums>S. 451-471</pagenums>
                <volumenum>Vol. 26, No. 2</volumenum>
            </biblioset>
        </biblioentry>
        <biblioentry role="article" xml:id="bib_Hol93">
            <abbrev>Hol93</abbrev>
            <biblioset relation="article">
                <author><personname><firstname>Robert</firstname><othername>C.</othername><surname>Holte</surname></personname></author>
                <title xml:lang="en">Very Simple Classification Rules Perform Well on Most Commonly Used Datasets</title>
            </biblioset>
            <biblioset relation="journal">
                <title xml:lang="en">Machine Learning</title>
                <pubdate>1993</pubdate>
                <pagenums>63-91</pagenums>
                <volumenum>11</volumenum>
            </biblioset>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_JB03">
            <abbrev>JB03</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Aleks</firstname><surname>Jakulin</surname></personname></author>
                    <author><personname><firstname>Ivan</firstname><surname>Bratko</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Analyzing Attribute Dependencies</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Nada</firstname><surname>Lavrač</surname></personname></editor>
                    <editor><personname><firstname>Dragan</firstname><surname>Gamberger</surname></personname></editor>
                    <editor><personname><firstname>Hendrik</firstname><surname>Blockeel</surname></personname></editor>
                    <editor><personname><firstname>Ljupco</firstname><surname>Todorovski</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Knowledge Discovery in Databases: PKDD 2003</title>
                <confgroup>
                    <conftitle>7th European Conference on Principles and Practice of Knowledge Discovery in Databases</conftitle>
                    <address><city>Cavtat-Dubrovnik</city><country>Croatia</country></address>
                    <confdates>September 22-26, 2003</confdates>
                </confgroup>
                <seriesvolnums xml:lang="en">Lecture Notes in Computer Science 2838</seriesvolnums>
                <publisher>
                    <publishername>Springer</publishername>
                    <address><city>Berlin, Heidelberg, New York</city></address>
                </publisher>
                <pubdate>2003</pubdate>
                <pagenums>S. 229-240</pagenums>
                <biblioid>3-540-20085-1</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_JL95">
            <abbrev>JL95</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>George</firstname><surname>John</surname></personname></author>
                    <author><personname><firstname>Pat</firstname><surname>Langley</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Estimating Continuous Distributions in Bayesian Classifiers</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Philippe</firstname><surname>Besnard</surname></personname></editor>
                    <editor><personname><firstname>Steve</firstname><surname>Hanks</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">UAI '95: Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence</title>
                <confgroup>
                    <conftitle>Eleventh Annual Conference on Uncertainty in Artificial Intelligence</conftitle>
                    <address><city>Montreal</city><state>Québec, Canada</state></address>
                    <confdates>August 18-20, 1995</confdates>
                </confgroup>
                <publisher>
                    <publishername>Morgan Kaufmann</publishername>
                    <address><city>San Francisco</city></address>
                </publisher>
                <pubdate>1995</pubdate>
                <pagenums>S. 338-345</pagenums>
                <biblioid class="isbn">1-55860-385-9</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_KLS03">
            <abbrev>KLS03</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Michael</firstname><surname>Kockelkorn</surname></personname></author>
                    <author><personname><firstname>Andreas</firstname><surname>Lüneburg</surname></personname></author>
                    <author><personname><firstname>Tobias</firstname><surname>Scheffer</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Using Transduction and Multi-view Learning to Answer Emails</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Nada</firstname><surname>Lavrač</surname></personname></editor>
                    <editor><personname><firstname>Dragan</firstname><surname>Gamberger</surname></personname></editor>
                    <editor><personname><firstname>Hendrik</firstname><surname>Blockeel</surname></personname></editor>
                    <editor><personname><firstname>Ljupco</firstname><surname>Todorovski</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Knowledge Discovery in Databases: PKDD 2003</title>
                <confgroup>
                    <conftitle>7th European Conference on Principles and Practice of Knowledge Discovery in Databases</conftitle>
                    <address><city>Cavtat-Dubrovnik</city><country>Croatia</country></address>
                    <confdates>September 22-26, 2003</confdates>
                </confgroup>
                <seriesvolnums xml:lang="en">Lecture Notes in Computer Science 2838</seriesvolnums>
                <publisher>
                    <publishername>Springer</publishername>
                    <address><city>Berlin, Heidelberg, New York</city></address>
                </publisher>
                <pubdate>2003</pubdate>
                <pagenums>S. 266-277</pagenums>
                <biblioid>3-540-20085-1</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="message" xml:id="bib_KNS97">
            <abbrev>KNS97</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Brett</firstname><surname>Kessler</surname></personname></author>
                    <author><personname><firstname>Geoffrey</firstname><surname>Nunberg</surname></personname></author>
                    <author><personname><firstname>Hinrich</firstname><surname>Schütze</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Automatic Detection of Text Genre</title>
            </biblioset>
            <biblioset relation="book">
                <title>arXiv:cmp-lg/9707002v1</title>
                <pagenums>7 S.</pagenums>
                <biblioid class="uri">https://arxiv.org/abs/cmp-lg/9707002</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_KW14">
            <abbrev>KW14</abbrev>
            <authorgroup>
                <author><personname><firstname>Markus</firstname><surname>Klose</surname></personname></author>
                <author><personname><firstname>Daniel</firstname><surname>Wirgley</surname></personname></author>
            </authorgroup>
            <title xml:lang="de">Einführung in Apache Solr. Praxiseinstieg in die innovative Suchtechnologie</title>
            <edition>1. Auflage</edition>
            <publisher>
                <publishername>O’Reilly Verlag</publishername>
                <address><city>Köln</city></address>
            </publisher>
            <pubdate>2014</pubdate>
            <pagenums>XVI + 319 S.</pagenums>
            <biblioid class="isbn">978-3-95561-421-8</biblioid>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_LH03">
            <abbrev>LH03</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Boris</firstname><surname>Lauser</surname></personname></author>
                    <author><personname><firstname>Andreas</firstname><surname>Hotho</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Automatic Multi-label Subject Indexing in a Multilingual Environment</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Traugott</firstname><surname>Koch</surname></personname></editor>
                    <editor><personname><firstname>Ingeborg</firstname><othername>T.</othername><surname>Sølvberg</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Research and Advanced Technology for Digital Libraries, 7th European Conference, ECDL 2003</title>
                <confgroup>
                    <conftitle xml:lang="de">7th European Conference on Digital Libraries (ECDL)</conftitle>
                    <address>Trondheim, Norway</address>
                    <confdates>August 17-22, 2003</confdates>
                </confgroup>
                <seriesvolnums xml:lang="en">Lecture Notes in Computer Science 2769</seriesvolnums>
                <publisher>
                    <publishername>Springer</publishername>
                    <address><city>Heidelberg</city></address>
                </publisher>
                <pubdate>2003</pubdate>
                <pagenums>S. 140-151</pagenums>
                <biblioid>3-540-40726-X</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="message" xml:id="bib_MCCD13">
            <abbrev>MCCD13</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Tomas</firstname><surname>Mikolov</surname></personname></author>
                    <author><personname><firstname>Kai</firstname><surname>Chen</surname></personname></author>
                    <author><personname><firstname>Greg</firstname><surname>Corrado</surname></personname></author>
                    <author><personname><firstname>Jeffrey</firstname><surname>Dean</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Efficient Estimation of Word Representations in Vector Space</title>
            </biblioset>
            <biblioset relation="book">
                <title>ArXiv:1301.3781 [cs.CL]</title>
                <pagenums>12 S.</pagenums>
                <biblioid class="uri">https://arxiv.org/abs/1301.3781</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_NS03">
            <abbrev>NS03</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Rita</firstname><surname>Nübel</surname></personname></author>
                    <author><personname><firstname>Paul</firstname><surname>Schmidt</surname></personname></author>
                </authorgroup>
                <title xml:lang="de">Automatische mehrsprachige Indexierung mit dem AUTINDEX System.</title>
            </biblioset>
            <biblioset relation="book">
                <editor><personname><firstname>Ralph</firstname><surname>Schmidt</surname></personname></editor>
                <title xml:lang="en">Competence in Content.</title>
                <confgroup>
                    <conftitle xml:lang="en">25. Online-Tagung der DGI.</conftitle>
                    <address>Frankfurt/Main</address>
                    <confdates>3.-5. Juni 2003</confdates>
                </confgroup>
                <publisher>
                    <publishername>Deutsche Gesellschaft für Informationswissenschaft und Informationspraxis e. V.</publishername>
                    <address><city>Frankfurt/Main</city></address>
                </publisher>
                <pubdate>2003</pubdate>
                <pagenums>S. 88-95</pagenums>
                <biblioid class="isbn">3-925474-48-X</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_Pla99">
            <abbrev>Pla99</abbrev>
            <biblioset relation="part">
                <author><personname><firstname>John</firstname><surname>Platt</surname></personname></author>
                <title xml:lang="en">Fast Training of SVMs Using Sequential Minimal Optimization</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Bernhard</firstname><surname>Schölkopf</surname></personname></editor>
                    <editor><personname><firstname>Christopher</firstname><othername>J. C.</othername><surname>Burges</surname></personname></editor>
                    <editor><personname><firstname>Alexander</firstname><othername>J.</othername><surname>Smola</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Advances in Kernel Methods. Support Vector Learning</title>
                <publisher>
                    <publishername>MIT Press</publishername>
                    <address><city>Cambridge</city><state>Massachusetts</state><country>USA</country></address>
                </publisher>
                <pagenums>S. 185-208</pagenums>
                <pubdate>1998</pubdate>
            </biblioset>
        </biblioentry>
        <biblioentry role="messagesystem" xml:id="bib_Rea15">
            <abbrev>Rea15</abbrev>
            <author><personname><firstname>Jesse</firstname><surname>Read</surname></personname></author>
            <title xml:lang="en">Multi-label Classification</title>
            <publisher>
                <publishername>Summer School on Data Sciences for Big Data</publishername>
            </publisher>
            <pubdate>2015</pubdate>
            <date role="cit">29.3.2018</date>
            <biblioid class="uri"><link xlink:href="http://jmread.github.io/talks/Tutorial-MLC-Porto.pdf"/></biblioid>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_RN16">
            <abbrev>RN16</abbrev>
            <authorgroup>
                <author><personname><firstname>Stuart</firstname><surname>Russell</surname></personname></author>
                <author><personname><firstname>Peter</firstname><surname>Norvig</surname></personname></author>
            </authorgroup>
            <title xml:lang="en">Artificial Intelligence. A Modern Approach. International Edition</title>
            <edition>Third Edition</edition>
            <publisher>
                <publishername>Pearson</publishername>
                <address><city>Boston et al.</city></address>
            </publisher>
            <pubdate>2016</pubdate>
            <pagenums>XVIII + 1132 Seiten</pagenums>
            <biblioid class="isbn">978-1-292-15396-4</biblioid>
        </biblioentry>
        <biblioentry role="article" xml:id="bib_RRPH16">
            <abbrev>RRPH16</abbrev>
            <biblioset relation="article">
                <authorgroup>
                    <author><personname><firstname>Jesse</firstname><surname>Read</surname></personname></author>
                    <author><personname><firstname>Peter</firstname><surname>Reutemann</surname></personname></author>
                    <author><personname><firstname>Bernhard</firstname><surname>Pfahringer</surname></personname></author>
                    <author><personname><firstname>Geoff</firstname><surname>Holmes</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">MEKA: A Multi-label/Multi-target Extension to Weka</title>
            </biblioset>
            <biblioset relation="journal">
                <title xml:lang="en">Journal of Machine Learning Research</title>
                <pubdate>2016</pubdate>
                <volumenum>17</volumenum>
                <pagenums>S. 1-5</pagenums>
            </biblioset>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_SC08">
            <abbrev>SC08</abbrev>
            <authorgroup>
                <author><personname><firstname>Ingo</firstname><surname>Steinwart</surname></personname></author>
                <author><personname><firstname>Andreas</firstname><surname>Christmann</surname></personname></author>
            </authorgroup>
            <title xml:lang="en">Support Vector Machines</title>
            <publisher>
                <publishername>Springer Science+Business Media</publishername>
                <address><city>New York</city></address>
            </publisher>
            <pubdate>2008</pubdate>
            <pagenums>XVI + 601 S.</pagenums>
            <biblioid class="isbn">978-0-387-77242-4</biblioid>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_Sch94">
            <abbrev>Sch94</abbrev>
            <biblioset relation="part">
                <author><personname><firstname>Helmut</firstname><surname>Schmidt</surname></personname></author>
                <title xml:lang="en">Probabilistic Part-of-Speech Tagging Using Decision Trees</title>
            </biblioset>
            <biblioset relation="book">
                <editor><personname><firstname>Daniel</firstname><surname>Jones</surname></personname></editor>
                <title xml:lang="en">International Conference on New Methods in Language Processing (NeMLaP)</title>
                <publisher>
                    <publishername>Centre for Computational Linguistics</publishername>
                    <address><city>Manchester</city></address>
                </publisher>
                <pubdate>2004</pubdate>
                <pagenums>S. 44-49</pagenums>
                <confgroup>
                    <conftitle>International Conference on New Methods in Language Processing</conftitle>
                    <address><city>Manchester</city><country>United Kingdom</country></address>
                    <confdates>September 14-16, 1994</confdates>
                </confgroup>
                <!-- This publication seems to have no biblioid -->
            </biblioset>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_Sch95">
            <abbrev>Sch95</abbrev>
            <biblioset relation="part">
                <author><personname><firstname>Helmut</firstname><surname>Schmidt</surname></personname></author>
                <title xml:lang="en">Improvements in Part-of-Speech Tagging with an Application to German</title>
            </biblioset>
            <biblioset relation="book">
                <editor><orgname>Institut Dalle Molle pour les études sémantiques et cognitives</orgname></editor>
                <title xml:lang="en">From Texts to Tags. Issues in Multilingual Language Analysis. Proceedings of the ACL Sigdat Workshop</title>
                <publisher>
                    <publishername>Centre for Computational Linguistics</publishername>
                    <address><city>Manchester</city></address>
                </publisher>
                <pubdate>1995</pubdate>
                <pagenums>S. 1-9</pagenums>
                <confgroup>
                    <conftitle>ACL Sigdat Workshop</conftitle>
                    <address> <phrase>University College, Belfield</phrase> <city>Dublin</city><country>Irland</country></address>
                    <confdates>March 27, 1995</confdates>
                </confgroup>
                <!-- This publication seems to have no biblioid -->
            </biblioset>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_SG17">
            <abbrev>SG17</abbrev>
            <authorgroup>
                <author><personname><firstname>Jimmy</firstname><surname>Soni</surname></personname></author>
                <author><personname><firstname>Rob</firstname><surname>Goodman</surname></personname></author>
            </authorgroup>
            <title xml:lang="en">A Mind at Play. How Claude Shannon Invented the Information Age.</title>
            <publisher>
                <publishername>Simon &amp; Schuster</publishername>
                <address><city>New York et al.</city></address>
            </publisher>
            <pubdate>2017</pubdate>
            <pagenums>XV + 366 S.</pagenums>
            <biblioid class="isbn">978-1-4767-6668-3</biblioid>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_SP03">
            <abbrev>SP03</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Maximilien</firstname><surname>Sauban</surname></personname></author>
                    <author><personname><firstname>Bernhard</firstname><surname>Pfahringer</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Text Categorisation Using Document Profiling</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Nada</firstname><surname>Lavrač</surname></personname></editor>
                    <editor><personname><firstname>Dragan</firstname><surname>Gamberger</surname></personname></editor>
                    <editor><personname><firstname>Hendrik</firstname><surname>Blockeel</surname></personname></editor>
                    <editor><personname><firstname>Ljupco</firstname><surname>Todorovski</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Knowledge Discovery in Databases: PKDD 2003</title>
                <confgroup>
                    <conftitle>7th European Conference on Principles and Practice of Knowledge Discovery in Databases</conftitle>
                    <address><city>Cavtat-Dubrovnik</city><country>Croatia</country></address>
                    <confdates>September 22-26, 2003</confdates>
                </confgroup>
                <seriesvolnums xml:lang="en">Lecture Notes in Computer Science 2838</seriesvolnums>
                <publisher>
                    <publishername>Springer</publishername>
                    <address><city>Berlin, Heidelberg, New York</city></address>
                </publisher>
                <pubdate>2003</pubdate>
                <pagenums>S. 411-422</pagenums>
                <biblioid>3-540-20085-1</biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="article" xml:id="bib_Seb02">
            <abbrev>Seb02</abbrev>
            <biblioset relation="article">
                <author><personname><firstname>Fabrizio</firstname><surname>Sebastiani</surname></personname></author>
                <title xml:lang="en">Machine Learning in Automated Text Categorization</title>
            </biblioset>
            <biblioset relation="journal">
                <title xml:lang="en">ACM Computing Surveys</title>
                <pubdate>March 2002</pubdate>
                <pagenums>S. 1-47</pagenums>
                <volumenum>Vol. 34 No. 1</volumenum>
            </biblioset>
        </biblioentry>
        <biblioentry role="message" xml:id="bib_STST99">
            <abbrev>STST99</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Anne</firstname><surname>Schiller</surname></personname></author>
                    <author><personname><firstname>Simone</firstname><surname>Teufel</surname></personname></author>
                    <author><personname><firstname>Christine</firstname><surname>Stöckert</surname></personname></author>
                    <author><personname><firstname>Christine</firstname><surname>Thielen</surname></personname></author>
                </authorgroup>
                <title xml:lang="de">Guidelines für das Tagging deutscher Textcorpora mit STTS (Kleines und großes Tagset)</title>
            </biblioset>
            <biblioset relation="book">
                <title xml:id="en">The Stuttgart-Tübingen Tagset (STTS)</title>
                <publisher>
                    <publishername>Institut für maschinelle Sprachverarbeitung der Universität Stuttgart</publishername>
                    <address><city>Stuttgart</city></address>
                </publisher>
                <publisher>
                    <publishername>Seminar für Sprachwissenschaft der Universität Tübingen</publishername>
                    <address><city>Tübingen</city></address>
                </publisher>
                <pubdate>1999</pubdate>
                <pagenums>86 S.</pagenums>
                <biblioid class="uri"><link xlink:href="http://www.sfs.uni-tuebingen.de/resources/stts-1999.pdf"/></biblioid>
            </biblioset>
        </biblioentry>
        <biblioentry role="book" xml:id="bib_Thema13">
            <abbrev>Thema13</abbrev>
            <!--<author><orgname>Thema International Steering Committee</orgname></author>-->
            <title xml:lang="de">THEMA Globales Themenschema</title>
            <subtitle xml:lang="de">Grundlegende Bedienungsanleitung, Version 1.0</subtitle>
            <pubdate>2013</pubdate>
            <pagenums>14 S.</pagenums>
            <biblioid class="uri"><link xlink:href="https://info.vlb.de/media/thema_globales_themenschema_-_grundlegende_bedienungsanleitung_v1_0_v2_1.pdf"/></biblioid>
        </biblioentry>
        <biblioentry role="article" xml:id="bib_TZ03">
            <abbrev>TZ03</abbrev>
            <biblioset relation="article">
                <authorgroup>
                    <author><personname><firstname>Kai Ming</firstname><surname>Ting</surname></personname></author>
                    <author><personname><firstname>Zheng</firstname><surname>Zijian</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">A Study of AdaBoost with Naive Bayesian Classifiers: Weakness and Improvement</title>
            </biblioset>
            <biblioset relation="journal">
                <title xml:lang="en">Computational Intelligence</title>
                <pubdate>2003</pubdate>
                <pagenums>S. 186-200</pagenums>
                <volumenum>Vol. 19 No. 2</volumenum>
            </biblioset>
        </biblioentry>
        <biblioentry role="messagesystem" xml:id="bib_VLBplus17">
            <abbrev>VLB+17</abbrev>
            <!--<editor><orgname>Marketing- und Verlagsservice des Buchhandels GmbH</orgname></editor>-->
            <title xml:lang="de">VLB+</title>
            <publisher>
                <publishername>Marketing- und Verlagsservice des Buchhandels GmbH</publishername>
                <address><city>Frankfurt am Main</city></address>
            </publisher>
            <pubdate>2017</pubdate>
            <date role="cit">8.1.2018</date>
            <biblioid class="uri">https://info.vlb.de/home/vlb-plus/</biblioid>
        </biblioentry>
        <biblioentry role="messagesystem" xml:id="bib_VLB17">
            <abbrev>VLB17</abbrev>
            <title xml:lang="de">thema. Neuer internationaler Standard zur Buchklassifikation</title>
            <publisher>
                <publishername>Marketing- und Verlagsservice des Buchhandels GmbH</publishername>
                <address><city>Frankfurt am Main</city></address>
            </publisher>
            <pubdate>2017</pubdate>
            <date role="cit">6.1.2018</date>
            <biblioid class="uri">http://info.vlb.de/dienstleister/thema-klassifikation/</biblioid>
        </biblioentry>
        <biblioentry role="monograph" xml:id="bib_WFHP17">
            <abbrev>WFHP17</abbrev>
            <authorgroup>
                <author><personname><firstname>Ian</firstname><othername>H.</othername><surname>Witten</surname></personname></author>
                <author><personname><firstname>Eibe</firstname><surname>Frank</surname></personname></author>
                <author><personname><firstname>Mark</firstname><othername>A.</othername><surname>Hall</surname></personname></author>
                <author><personname><firstname>Christopher</firstname><othername>J.</othername><surname>Pal</surname></personname></author>
            </authorgroup>
            <title xml:lang="en">Data Mining. Practical Machine Learning Tools and Techniques</title>
            <edition>Fourth Edition</edition>
            <publisher>
                <publishername>Morgan Kaufmann</publishername>
                <address><city>Cambridge/Massachusetts</city></address>
            </publisher>
            <pubdate>2017</pubdate>
            <pagenums>XXXII + 621 S.</pagenums>
            <biblioid class="isbn">978-0-12-804291-5</biblioid>
        </biblioentry>
        <biblioentry role="messagesystem" xml:id="bib_WGS06">
            <abbrev>WGS06</abbrev>
            <title xml:lang="de">Warengruppen-Systematik neu (WGSneu) – Version 2.0</title>
            <publisher>
                <publishername>Börsenverein des Deutschen Buchhandels</publishername>
                <address><city>Frankfurt am Main</city></address>
            </publisher>
            <pubdate>2006</pubdate>
            <date role="cit">21.1.2018</date>
            <biblioid class="uri">http://info.vlb.de/media/wgsneuversion2_0.pdf</biblioid>
        </biblioentry>
        <biblioentry role="contribution" xml:id="bib_ZVJ03">
            <abbrev>ZVJ03</abbrev>
            <biblioset relation="part">
                <authorgroup>
                    <author><personname><firstname>Huaiyu</firstname><surname>Zhu</surname></personname></author>
                    <author><personname><firstname>Shivakumar</firstname><surname>Vaithyanathan</surname></personname></author>
                    <author><personname><firstname>Mahesh</firstname><othername>V.</othername><surname>Mahesh</surname></personname></author>
                </authorgroup>
                <title xml:lang="en">Topic Learning from Few Examples</title>
            </biblioset>
            <biblioset relation="book">
                <authorgroup>
                    <editor><personname><firstname>Nada</firstname><surname>Lavrač</surname></personname></editor>
                    <editor><personname><firstname>Dragan</firstname><surname>Gamberger</surname></personname></editor>
                    <editor><personname><firstname>Hendrik</firstname><surname>Blockeel</surname></personname></editor>
                    <editor><personname><firstname>Ljupco</firstname><surname>Todorovski</surname></personname></editor>
                </authorgroup>
                <title xml:lang="en">Knowledge Discovery in Databases: PKDD 2003</title>
                <confgroup>
                    <conftitle>7th European Conference on Principles and Practice of Knowledge Discovery in Databases</conftitle>
                    <address><city>Cavtat-Dubrovnik</city><country>Croatia</country></address>
                    <confdates>September 22-26, 2003</confdates>
                </confgroup>
                <seriesvolnums xml:lang="en">Lecture Notes in Computer Science 2838</seriesvolnums>
                <publisher>
                    <publishername>Springer</publishername>
                    <address><city>Berlin, Heidelberg, New York</city></address>
                </publisher>
                <pubdate>2003</pubdate>
                <pagenums>S. 483-494</pagenums>
                <biblioid>3-540-20085-1</biblioid>
            </biblioset>
        </biblioentry>
    </bibliography>    
    <index/><!-- Index -->
    <glossary><!-- Glossar -->
        <info>
            <title>Glossar</title>
        </info>
        <glossdiv> <!-- A -->
            <title>A</title>
            <glossentry xml:id="gt_arff">
                <glossterm>ARFF</glossterm>
                <glossdef>
                    <para><indexterm><primary>ARFF</primary></indexterm>Attribute-Relation File Format. Ein Textdateiformat, welches im Umfeld der Weka-Bibliotheken zum Einsatz kommt. Die Dokumentation
                        des Formats findet man unter <link xlink:href="https://www.cs.waikato.ac.nz/ml/weka/arff.html"/> (Zugriff 1.4.2018).</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_avve">
                <glossterm>Avve</glossterm>
                <glossdef>
                    <para>Ein im Rahmen der vorliegenden Arbeit entstandenes Programm zu statistischen Aufbereitung von E-Books im EPUB-Format und Aufbereitung der
                        gewonnenen Statistiken im ARFF- oder XRFF-Format, so dass eine Weiterverarbeitung mit Weka-Maschinenlernalgorithmen leicht möglich ist. Avve ist ein 
                        Akronym für <emphasis>Automatische Verschlagwortung von E-Books</emphasis>.</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- B -->
            <title>B</title>
            <glossentry xml:id="gt_barsortiment">
                <glossterm>Barsortiment</glossterm>
                <glossdef>
                    <para>Eine im deutschsprachigen Buchhandel übliche Bezeichung für Buchgroßhändler, welche als Dienstleister für 
                        Buchhandlungen und andere Buchverkaufsstellen ein großes Buchsortiment vorrätig halten und über Nacht ausliefern.
                        Barsortimente erbringen neben dem Kerngeschäft der Bevorratung und schnellen Belieferung von Medienprodukten aus einer Hand auch 
                        weitere Dienstleistungen wie Erstellung von bibliografischen Verzeichnissen (Kataloge für Buchhändlerinnen und Buchhändler), 
                        Werbemitteln (Katalog für Endkunden und -kundinnen), EDV-Dienstleistungen (z.&#160;B. Warenwirtschaftssysteme und White-Label-Webshops
                        für Buchhandlungen, sowie elektronische Bestellungsweiterleitung von Buchhandlungen zu Verlagen).</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- D -->
            <title>D</title>
            <glossentry>
                <glossterm xml:id="gt_diskretisierung">Diskretisierung</glossterm>
                <glossdef>
                    <para>Im Kontext des Maschinenlernens meint Diskretisierung das Aufteilen des kontinuierlichen Wertebereichs von numerischen Attributen in 
                        mehrere Intervalle. Der Maschinenlernalgorithmus arbeitet dann nicht mehr mit dem Attributwert aus dem kontinuierlichen Wertebereich, 
                        sondern betrachtet jedes Intervall als einen der möglichen Attributwerte. Das Festlegen der Intervallgrenzen kann nach verschiedenen 
                        Verfahren und in überwachter Form (d. h. mit Blick auf klassifizierte Trainingsdaten) oder in unüberwachter Form (d. h. ohne Kenntnis 
                        der Zielklassen) erfolgen (vgl. <biblioref linkend="bib_WFHP17"/>, S. 296-303).</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- E -->
            <title>E</title>
            <glossentry xml:id="gt_ebook">
                <glossterm>E-Book</glossterm>
                <glossdef>
                    <para><indexterm><primary>E-Book</primary></indexterm>Eine Computerdatei, die den Inhalt eines Buches enthält und mit Hilfe dazu vorgesehener Hardware (E-Book-Lesegerät) oder Software
                        (E-Book-Leseprogramm, App) angezeigt werden kann.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_empfehlungsdienst">
                <glossterm>Empfehlungsdienst</glossterm>
                <glossdef>
                    <para>Ein Empfehlungsdienst ist ein Softwaresystem, welches vor allem im E-Commerce eingesetzt wird, um Kundinnen und Kunden 
                        Produkte basierend auf deren Interessen, die aus der Kaufhistorie oder aus Produktbewertungen durch ebendiese Kunden
                        quantitativ erhoben wurden, anzubieten.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_entscheidungsbaum">
                <glossterm>Entscheidungsbaum</glossterm>
                <glossdef>
                    <para>Im Maschinenlernen eine Form von Lernmodellen in Baumstruktur, in welcher in jedem Knoten ein Attribut der zu klassifizierenden
                         Instanzen auf einen bestimmten Wert oder Wertebereich hin geprüft wird. Auf Basis des Attributwerts erfolgt dann in dem Knoten eine 
                         Aufspaltung der Instanzen in Blattknoten oder weitere Teilbäume. Die Zahl der Kindknoten eines Knotens im Entscheidungsbaums ist
                         variabel.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_epub">
                <glossterm>EPUB</glossterm>
                <glossdef>
                    <para><indexterm><primary>EPUB</primary></indexterm>Ein Format-Standard für E-Books, in Versionen 2 und 3 vom International Digital Publishing Forum (IDPF) entwickelt, inzwischen
                        vom World Wide Web Consortium (W3C) gepflegt und weiter entwickelt. Eine EPUB-Datei besteht aus einem Zip-Archiv, in dem einige XML-Dateien 
                        mit Metadaten (Autor, Titel, Sprache, Verlag, Inhaltsverzeichnis, Lesereihenfolge) verpflichtend vorhanden sein müssen. Der eigentliche
                        Buchtext ist mit WWW-Technologien (XHTML, CSS) ausgezeichnet und in das Zip-Archiv zur Offlinenutzung integriert.</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv><!-- G -->
            <title>G</title>
            <glossentry xml:id="gt_gui">
                <glossterm>GUI</glossterm>
                <glossdef>
                    <para><indexterm><primary>Benutzeroberfläche, grafische</primary></indexterm>Abkürzung für „Graphical User Interface“, dt. grafische Benutzeroberfläche.</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- I -->
            <title>I</title>
            <glossentry xml:id="gt_imprint">
                <glossterm>Imprint</glossterm>
                <glossdef>
                    <para>Eine Verlagsmarke, die entweder keine rechtlich selbständige Firma ist oder als Firma zu einer Verlagsgruppe (Konzern) gehört, die aber
                        im Markt als Verlag auftritt, z.&#160;B. Penguin Deutschland als Teil der Verlagsgruppe Random House.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_information_retrieval">
                <glossterm>Information Retrieval</glossterm>
                <glossdef>
                    <para><indexterm><primary>Information Retrieval</primary>
                    </indexterm>Teildisziplin der Informatik, welche sich mit der Gewinnung von Information aus Texten und (oft textorierentierten) Datenbanken
                        beschäftigt. Beim Information Retrieval geht es meist darum, die Informationsbedürfnisse menschlicher Anwenderinnen und Anwendern
                        in idealer Weise zu befriedigen, und zwar sowohl bei der gezielten Suche nach Information als auch beim unterstützen Stöbern, Blättern
                        oder Browsen; vgl. <biblioref linkend="bib_BR99"/>.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_isbn">
                <glossterm>ISBN</glossterm>
                <glossdef>
                    <para>Akronym für die International Standardbuchnummer. Seit 2004 ist die ISBN eine dreizehnstellige Nummer, welche in das EAN-System integriert 
                        ist. EANs für Bücher beginnen mit den Ziffern 978, dann folgt ein Sprachcode (z.&#160;B. 3 für den deutschsprachigen Raum), anschließend eine 
                        Verlagsnummer und eine vom Verlag selbst vergebene Nummer sowie abschließend eine Prüfziffer (Algorithmus der Prüfziffernberechnung findet sich 
                        auf <link xlink:href="http://www.arndt-bruenner.de/mathe/scripts/pruefziffern.htm"/> (Zugriff am 16.02.2018). Verlagsnummern für den deutschsprachigen
                        Raum sowie Einzel-ISBNs werden von der ISBN-Agentur vergeben (<link xlink:href="https://www.german-isbn.de/"/>, Zugriff am 16.02.2018).</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- L -->
            <title>L</title>
            <glossentry xml:id="gt_ligatur">
                <glossterm>Ligatur</glossterm>
                <glossdef>
                    <para>In der Typografie eine Verbindung von zwei oder mehr Buchstaben zu einem Zeichen (Glyphe). Einige Zeichen, die als Ligatur
                        entstanden sind, wurden im Laufe der Zeit als vollwertige Buchstaben angesehen (z.&#160;B. das deutsche ß, welches aus einer s-z-Ligatur
                        gebildet wurde), andere Ligaturen dienen rein typografischen Zwecken (Lesbarkeit, Ästhetik).</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_lucene">
                <glossterm>Lucene</glossterm>
                <glossdef>
                    <para>Eine von der Apache Software Foundation bereitgestellte und gepflegte Java-Bibliothek zum Erstellen von Suchindizes 
                        und zur Bereitstellung von Suchfunktionen auf diesen Indizes: <link xlink:href="http://lucene.apache.org/"/></para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- M -->
            <title>M</title>
            <glossentry xml:id="gt_mimetyp">
                <glossterm>MIME-Typ</glossterm>
                <glossdef>
                    <para>MIME steht für Multipurpose Internet Mail Extensions. Dieser Internetstandard definierte ursprünglich Codes für,
                        Dateitypen, welche in E-Mails eingebettet werden, haben sich darüber hinaus aber auch in anderen Internetprotokollen,
                        bei denen der Dateityp einer zu übermittelnden Datei mitgegeben werden muss, durchgesetzt. In der EPUB-Spezifikation
                        dienen MIME-Typen dazu, im Manifest den Dateityp jeder im <glossterm linkend="gt_epub">EPUB</glossterm> enthaltenen Datei anzugeben.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_multiklassen_klassifikation">
                <glossterm>Multiklassen-Klassifikation</glossterm>
                <glossdef>
                    <para>In der Multiklassen-Klassifikation wird der Attributvektor
                        <inlineequation> <!-- Attributvektor -->
                            <mml:math>
                                <mml:mover>
                                    <mml:mrow>
                                        <mml:msub>
                                            <mml:mi>x</mml:mi>
                                            <mml:mi>i</mml:mi>
                                        </mml:msub>
                                    </mml:mrow>
                                    <mml:mo stretchy="true">→</mml:mo>
                                </mml:mover>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mo>∈</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:msup>
                                    <mml:mi>ℝ</mml:mi>
                                    <mml:mi>d</mml:mi>
                                </mml:msup>
                            </mml:math>
                        </inlineequation>
                        eines Eingabedatensatzes auf genau eine Klasse <inlineequation><mml:math><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub>
                            <mml:mo>∈</mml:mo> <mml:mi>C</mml:mi></mml:math></inlineequation> mit <inlineequation><mathphrase>C = { c<subscript>1</subscript>, c<subscript>2</subscript>, ..., c<subscript>n</subscript> }
                            </mathphrase></inlineequation> abgebildet:
                        <inlineequation> <!-- Klassifikationsabbildung -->
                            <mml:math>
                                <mml:mi>f</mml:mi>
                                <mml:mo>(</mml:mo>
                                <mml:mover>
                                    <mml:mi>x</mml:mi>
                                    <mml:mo stretchy="true">→</mml:mo>
                                </mml:mover>
                                <mml:mo>)</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mo>↦</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:msup>
                                    <mml:mi>ℝ</mml:mi>
                                    <mml:mi>d</mml:mi>
                                </mml:msup>
                                <mml:mo>⨯</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mi>C</mml:mi>
                            </mml:math>
                        </inlineequation>.
                    </para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_multilabel_klassifikation">
                <glossterm>Multilabel-Klassifikation</glossterm>
                <glossdef>
                    <para>In der Multilabel-Klassifikation wird der Attributvektor <inlineequation><mml:math><mml:mover><mml:mrow><mml:msub><mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mtext>&#160;</mml:mtext><mml:mo>∈</mml:mo>
                        <mml:mtext>&#160;</mml:mtext><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>d</mml:mi></mml:msup>
                    </mml:math></inlineequation> eines Eingabedatensatzes auf die Potenzmenge der Klassenmenge <inlineequation><mathphrase>C = {
                        c<subscript>1</subscript>, c<subscript>2</subscript>, ..., c<subscript>n</subscript> }
                    </mathphrase></inlineequation> abgebildet: <inlineequation><mml:math><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mover><mml:mrow><mml:mi>x</mml:mi>
                    </mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo>)</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mo>↦</mml:mo>
                        <mml:mtext>&#160;</mml:mtext><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>⨯</mml:mo><mml:mtext>&#160;</mml:mtext>
                        <mml:msup><mml:mi>2</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:math></inlineequation>.</para>
                    <para>Vgl. im Gegensatz dazu die <glossterm linkend="gt_multiklassen_klassifikation">Multiklassen-Klassifikation</glossterm>.</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- N -->
            <title>N</title>
            <glossentry xml:id="gt_nominelle_klasse">
                <glossterm>nominelle Klasse</glossterm>
                <glossdef>
                    <para>Klassen im Maschinenlernen, bei denen jede Klasse einen Namen hat und bei denen keine mathematische Beziehung zwischen den 
                        unterschiedlichen Klassen besteht. Die Klassen können somit nicht in eine berechenbare Nähe-Ferne-Relation oder in eine 
                        Reihenfolge gebracht werden.</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- O -->
            <title>O</title>
            <glossentry xml:id="gt_ocr">
                <glossterm>OCR</glossterm>
                <glossdef>
                    <para>Abkürzung für „optical character recognition“. Bezeichnung für Verfahren, welche in Grafikdateien (z.&#160;B. Scans oder Fotografien)
                        enthaltenen Text extrahieren.</para>
                </glossdef>
            </glossentry>
        </glossdiv>    
        <glossdiv> <!-- P -->
            <title>P</title>
            <glossentry xml:id="gt_pdf">
                <glossterm>PDF</glossterm>
                <glossdef>
                    <para>Ein ISO-Standard für ein Dokumentaustauschformat, das neben vielen anderen Einsatzgebieten (z.&#160;B. in der Druckvorstufe) auch für E-Books 
                        verwendet werden kann.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_pragmatik">
                <glossterm>Pragmatik</glossterm>
                <glossdef>
                    <para>In der Linguistik diejenige Teildisziplin, die sich mit „dem Gebrauch sprachlicher Ausdrücke in 
                        Äußerungssituationen befasst“ (<biblioref linkend="bib_Buß02"/>, S. 534). </para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- S -->
            <title>S</title>
            <glossentry xml:id="gt_semantik">
                <glossterm>Semantik</glossterm>
                <glossdef>
                    <para>Teildisziplin der Linguistik, die sich mit der Analyse der Bedeutung von sprachlichen Ausdrücken beschäftigt.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_stochastisch_unabhängig">
                <glossterm>stochastisch unabhängig</glossterm>
                <glossdef>
                    <para>Zwei (stochastische) Ereignisse A und B heißen stochastisch unabhängig voneinander, wenn gilt:</para>
                    <equation xml:id="formula_stochastisch_unabhängig"><!-- Formel stochastische Unabhängigkeit -->
                        <mml:math>
                            <mml:mrow>
                                <mml:mi style="italic">P</mml:mi>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:apply>
                                    <mml:intersect/>
                                    <mml:ci type="set">A</mml:ci>
                                    <mml:ci type="set">B</mml:ci>
                                </mml:apply>
                                <mml:mo stretchy="false">)</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:mo stretchy="false">=</mml:mo>
                                <mml:mtext>&#160;</mml:mtext>
                                <mml:apply>
                                    <mml:times/>
                                    <mml:mrow>
                                        <mml:mi style="italic">P</mml:mi>
                                        <mml:mo stretchy="false">(</mml:mo>
                                        <mml:ci type="set">A</mml:ci>
                                        <mml:mo stretchy="false">)</mml:mo>
                                    </mml:mrow>
                                    <mml:mrow>
                                        <mml:mi style="italic">P</mml:mi>
                                        <mml:mo stretchy="false">(</mml:mo>
                                        <mml:ci type="set">B</mml:ci>
                                        <mml:mo stretchy="false">)</mml:mo>
                                    </mml:mrow>
                                </mml:apply>
                            </mml:mrow>
                        </mml:math>
                    </equation>
                    <para>„Zwei Ereignisse sind also (stochastisch) unabhängig, wenn die Wahrscheinlichkeit, dass beide Ereignisse 
                        eintreten, gleich dem Produkt ihrer Einzelwahrscheinlichkeiten ist.“<footnote><para>Zitat aus dem Wikipedia-Artikel <link
                        xlink:href="https://de.wikipedia.org/wiki/Stochastisch_unabh%C3%A4ngige_Ereignisse"/>, Zugriff 25.2.2018. Diese Definition lässt sich vom
                        Ereignisbegriff lösen und allgemein auch auf Zufallsvariablen wenden, vgl. Wikipedia-Artikel <link
                        xlink:href="https://de.wikipedia.org/wiki/Stochastisch_unabh%C3%A4ngige_Zufallsvariablen"/>, Zugriff 25.2.2018.</para></footnote></para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- T -->
            <title>T</title>
            <glossentry xml:id="gt_tokenisierung">
                <glossterm>Tokenisierung</glossterm>
                <glossdef>
                    <para>In Bezug auf natürliche Sprachen die Zerlegung eines Textes in seine einzelnen Wörter; beim Parsen von Programmier-
                        oder Auszeichnungssprachen auch die Vorarbeit der Zerlegung einer Textdatei in Schlüsselwörter, Variablen, Operatoren u.ä.</para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- U -->
            <title>U</title>
            <glossentry xml:id="gt_überanpassung">
                <glossterm>Überanpassung</glossterm>
                <glossdef>
                    <para>Ein Phänomen im Maschinenlernen, bei dem ein Klassifikator nicht nur tatsächlich signifikante Unterschiede der Attribute im 
                        Bezug auf die zu lernenden Klassen findet, sondern darüber hinaus auch nicht-signifikante, zufällige Konstellationen in den 
                        Trainingsdaten oder fehlerhaft klassifizierte Trainingsdaten (auch Rauschen genannt) als Grundlage für die Klassifikation annimmt. 
                        Ein überangepasster Klassifikator wird in der späteren Anwendungsphase viele Instanzen falsch klassifizieren.</para>
                    <para>Die Wahrscheinlichkeit für das Auftreten von Überanpassung steigt mit der Anzahl der Attribute und der Anzahl der Klassen und sinkt
                        mit der Anzahl der Trainingsinstanzen (vgl. <biblioref linkend="bib_RN16"/>, S. 705, <biblioref linkend="bib_SC08"/>, S. 14).</para>
                    <para>Für eine formale Definition von Überanpassung siehe <biblioref linkend="bib_Ert16"/></para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- V -->
            <title>V</title>
            <glossentry xml:id="gt_verlagsauslieferung">
                <glossterm>Verlagsauslieferung</glossterm>
                <glossdef>
                    <para>Ein Lager- und Versanddienstleister für Verlage, der in der Regel die gesamte Buchproduktion von Verlagen einlagert und auf Bestellung hin
                        fakturiert und ausliefert. Neben Lagerung, Faktur und Lieferlogistik für traditionelle Produkte bieten Verlagsauslieferungen heutztage auch
                        Dienstleistungen in der Aggregation und Bündelung der Auslieferung elektronischer Produkte (z.&#160;B. E-Books) an.</para>
                </glossdef>
            </glossentry>
            <glossentry xml:id="gt_vlb">
                <glossterm>VLB</glossterm>
                <glossdef><para>Verzeichnis lieferbarer Bücher: Eine Buchdatenbank mit bibliografischen Angaben zu Büchern, welche im deutschsprachigen Buchmarkt
                    aktuell lieferbar sind. Die Datenbank wird von einer Wirtschaftstochter des Börsenvereins des Deutschen Buchhandels betrieben und mit
                    Daten von Verlagen gepflegt. Sie dient Buchhändlerinnen vor allem zur Recherche der Bestellbarkeit von Titeln und findet daneben im 
                    E-Commerce (Buchshops im Internet) Verwendung. Neben dem VLB spielen die Katalog der <glossterm
                        linkend="gt_barsortiment">Barsortimente</glossterm> eine große Rolle, welche diejenigen Bücher listen, die diese Großhändler in ihren 
                    Lagern führen. Sowohl VLB als auch Barsortimentskataloge enthalten auch Angaben zu E-Books.</para></glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- W -->
            <title>W</title>
            <glossentry xml:id="gt_weka">
                <glossterm>Weka</glossterm>
                <glossdef>
                    <para>Eine Programmbibliothek, welche viele Maschinenlern-Algorithmen in einer Java-Implementierung bereitstellt und über ein Pluginkonzept
                        weitere Algorithmen nachladen kann. Neben dem Quellcode der Algorithmen bietet Weka auch mehrere grafische Benutzeroberflächen zum Prüfen 
                        und Experimentieren mit Daten und Algorithmen. Das Programm entstand an der University of Waikato in Neuseeland, der Name ist ein Akronym für
                        <emphasis>Waikato Environment for Knowledge Analysis</emphasis> (vgl. <biblioref linkend="bib_WFHP17"/>, S. 553). Die offizielle Website
                        des Weka-Projekts ist erreichbar unter <link xlink:href="https://www.cs.waikato.ac.nz/ml/weka/"/></para>
                </glossdef>
            </glossentry>
        </glossdiv>
        <glossdiv> <!-- X -->
            <title>X</title>
            <glossentry xml:id="gt_xrff">
                <glossterm>XRFF</glossterm>
                <glossdef>
                    <para>Eine XML-Variante des <glossterm linkend="gt_arff">ARFF</glossterm>-Formats.</para>
                </glossdef>
            </glossentry>
        </glossdiv>
    </glossary>

</book>
